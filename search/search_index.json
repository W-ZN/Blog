{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"[\u7f6e\u9876]\u6b63\u5728\u7f13\u6162\u642c\u8fd0\u4e0e\u6dfb\u52a0\u535a\u5ba2\u4e2d (\u5495\u5495\u5495)","text":""},{"location":"#who-am-i-","title":"Who am I ?","text":"<p>I am WZN, an undergraduate. Student in the Computer Department at Qufu Normal University (QFNU, China) since 2021.</p> <p></p>"},{"location":"#research-interests-and-specialties","title":"Research Interests and specialties","text":"<p>Interests\uff1a</p> <p>Vision-and-Language, Natural Language Processing.</p> <p>Specialties\uff1a</p> <p>React\u3001Go\u3001Qt\u3001Pytorch</p>"},{"location":"#experience","title":"Experience","text":""},{"location":"#awards","title":"Awards","text":"<ul> <li>2022 \u5e74<ul> <li>\u4e2d\u56fd\u673a\u5668\u4eba\u4e0e\u4eba\u5de5\u667a\u80fd\u5927\u8d5b, \u4eba\u5de5\u667a\u80fd\u521b\u65b0\u8d5b\u9053, \u56fd\u5bb6\u4e09\u7b49\u5956</li> </ul> </li> <li>2023 \u5e74<ul> <li>\u4e2d\u56fd\u5927\u5b66\u751f\u670d\u52a1\u5916\u5305\u521b\u65b0\u521b\u4e1a\u5927\u8d5b\uff0c\u533a\u57df\u8d5b\u4e09\u7b49\u5956</li> <li>\u5168\u56fd\u5927\u5b66\u751f\u4fe1\u606f\u5b89\u5168\u7ade\u8d5b\uff0c\u533a\u57df\u8d5b\u4e09\u7b49\u5956\uff0833/80\uff09</li> <li>\u4e2d\u56fd\u5927\u5b66\u751f\u8ba1\u7b97\u673a\u8bbe\u8ba1\u5927\u8d5b\uff0c\u5927\u6570\u636e\u5e94\u7528\u8d5b\u9053\uff0c\u56fd\u5bb6\u4e09\u7b49\u5956</li> </ul> </li> </ul>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/","title":"Diffusion \u6e90\u7801\u89e3\u6790(Pytorch)","text":"<p>\u8be5 Blog \u662f\u5bf9 DDPM \u5bf9\u56fe\u7247\u6570\u636e\u548c\u4e00\u7ef4\u6570\u636e\u4ee3\u7801\u7684\u89e3\u8bfb\u6bd4\u8f83\u3002</p> <p>\u4ee3\u7801\u4ed3\u5e93\uff1a</p> <p>https://github.com/lucidrains/denoising-diffusion-pytorch/tree/main</p> <p>\u53c2\u8003\u6587\u732e/\u6587\u7ae0\uff1a</p> <ul> <li>https://arxiv.org/abs/2006.11239</li> <li>https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</li> <li>https://huggingface.co/blog/annotated-diffusion</li> </ul>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#\u8fd0\u884c\u65b9\u6cd5","title":"\u8fd0\u884c\u65b9\u6cd5","text":""},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#\u4e00\u7ef4","title":"\u4e00\u7ef4","text":"<pre><code>from denoising_diffusion_pytorch import Unet1D, GaussianDiffusion1D, Trainer1D, Dataset1D\n\nmodel = Unet1D(\n    dim = 64,\n    dim_mults = (1, 2, 4, 8),\n    channels = 32\n)\n\ndiffusion = GaussianDiffusion1D(\n    model,\n    seq_length = 128,\n    timesteps = 1000,\n    objective = 'pred_v'\n)\n\ntraining_seq = torch.rand(64, 32, 128) # features are normalized from 0 to 1\ndataset = Dataset1D(training_seq)  # this is just an example, but you can formulate your own Dataset and pass it into the `Trainer1D` below\n\nloss = diffusion(training_seq)\nloss.backward()\n\n# Or using trainer\n\ntrainer = Trainer1D(\n    diffusion,\n    dataset = dataset,\n    train_batch_size = 32,\n    train_lr = 8e-5,\n    train_num_steps = 700000,         # total training steps\n    gradient_accumulate_every = 2,    # gradient accumulation steps\n    ema_decay = 0.995,                # exponential moving average decay\n    amp = True,                       # turn on mixed precision\n)\ntrainer.train()\n\n# after a lot of training\n\nsampled_seq = diffusion.sample(batch_size = 4)\nsampled_seq.shape # (4, 32, 128)\n</code></pre>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#\u591a\u7ef4","title":"\u591a\u7ef4","text":"<pre><code>from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n\nmodel = Unet(\n    dim = 64,\n    dim_mults = (1, 2, 4, 8),\n    flash_attn = True\n)\n\ndiffusion = GaussianDiffusion(\n    model,\n    image_size = 128,\n    timesteps = 1000,           # number of steps\n    sampling_timesteps = 250    # number of sampling timesteps (using ddim for faster inference [see citation for ddim paper])\n)\n\ntrainer = Trainer(\n    diffusion,\n    'path/to/your/images',\n    train_batch_size = 32,\n    train_lr = 8e-5,\n    train_num_steps = 700000,         # total training steps\n    gradient_accumulate_every = 2,    # gradient accumulation steps\n    ema_decay = 0.995,                # exponential moving average decay\n    amp = True,                       # turn on mixed precision\n    calculate_fid = True              # whether to calculate fid during training\n)\n\ntrainer.train()\n</code></pre>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#1\u5bfc\u5165\u76f8\u5173\u5e93","title":"1\u3001\u5bfc\u5165\u76f8\u5173\u5e93","text":"<pre><code>import math  \nfrom pathlib import Path  \nfrom random import random \nfrom tqdm.auto import tqdm\n\nfrom ema_pytorch import EMA\nfrom functools import partial  \nfrom accelerate import Accelerator\nfrom collections import namedtuple  \nfrom multiprocessing import cpu_count  \n\nimport torch  \nfrom torch import nn, einsum, Tensor  \nimport torch.nn.functional as F  \nfrom torch.cuda.amp import autocast  \nfrom torch.optim import Adam  \nfrom torch.utils.data import Dataset, DataLoader  \n\nfrom einops import rearrange, reduce  \nfrom einops.layers.torch import Rearrange  \n</code></pre> <p>\u5bfc\u5165\u5e93\u7684\u529f\u80fd\u5982\u4e0b\u6240\u793a\uff1a</p> <ul> <li><code>math</code>: \u7528\u4e8e\u6570\u5b66\u8fd0\u7b97\u3002</li> <li><code>pathlib.Path</code>: \u7528\u4e8e\u5904\u7406\u6587\u4ef6\u548c\u76ee\u5f55\u8def\u5f84\u7684\u5bf9\u8c61\u3002</li> <li><code>random.random</code>: \u7528\u4e8e\u751f\u62100\u52301\u4e4b\u95f4\u7684\u968f\u673a\u6570\u3002</li> <li> <p><code>tqdm</code>: \u7528\u4e8e\u5728\u5faa\u73af\u4e2d\u663e\u793a\u8fdb\u5ea6\u6761\u3002</p> </li> <li> <p><code>ema_pytorch</code>: \u7528\u4e8e\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08EMA\uff09\u7684PyTorch\u6269\u5c55\u3002</p> </li> <li><code>functools.partial</code>: \u7528\u4e8e\u521b\u5efa\u504f\u51fd\u6570\u3002</li> <li><code>accelerate</code>: \u52a0\u901f\u8bad\u7ec3\uff0c\u7528\u4e8e\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002</li> <li><code>collections.namedtuple</code>: \u7528\u4e8e\u521b\u5efa\u547d\u540d\u5143\u7ec4\u3002</li> <li> <p><code>multiprocessing.cpu_count</code>: \u7528\u4e8e\u83b7\u53d6\u8ba1\u7b97\u673a\u7684CPU\u6838\u5fc3\u6570\u3002</p> </li> <li> <p><code>torch</code>: PyTorch\u7684\u6839\u6a21\u5757\u3002</p> </li> <li><code>torch.nn</code>: \u795e\u7ecf\u7f51\u7edc\u6a21\u5757\uff0c\u5305\u542b\u4e86\u5404\u79cd\u795e\u7ecf\u7f51\u7edc\u5c42\u548c\u51fd\u6570\u3002</li> <li><code>torch.cuda.amp.autocast</code>: \u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u6a21\u5757\u3002</li> <li><code>torch.optim.Adam</code>: Adam\u4f18\u5316\u5668\u3002</li> <li><code>torch.utils.data.Dataset</code>: \u6570\u636e\u96c6\u7c7b\uff0c\u7528\u4e8e\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u3002</li> <li> <p><code>torch.utils.data.DataLoader</code>: \u6570\u636e\u52a0\u8f7d\u5668\uff0c\u7528\u4e8e\u6279\u91cf\u52a0\u8f7d\u6570\u636e\u3002</p> </li> <li> <p><code>einops</code>:\u63d0\u4f9b\u4e86\u7b80\u6d01\u7684\u65b9\u5f0f\u5bf9PyTorch\u5f20\u91cf\u8fdb\u884c\u91cd\u7ec4\u548c\u64cd\u4f5c\u3002</p> <ul> <li><code>rearrange</code> \u51fd\u6570\u7528\u4e8e\u91cd\u65b0\u6392\u5217\u5f20\u91cf\u7684\u7ef4\u5ea6\uff1b</li> <li><code>reduce</code> \u51fd\u6570\u7528\u4e8e\u6cbf\u7740\u6307\u5b9a\u7684\u7ef4\u5ea6\u8fdb\u884c\u7f29\u51cf\u64cd\u4f5c\uff08\u5982\u6c42\u548c\u3001\u6c42\u5e73\u5747\u7b49\uff09\uff1b</li> <li><code>Rearrange</code> \u7c7b\u662f <code>einops</code> \u63d0\u4f9b\u7684\u4e00\u4e2aPyTorch\u5c42\uff0c\u7528\u4e8e\u5728PyTorch\u6a21\u578b\u4e2d\u91cd\u65b0\u6392\u5217\u5f20\u91cf\u7684\u7ef4\u5ea6\u3002</li> </ul> </li> </ul>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#2-dataset","title":"2\u3001 Dataset","text":""},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#\u4e00\u7ef4_1","title":"\u4e00\u7ef4","text":"<pre><code>class Dataset1D(Dataset):  \n    def __init__(self, tensor: Tensor):  \n        super().__init__()  \n        self.tensor = tensor.clone()  \n\n    def __len__(self):  \n        return len(self.tensor)  \n\n    def __getitem__(self, idx):  \n        return self.tensor[idx].clone()\n</code></pre> <p>\u8be5\u4ee3\u7801\u5b9a\u4e49\u4e86\u4e00\u4e2a\u4e00\u7ef4\u7684\u6570\u636e\u96c6\u7c7b\uff0c\u7528\u4e8e\u5904\u7406\u4e00\u7ef4\u6570\u636e\u3002\u5177\u4f53\u6765\u8bf4\uff1a</p> <ul> <li> <p><code>__init__</code>\uff1a\u8fd9\u662f\u7c7b\u7684\u521d\u59cb\u5316\u51fd\u6570\uff0c\u5b83\u63a5\u6536\u4e00\u4e2aTensor\u4f5c\u4e3a\u53c2\u6570\uff0c\u5e76\u5c06\u5176\u514b\u9686\u4fdd\u5b58\u5728self.tensor\u4e2d\u3002\u8fd9\u6837\u505a\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u907f\u514d\u5728\u539f\u59cb\u6570\u636e\u4e0a\u8fdb\u884c\u4fee\u6539\u3002</p> </li> <li> <p><code>__len__</code>\uff1a\u8fd9\u4e2a\u65b9\u6cd5\u8fd4\u56de\u6570\u636e\u96c6\u7684\u5927\u5c0f\uff0c\u4e5f\u5c31\u662f\u4e00\u7ef4\u6570\u636e\u7684\u957f\u5ea6\u3002\u5728PyTorch\u7684\u6570\u636e\u52a0\u8f7d\u5668\uff08DataLoader\uff09\u4e2d\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u88ab\u7528\u6765\u786e\u5b9a\u6bcf\u4e2aepoch\u7684\u8fed\u4ee3\u6b21\u6570\u3002</p> </li> <li> <p><code>__getitem__</code>\uff1a\u8fd9\u4e2a\u65b9\u6cd5\u63a5\u6536\u4e00\u4e2a\u7d22\u5f15\uff0c\u8fd4\u56de\u5bf9\u5e94\u7d22\u5f15\u7684\u6570\u636e\u3002\u5728PyTorch\u7684\u6570\u636e\u52a0\u8f7d\u5668\uff08DataLoader\uff09\u4e2d\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u88ab\u7528\u6765\u6309\u9700\u52a0\u8f7d\u6570\u636e\uff0c\u8fd9\u6837\u53ef\u4ee5\u8282\u7701\u5185\u5b58\uff0c\u63d0\u9ad8\u6570\u636e\u52a0\u8f7d\u7684\u6548\u7387\u3002</p> </li> </ul>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#\u591a\u7ef4_1","title":"\u591a\u7ef4","text":"<pre><code>class Dataset(Dataset):\n    def __init__(\n        self,\n        folder,\n        image_size,\n        exts = ['jpg', 'jpeg', 'png', 'tiff'],\n        augment_horizontal_flip = False,\n        convert_image_to = None\n    ):\n        super().__init__()\n        self.folder = folder\n        self.image_size = image_size\n        self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n\n        maybe_convert_fn = partial(convert_image_to_fn, convert_image_to) if exists(convert_image_to) else nn.Identity()\n\n        self.transform = T.Compose([\n            T.Lambda(maybe_convert_fn),\n            T.Resize(image_size),\n            T.RandomHorizontalFlip() if augment_horizontal_flip else nn.Identity(),\n            T.CenterCrop(image_size),\n            T.ToTensor()\n        ])\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, index):\n        path = self.paths[index]\n        img = Image.open(path)\n        return self.transform(img)\n</code></pre> <p>\u76f8\u5bf9\u4e8e\u5904\u7406\u4e00\u7ef4\u6570\u636e\u7684 Dataset\uff0c\u8be5\u6570\u636e\u96c6\u7528\u4e8e\u5904\u7406\u56fe\u7247\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8fd9\u4e2a\u7c7b\u7684\u8f93\u5165\u662f\u4e00\u4e2a\u56fe\u7247\u6587\u4ef6\u5939\u7684\u8def\u5f84\uff0c\u800c\u4e0d\u662f\u4e00\u4e2aTensor\u3002\u5b83\u4f1a\u641c\u7d22\u8fd9\u4e2a\u6587\u4ef6\u5939\u4e2d\u7684\u6240\u6709\u56fe\u7247\u6587\u4ef6\uff0c\u5e76\u5c06\u5b83\u4eec\u7684\u8def\u5f84\u4fdd\u5b58\u5728self.paths\u4e2d\uff1b\u540c\u65f6\uff0c\u8be5\u7c7b\u5728 <code>__init__</code> \u4e2d\u5b9a\u4e49\u4e86\u4e00\u4e2a\u6570\u636e\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u8fd9\u4e2a\u6d41\u7a0b\u5305\u62ec\u56fe\u7247\u683c\u5f0f\u8f6c\u6362\u3001\u56fe\u7247\u5927\u5c0f\u8c03\u6574\u3001\u968f\u673a\u6c34\u5e73\u7ffb\u8f6c\u3001\u4e2d\u5fc3\u88c1\u526a\u548c\u8f6c\u6362\u4e3aTensor\u3002\u8fd9\u4e2a\u9884\u5904\u7406\u6d41\u7a0b\u4f1a\u5728<code>__getitem__</code>\u65b9\u6cd5\u4e2d\u88ab\u5e94\u7528\u5230\u6bcf\u4e00\u5f20\u56fe\u7247\u4e0a\u3002</p> <p><code>__getitem__</code>\u65b9\u6cd5\u6700\u540e\u8fd4\u56de\u7684Tensor\u7684\u5f62\u72b6\u5e94\u8be5\u662f<code>(C, H, W)</code>\uff0c\u5176\u4e2d\uff1a</p> <ul> <li><code>C</code> \u662f\u901a\u9053\u6570\uff0c\u5bf9\u4e8e\u5f69\u8272\u56fe\u7247\uff0c\u901a\u9053\u6570\u901a\u5e38\u662f3\uff08RGB\uff09\uff1b\u5bf9\u4e8e\u7070\u5ea6\u56fe\u7247\uff0c\u901a\u9053\u6570\u662f1\u3002</li> <li><code>H</code> \u662f\u56fe\u7247\u7684\u9ad8\u5ea6\uff0c\u8fd9\u4e2a\u503c\u7531<code>image_size</code>\u53c2\u6570\u51b3\u5b9a\uff0c\u8fd9\u4e2a\u53c2\u6570\u5728\u521d\u59cb\u5316\u6570\u636e\u96c6\u7c7b\u65f6\u88ab\u4f20\u5165\u3002</li> <li><code>W</code> \u662f\u56fe\u7247\u7684\u5bbd\u5ea6\uff0c\u8fd9\u4e2a\u503c\u4e5f\u7531<code>image_size</code>\u53c2\u6570\u51b3\u5b9a\u3002</li> </ul>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#3trainer","title":"3\u3001Trainer","text":""},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#part-1","title":"Part 1","text":""},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#\u4e00\u7ef4_2","title":"\u4e00\u7ef4","text":"<pre><code>class Trainer1D(object):\n    def __init__(\n        self,\n        diffusion_model: GaussianDiffusion1D,\n        dataset: Dataset,\n        *,\n        train_batch_size = 16,\n        gradient_accumulate_every = 1,\n        train_lr = 1e-4,\n        train_num_steps = 100000,\n        ema_update_every = 10,\n        ema_decay = 0.995,\n        adam_betas = (0.9, 0.99),\n        save_and_sample_every = 1000,\n        num_samples = 25,\n        results_folder = './results',\n        amp = False,\n        mixed_precision_type = 'fp16',\n        split_batches = True,\n    ):\n        super().__init__()\n\n        # accelerator\n        self.accelerator = Accelerator(\n            split_batches = split_batches,\n            mixed_precision = mixed_precision_type if amp else 'no'\n        )\n\n        # model\n        self.model = diffusion_model\n        self.channels = diffusion_model.channels\n\n        # sampling and training hyperparameters\n        assert has_int_squareroot(num_samples), 'number of samples must have an integer square root'\n        self.num_samples = num_samples\n        self.save_and_sample_every = save_and_sample_every\n\n        self.batch_size = train_batch_size\n        self.gradient_accumulate_every = gradient_accumulate_every\n\n        self.train_num_steps = train_num_steps\n\n        # dataset and dataloader\n        dl = DataLoader(dataset, batch_size = train_batch_size, shuffle = True, pin_memory = True, num_workers = cpu_count())\n        dl = self.accelerator.prepare(dl)\n        self.dl = cycle(dl)\n\n        # optimizer\n        self.opt = Adam(diffusion_model.parameters(), lr = train_lr, betas = adam_betas)\n\n        # for logging results in a folder periodically\n        if self.accelerator.is_main_process:\n            self.ema = EMA(diffusion_model, beta = ema_decay, update_every = ema_update_every)\n            self.ema.to(self.device)\n\n        self.results_folder = Path(results_folder)\n        self.results_folder.mkdir(exist_ok = True)\n\n        # step counter state\n        self.step = 0\n\n        # prepare model, dataloader, optimizer with accelerator\n        self.model, self.opt = self.accelerator.prepare(self.model, self.opt)\n</code></pre> <p>\u8fd9\u4e2a\u521d\u59cb\u5316\u51fd\u6570\u63a5\u53d7\u4e86\u5f88\u591a\u53c2\u6570\uff0c\u4e0b\u9762\u662f\u6bcf\u4e2a\u53c2\u6570\u7684\u542b\u4e49\uff1a</p> <p><code>diffusion_model</code>: GaussianDiffusion1D\u6a21\u578b\u5bf9\u8c61\uff0c\u5b83\u662f\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9700\u8981\u4f18\u5316\u7684\u6a21\u578b\u3002</p> <p><code>dataset</code>: \u6570\u636e\u96c6\u5bf9\u8c61\uff0c\u5b83\u63d0\u4f9b\u4e86\u8bad\u7ec3\u6a21\u578b\u6240\u9700\u7684\u6570\u636e\u3002</p> <p><code>train_batch_size</code>: \u6574\u6570\uff0c\u8868\u793a\u6bcf\u4e2a\u8bad\u7ec3\u6279\u6b21\u7684\u5927\u5c0f\u3002</p> <p><code>gradient_accumulate_every</code>: \u6574\u6570\uff0c\u8868\u793a\u6bcf\u9694\u591a\u5c11\u6b65\u8fdb\u884c\u4e00\u6b21\u68af\u5ea6\u7d2f\u79ef\u3002</p> <p><code>train_lr</code>: \u6d6e\u70b9\u6570\uff0c\u8868\u793a\u8bad\u7ec3\u7684\u5b66\u4e60\u7387\u3002</p> <p><code>train_num_steps</code>: \u6574\u6570\uff0c\u8868\u793a\u8bad\u7ec3\u7684\u603b\u6b65\u6570\u3002</p> <p><code>ema_update_every</code>: \u6574\u6570\uff0c\u8868\u793a\u6bcf\u9694\u591a\u5c11\u6b65\u66f4\u65b0\u4e00\u6b21\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08EMA\uff09\u3002</p> <p><code>ema_decay</code>: \u6d6e\u70b9\u6570\uff0c\u8868\u793aEMA\u7684\u8870\u51cf\u7387\u3002</p> <p><code>adam_betas</code>: \u5143\u7ec4\uff0c\u8868\u793aAdam\u4f18\u5316\u5668\u7684beta\u53c2\u6570\u3002</p> <p><code>save_and_sample_every</code>: \u6574\u6570\uff0c\u8868\u793a\u6bcf\u9694\u591a\u5c11\u6b65\u4fdd\u5b58\u4e00\u6b21\u6a21\u578b\u5e76\u751f\u6210\u6837\u672c\u3002</p> <p><code>num_samples</code>: \u6574\u6570\uff0c\u8868\u793a\u6bcf\u6b21\u751f\u6210\u6837\u672c\u7684\u6570\u91cf\u3002</p> <p><code>results_folder</code>: \u5b57\u7b26\u4e32\uff0c\u8868\u793a\u4fdd\u5b58\u7ed3\u679c\u7684\u6587\u4ef6\u5939\u8def\u5f84\u3002</p> <p><code>amp</code>: \u5e03\u5c14\u503c\uff0c\u8868\u793a\u662f\u5426\u4f7f\u7528\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\uff08AMP\uff09\u8bad\u7ec3\u3002</p> <p><code>mixed_precision_type</code>: \u5b57\u7b26\u4e32\uff0c\u8868\u793a\u6df7\u5408\u7cbe\u5ea6\u7684\u7c7b\u578b\uff0c\u53ef\u80fd\u7684\u503c\u662f'fp16'\u6216'fp32'\u3002</p> <p><code>split_batches</code>: \u5e03\u5c14\u503c\uff0c\u8868\u793a\u662f\u5426\u5c06\u6279\u6b21\u5206\u5272\u5230\u591a\u4e2a\u8bbe\u5907\u4e0a\u3002</p> <p>\u5728\u521d\u59cb\u5316\u51fd\u6570\u4e2d\uff0c\u9996\u5148\u521d\u59cb\u5316\u4e86\u4e00\u4e2a<code>Accelerator</code>\u5bf9\u8c61\uff0c\u8fd9\u4e2a\u5bf9\u8c61\u7528\u4e8e\u7ba1\u7406\u6a21\u578b\u7684\u8bbe\u5907\u5206\u914d\u548c\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u3002\u7136\u540e\uff0c\u521d\u59cb\u5316\u4e86\u6a21\u578b\u3001\u8bad\u7ec3\u53c2\u6570\u3001\u6570\u636e\u52a0\u8f7d\u5668\u548c\u4f18\u5316\u5668\u3002\u5982\u679c\u5f53\u524d\u8fdb\u7a0b\u662f\u4e3b\u8fdb\u7a0b\uff0c\u8fd8\u4f1a\u521d\u59cb\u5316\u4e00\u4e2aEMA\u5bf9\u8c61\uff0c\u7528\u4e8e\u8ddf\u8e2a\u6a21\u578b\u7684\u79fb\u52a8\u5e73\u5747\u3002\u6700\u540e\uff0c\u51c6\u5907\u4e86\u6a21\u578b\u3001\u6570\u636e\u52a0\u8f7d\u5668\u548c\u4f18\u5316\u5668\uff0c\u4ee5\u9002\u5e94<code>Accelerator</code>\u7684\u8bbe\u7f6e\u3002</p>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#\u591a\u7ef4_2","title":"\u591a\u7ef4","text":"<pre><code>class Trainer(object):\n    def __init__(\n        self,\n        diffusion_model,\n        folder,\n        *,\n        train_batch_size = 16,\n        gradient_accumulate_every = 1,\n        augment_horizontal_flip = True,\n        train_lr = 1e-4,\n        train_num_steps = 100000,\n        ema_update_every = 10,\n        ema_decay = 0.995,\n        adam_betas = (0.9, 0.99),\n        save_and_sample_every = 1000,\n        num_samples = 25,\n        results_folder = './results',\n        amp = False,\n        mixed_precision_type = 'fp16',\n        split_batches = True,\n        convert_image_to = None,\n        calculate_fid = True,\n        inception_block_idx = 2048,\n        max_grad_norm = 1.,\n        num_fid_samples = 50000,\n        save_best_and_latest_only = False\n    ):\n        super().__init__()\n\n        # accelerator\n\n        self.accelerator = Accelerator(\n            split_batches = split_batches,\n            mixed_precision = mixed_precision_type if amp else 'no'\n        )\n\n        # model\n\n        self.model = diffusion_model\n        self.channels = diffusion_model.channels\n\n        # sampling and training hyperparameters\n\n        assert has_int_squareroot(num_samples), 'number of samples must have an integer square root'\n        self.num_samples = num_samples\n        self.save_and_sample_every = save_and_sample_every\n\n        self.batch_size = train_batch_size\n        self.gradient_accumulate_every = gradient_accumulate_every\n        assert (train_batch_size * gradient_accumulate_every) &gt;= 16, f'your effective batch size (train_batch_size x gradient_accumulate_every) should be at least 16 or above'\n\n        self.train_num_steps = train_num_steps\n        self.image_size = diffusion_model.image_size\n\n        self.max_grad_norm = max_grad_norm\n\n        # dataset and dataloader\n\n        self.ds = Dataset(folder, self.image_size, augment_horizontal_flip = augment_horizontal_flip, convert_image_to = convert_image_to)\n\n        assert len(self.ds) &gt;= 100, 'you should have at least 100 images in your folder. at least 10k images recommended'\n\n        dl = DataLoader(self.ds, batch_size = train_batch_size, shuffle = True, pin_memory = True, num_workers = cpu_count())\n\n        dl = self.accelerator.prepare(dl)\n        self.dl = cycle(dl)\n\n        # optimizer\n\n        self.opt = Adam(diffusion_model.parameters(), lr = train_lr, betas = adam_betas)\n\n        # for logging results in a folder periodically\n        if self.accelerator.is_main_process:\n            self.ema = EMA(diffusion_model, beta = ema_decay, update_every = ema_update_every)\n            self.ema.to(self.device)\n\n        self.results_folder = Path(results_folder)\n        self.results_folder.mkdir(exist_ok = True)\n\n        # step counter state\n        self.step = 0\n\n        # prepare model, dataloader, optimizer with accelerator\n        self.model, self.opt = self.accelerator.prepare(self.model, self.opt)\n\n        # FID-score computation\n        self.calculate_fid = calculate_fid and self.accelerator.is_main_process\n\n        if self.calculate_fid:\n            if not self.model.is_ddim_sampling:\n                self.accelerator.print(\n                    \"WARNING: Robust FID computation requires a lot of generated samples and can therefore be very time consuming.\"\\\n                    \"Consider using DDIM sampling to save time.\"\n                )\n            self.fid_scorer = FIDEvaluation(\n                batch_size=self.batch_size,\n                dl=self.dl,\n                sampler=self.ema.ema_model,\n                channels=self.channels,\n                accelerator=self.accelerator,\n                stats_dir=results_folder,\n                device=self.device,\n                num_fid_samples=num_fid_samples,\n                inception_block_idx=inception_block_idx\n            )\n\n        if save_best_and_latest_only:\n            assert calculate_fid, \"`calculate_fid` must be True to provide a means for model evaluation for `save_best_and_latest_only`.\"\n            self.best_fid = 1e10 # infinite\n\n        self.save_best_and_latest_only = save_best_and_latest_only\n</code></pre> <p>\u4e0e\u4e0a\u4e00\u4e2a<code>__init__</code>\u51fd\u6570\u76f8\u6bd4\uff0c\u8fd9\u4e2a\u51fd\u6570\u591a\u4e86\u4e00\u4e9b\u53c2\u6570\uff0c\u4e3b\u8981\u662f\u5173\u4e8e\u6570\u636e\u589e\u5f3a\u3001\u56fe\u50cf\u683c\u5f0f\u8f6c\u6362\u548cFID\u8bc4\u5206\u7684\u53c2\u6570\u3002\u8fd9\u4e9b\u53c2\u6570\u4f7f\u5f97\u8fd9\u4e2a\u8bad\u7ec3\u5668\u66f4\u9002\u5408\u5904\u7406\u56fe\u50cf\u6570\u636e\u3002\u4e0d\u540c/\u589e\u52a0\u7684\u53c2\u6570\u5982\u4e0b\uff1a</p> <p><code>folder</code>: \u5b57\u7b26\u4e32\uff0c\u8868\u793a\u6570\u636e\u96c6\u7684\u6587\u4ef6\u5939\u8def\u5f84\u3002</p> <p><code>augment_horizontal_flip</code>: \u5e03\u5c14\u503c\uff0c\u8868\u793a\u662f\u5426\u5bf9\u56fe\u50cf\u8fdb\u884c\u6c34\u5e73\u7ffb\u8f6c\u7684\u6570\u636e\u589e\u5f3a\u3002</p> <p><code>convert_image_to</code>: \u5b57\u7b26\u4e32\uff0c\u8868\u793a\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u4f55\u79cd\u683c\u5f0f\uff0c\u53ef\u80fd\u7684\u503c\u662f'RGB'\u6216'YCbCr'\u3002</p> <p><code>calculate_fid</code>: \u5e03\u5c14\u503c\uff0c\u8868\u793a\u662f\u5426\u8ba1\u7b97Frechet Inception Distance\uff08FID\uff09\u8bc4\u5206\u3002</p> <p><code>inception_block_idx</code>: \u6574\u6570\uff0c\u8868\u793a\u7528\u4e8e\u8ba1\u7b97FID\u8bc4\u5206\u7684Inception\u6a21\u578b\u7684\u54ea\u4e2a\u5757\u7684\u8f93\u51fa\u3002</p> <p><code>max_grad_norm</code>: \u6d6e\u70b9\u6570\uff0c\u8868\u793a\u68af\u5ea6\u88c1\u526a\u7684\u9608\u503c\u3002</p> <p><code>num_fid_samples</code>: \u6574\u6570\uff0c\u8868\u793a\u7528\u4e8e\u8ba1\u7b97FID\u8bc4\u5206\u7684\u6837\u672c\u6570\u91cf\u3002</p> <p><code>save_best_and_latest_only</code>: \u5e03\u5c14\u503c\uff0c\u8868\u793a\u662f\u5426\u53ea\u4fdd\u5b58\u6700\u597d\u548c\u6700\u65b0\u7684\u6a21\u578b\u3002</p>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#part-2","title":"Part 2","text":"<pre><code>    @property\n    def device(self):\n        return self.accelerator.device\n\n    def save(self, milestone):\n        if not self.accelerator.is_local_main_process:\n            return\n\n        data = {\n            'step': self.step,\n            'model': self.accelerator.get_state_dict(self.model),\n            'opt': self.opt.state_dict(),\n            'ema': self.ema.state_dict(),\n            'scaler': self.accelerator.scaler.state_dict() if exists(self.accelerator.scaler) else None,\n            'version': __version__\n        }\n\n        torch.save(data, str(self.results_folder / f'model-{milestone}.pt'))\n\n    def load(self, milestone):\n        accelerator = self.accelerator\n        device = accelerator.device\n\n        data = torch.load(str(self.results_folder / f'model-{milestone}.pt'), map_location=device)\n\n        model = self.accelerator.unwrap_model(self.model)\n        model.load_state_dict(data['model'])\n\n        self.step = data['step']\n        self.opt.load_state_dict(data['opt'])\n        if self.accelerator.is_main_process:\n            self.ema.load_state_dict(data[\"ema\"])\n\n        if 'version' in data:\n            print(f\"loading from version {data['version']}\")\n\n        if exists(self.accelerator.scaler) and exists(data['scaler']):\n            self.accelerator.scaler.load_state_dict(data['scaler'])\n</code></pre> <p><code>device</code>\uff1a\u8fd4\u56de\u8bad\u7ec3\u5668\u6b63\u5728\u4f7f\u7528\u7684\u8bbe\u5907\uff0cCPU\u6216\u8005GPU\u3002</p> <p><code>save</code>\uff1a\u4fdd\u5b58\u8bad\u7ec3\u5668\u7684\u72b6\u6001\uff0c\u5305\u62ec\u6a21\u578b\u7684\u53c2\u6570\u3001\u4f18\u5316\u5668\u7684\u72b6\u6001\u3001\u8bad\u7ec3\u6b65\u6570\u7b49\u3002\u51fd\u6570\u5728\u6bcf\u4e2a\u8bad\u7ec3\u5468\u671f\u7ed3\u675f\u65f6\u88ab\u8c03\u7528\uff0c\u7528\u4e8e\u4fdd\u5b58\u8bad\u7ec3\u7684\u8fdb\u5ea6\uff0c\u4ee5\u4fbf\u5728\u9700\u8981\u65f6\u6062\u590d\u8bad\u7ec3\u3002</p> <p><code>load</code>\uff1a\u52a0\u8f7d\u4fdd\u5b58\u7684\u8bad\u7ec3\u5668\u72b6\u6001\uff0c\u5305\u62ec\u6a21\u578b\u7684\u53c2\u6570\u3001\u4f18\u5316\u5668\u7684\u72b6\u6001\u3001\u8bad\u7ec3\u6b65\u6570\u7b49\u3002\u51fd\u6570\u5728\u8bad\u7ec3\u5f00\u59cb\u65f6\u88ab\u8c03\u7528\uff0c\u7528\u4e8e\u4ece\u4fdd\u5b58\u7684\u72b6\u6001\u6062\u590d\u8bad\u7ec3\u3002</p> <p>\uff08\u4e00\u7ef4\u4e0e\u591a\u7ef4\u57fa\u672c\u4e00\u81f4\uff09</p>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#part-3","title":"Part 3","text":"<pre><code>    def train(self):\n        accelerator = self.accelerator\n        device = accelerator.device\n\n        with tqdm(initial = self.step, total = self.train_num_steps, disable = not accelerator.is_main_process) as pbar:\n\n            while self.step &lt; self.train_num_steps:\n\n                total_loss = 0.\n\n                for _ in range(self.gradient_accumulate_every):\n                    data = next(self.dl).to(device)\n\n                    with self.accelerator.autocast():\n                        loss = self.model(data)\n                        loss = loss / self.gradient_accumulate_every\n                        total_loss += loss.item()\n\n                    self.accelerator.backward(loss)\n\n                accelerator.clip_grad_norm_(self.model.parameters(), 1.0)\n                pbar.set_description(f'loss: {total_loss:.4f}')\n\n                accelerator.wait_for_everyone()\n\n                self.opt.step()\n                self.opt.zero_grad()\n\n                accelerator.wait_for_everyone()\n\n                self.step += 1\n                if accelerator.is_main_process:\n                    self.ema.update()\n\n                    # \u5224\u65ad\u662f\u5426\u8fbe\u5230\u4e86\u4fdd\u5b58\u548c\u91c7\u6837\u7684\u6b65\u6570\u3002\u5982\u679c\u8fbe\u5230\u4e86\uff0c\u5c31\u751f\u6210\u6837\u672c\u5e76\u4fdd\u5b58\u6a21\u578b\n                    if self.step != 0 and self.step % self.save_and_sample_every == 0:\n                        self.ema.ema_model.eval()\n\n                        with torch.no_grad():\n                            milestone = self.step // self.save_and_sample_every\n                            batches = num_to_groups(self.num_samples, self.batch_size)\n                            all_samples_list = list(map(lambda n: self.ema.ema_model.sample(batch_size=n), batches))\n\n                        all_samples = torch.cat(all_samples_list, dim = 0)\n\n                        torch.save(all_samples, str(self.results_folder / f'sample-{milestone}.png'))\n                        self.save(milestone)\n\n                pbar.update(1)\n\n        accelerator.print('training complete')\n</code></pre> <p><code>train</code>\uff1a\u8bad\u7ec3\u5668\u7684\u4e3b\u8981\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002\u51fd\u6570\u5728\u6bcf\u4e2a\u8bad\u7ec3\u5468\u671f\u4e2d\u88ab\u8c03\u7528\uff0c\u7528\u4e8e\u6267\u884c\u6a21\u578b\u7684\u524d\u5411\u9636\u6bb5\u548c\u540e\u5411\u9636\u6bb5\uff0c\u4ee5\u53ca\u53c2\u6570\u7684\u66f4\u65b0\u3002\u5728\u51fd\u6570\u4e2d\uff0c\u6a21\u578b\u7684\u524d\u5411\u9636\u6bb5\u662f\u5728<code>loss = self.model(data)</code>\u8fd9\u884c\u4ee3\u7801\u4e2d\u8fdb\u884c\u7684\uff0c\u6a21\u578b\u7684\u540e\u5411\u9636\u6bb5\u662f\u5728<code>self.accelerator.backward(loss)</code>\u8fd9\u884c\u4ee3\u7801\u4e2d\u8fdb\u884c\u7684\u3002\u6700\u540e\uff0c\u53c2\u6570\u7684\u66f4\u65b0\u662f\u5728<code>self.opt.step()</code>\u8fd9\u884c\u4ee3\u7801\u4e2d\u8fdb\u884c\u7684\u3002\u5728<code>train</code>\u51fd\u6570\u7684\u6700\u540e\uff0c\u5f53\u8fbe\u5230\u4fdd\u5b58\u548c\u91c7\u6837\u7684\u6b65\u6570\u65f6\uff0c\u4f1a\u8c03\u7528<code>self.ema.ema_model.sample(batch_size=n)</code>\u751f\u6210\u6837\u672c\uff0c\u5e76\u4fdd\u5b58\u5230\u6587\u4ef6\u4e2d\uff0c\u8fd9\u90e8\u5206\u5bf9\u5e94\u4e8e\u751f\u6210\u4fdd\u5b58\u7684\u751f\u6210\u6587\u4ef6\u3002</p> <pre><code>    def train(self):\n        accelerator = self.accelerator\n        device = accelerator.device\n\n        with tqdm(initial = self.step, total = self.train_num_steps, disable = not accelerator.is_main_process) as pbar:\n\n            while self.step &lt; self.train_num_steps:\n\n                total_loss = 0.\n\n                for _ in range(self.gradient_accumulate_every):\n                    data = next(self.dl).to(device)\n\n                    with self.accelerator.autocast():\n                        loss = self.model(data)\n                        loss = loss / self.gradient_accumulate_every\n                        total_loss += loss.item()\n\n                    self.accelerator.backward(loss)\n\n                accelerator.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n                pbar.set_description(f'loss: {total_loss:.4f}')\n\n                accelerator.wait_for_everyone()\n\n                self.opt.step()\n                self.opt.zero_grad()\n\n                accelerator.wait_for_everyone()\n\n                self.step += 1\n                if accelerator.is_main_process:\n                    self.ema.update()\n\n                    if self.step != 0 and divisible_by(self.step, self.save_and_sample_every):\n                        self.ema.ema_model.eval()\n\n                        with torch.inference_mode():\n                            milestone = self.step // self.save_and_sample_every\n                            batches = num_to_groups(self.num_samples, self.batch_size)\n                            all_images_list = list(map(lambda n: self.ema.ema_model.sample(batch_size=n), batches))\n\n                        all_images = torch.cat(all_images_list, dim = 0)\n\n                        utils.save_image(all_images, str(self.results_folder / f'sample-{milestone}.png'), nrow = int(math.sqrt(self.num_samples)))\n\n                        # whether to calculate fid\n                        if self.calculate_fid:\n                            fid_score = self.fid_scorer.fid_score()\n                            accelerator.print(f'fid_score: {fid_score}')\n                        if self.save_best_and_latest_only:\n                            if self.best_fid &gt; fid_score:\n                                self.best_fid = fid_score\n                                self.save(\"best\")\n                            self.save(\"latest\")\n                        else:\n                            self.save(milestone)\n\n                pbar.update(1)\n\n        accelerator.print('training complete')\n</code></pre> <p>\u4e0e\u4e00\u7ef4\u903b\u8f91\u57fa\u672c\u76f8\u540c\uff0c\u4f46\u5728\u4fdd\u5b58\u6a21\u578b\u90e8\u5206\u4fdd\u5b58\u6700\u4f18\u5206\u6570\u6a21\u578b\u548c\u6700\u65b0\u6a21\u578b\u3002\uff08\u4e3b\u8981\u662f\u56e0\u4e3a\u4e00\u7ef4\u6a21\u578b\u7f3a\u5c11\u5ea6\u91cf\u673a\u5236\uff09</p>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#4gaussiandiffusion","title":"4\u3001GaussianDiffusion","text":"<p>\u4ee3\u7801\u5927\u81f4\u8fd0\u884c\u6d41\u7a0b\uff1a</p> <ol> <li> <p><code>__init__</code>\uff1a\u521d\u59cb\u5316\u51fd\u6570\uff0c\u7528\u4e8e\u8bbe\u7f6e\u6a21\u578b\u7684\u53c2\u6570\uff0c\u8ba1\u7b97beta\u548calpha\u503c\uff0c\u521d\u59cb\u5316\u6a21\u578b\u3001\u4f18\u5316\u5668\u548c\u6570\u636e\u52a0\u8f7d\u5668\uff0c\u6839\u636e<code>objective</code>\u53c2\u6570\u8ba1\u7b97\u635f\u5931\u6743\u91cd\uff0c\u6839\u636e<code>auto_normalize</code>\u53c2\u6570\u9009\u62e9\u662f\u5426\u81ea\u52a8\u5f52\u4e00\u5316\u3002\u8fd9\u4e2a\u51fd\u6570\u5728\u521b\u5efa\u7c7b\u7684\u5b9e\u4f8b\u65f6\u8fd0\u884c\u3002</p> </li> <li> <p><code>forward</code>\uff1a\u524d\u5411\u4f20\u64ad\u51fd\u6570\uff0c\u7528\u4e8e\u8ba1\u7b97\u635f\u5931\u3002\u8fd9\u4e2a\u51fd\u6570\u5728\u8bad\u7ec3\u6a21\u578b\u65f6\u8fd0\u884c\uff0c\u5b83\u4f1a\u8c03\u7528<code>p_losses</code>\u51fd\u6570\u6765\u8ba1\u7b97\u635f\u5931\u3002</p> </li> <li> <p><code>p_losses</code>\uff1a\u8ba1\u7b97\u635f\u5931\u51fd\u6570\uff0c\u5b83\u4f1a\u751f\u6210\u566a\u58f0\uff0c\u7136\u540e\u4ece\u8d77\u59cb\u56fe\u50cf\u548c\u566a\u58f0\u4e2d\u91c7\u6837\uff0c\u5f97\u5230\u4e00\u4e2a\u65b0\u7684\u56fe\u50cf\u3002\u7136\u540e\uff0c\u6839\u636e<code>self_condition</code>\u53c2\u6570\u51b3\u5b9a\u662f\u5426\u8fdb\u884c\u81ea\u6211\u6761\u4ef6\u5316\uff0c\u7136\u540e\u8c03\u7528<code>model_predictions</code>\u51fd\u6570\u8ba1\u7b97\u6a21\u578b\u9884\u6d4b\u3002\u6700\u540e\uff0c\u6839\u636e<code>objective</code>\u53c2\u6570\u8ba1\u7b97\u635f\u5931\u3002\u8fd9\u4e2a\u51fd\u6570\u5728<code>forward</code>\u51fd\u6570\u4e2d\u88ab\u8c03\u7528\u3002</p> </li> <li> <p><code>model_predictions</code>\uff1a\u8ba1\u7b97\u6a21\u578b\u9884\u6d4b\u51fd\u6570\uff0c\u5b83\u4f1a\u6839\u636e<code>objective</code>\u53c2\u6570\u8c03\u7528\u76f8\u5e94\u7684\u51fd\u6570\uff08<code>predict_start_from_noise</code>\u3001<code>predict_noise_from_start</code>\u6216<code>predict_v</code>\uff09\u8ba1\u7b97\u6a21\u578b\u9884\u6d4b\u3002\u8fd9\u4e2a\u51fd\u6570\u5728<code>p_losses</code>\u51fd\u6570\u4e2d\u88ab\u8c03\u7528\u3002</p> </li> <li> <p><code>p_sample</code>\uff1a\u91c7\u6837\u51fd\u6570\uff0c\u7528\u4e8e\u751f\u6210\u65b0\u7684\u56fe\u50cf\u3002\u8fd9\u4e2a\u51fd\u6570\u5728\u751f\u6210\u65b0\u7684\u56fe\u50cf\u65f6\u8fd0\u884c\uff0c\u5b83\u4f1a\u88ab<code>p_sample_loop</code>\u3001<code>ddim_sample</code>\u548c<code>sample</code>\u51fd\u6570\u8c03\u7528\u3002</p> </li> <li> <p><code>p_sample_loop</code>\uff1a\u5faa\u73af\u91c7\u6837\u51fd\u6570\uff0c\u5b83\u4f1a\u5728\u4e00\u4e2a\u5faa\u73af\u4e2d\u591a\u6b21\u8c03\u7528<code>p_sample</code>\u51fd\u6570\u8fdb\u884c\u91c7\u6837\u3002\u8fd9\u4e2a\u51fd\u6570\u5728<code>sample</code>\u51fd\u6570\u4e2d\u88ab\u8c03\u7528\u3002</p> </li> <li> <p><code>ddim_sample</code>\uff1aDDIM\u91c7\u6837\u51fd\u6570\uff0c\u5b83\u4f1a\u5728\u4e00\u4e2a\u5faa\u73af\u4e2d\u591a\u6b21\u8c03\u7528<code>p_sample</code>\u51fd\u6570\u8fdb\u884cDDIM\u91c7\u6837\u3002\u8fd9\u4e2a\u51fd\u6570\u5728<code>sample</code>\u51fd\u6570\u4e2d\u88ab\u8c03\u7528\u3002</p> </li> <li> <p><code>sample</code>\uff1a\u91c7\u6837\u51fd\u6570\uff0c\u5b83\u4f1a\u6839\u636e<code>is_ddim_sampling</code>\u53c2\u6570\u9009\u62e9\u91c7\u6837\u51fd\u6570\uff08<code>p_sample_loop</code>\u6216<code>ddim_sample</code>\uff09\u3002\u8fd9\u4e2a\u51fd\u6570\u5728\u751f\u6210\u65b0\u7684\u56fe\u50cf\u65f6\u8fd0\u884c\u3002</p> </li> <li> <p><code>interpolate</code>\uff1a\u63d2\u503c\u51fd\u6570\uff0c\u5b83\u4f1a\u5728\u4e00\u4e2a\u5faa\u73af\u4e2d\u591a\u6b21\u8c03\u7528<code>p_sample</code>\u51fd\u6570\u8fdb\u884c\u63d2\u503c\u3002\u8fd9\u4e2a\u51fd\u6570\u5728\u751f\u6210\u63d2\u503c\u56fe\u50cf\u65f6\u8fd0\u884c\u3002</p> </li> <li> <p><code>q_sample</code>\uff1a\u91c7\u6837\u51fd\u6570\uff0c\u5b83\u4f1a\u4ece\u8d77\u59cb\u56fe\u50cf\u548c\u566a\u58f0\u4e2d\u91c7\u6837\uff0c\u5f97\u5230\u4e00\u4e2a\u65b0\u7684\u56fe\u50cf\u3002\u8fd9\u4e2a\u51fd\u6570\u5728<code>p_losses</code>\u548c<code>interpolate</code>\u51fd\u6570\u4e2d\u88ab\u8c03\u7528\u3002</p> </li> </ol> <p>\u5177\u4f53\u6765\u8bf4\uff0c\u5728 <code>GaussianDiffusion</code> \u7c7b\u4e2d\uff0c<code>forward</code> \u51fd\u6570\u662f\u6a21\u578b\u7684\u4e3b\u8981\u5165\u53e3\uff0c\u5b83\u8d1f\u8d23\u5728\u524d\u5411\u8fc7\u7a0b\u4e2d\u6dfb\u52a0\u566a\u58f0\uff0c\u5e76\u5728\u540e\u5411\u8fc7\u7a0b\u4e2d\u8ba1\u7b97\u635f\u5931\u3002</p> <p>\u5728\u524d\u5411\u8fc7\u7a0b\u4e2d\uff0c<code>forward</code> \u51fd\u6570\u9996\u5148\u5c06\u8f93\u5165\u56fe\u50cf\u6b63\u5219\u5316\uff0c\u7136\u540e\u8c03\u7528 <code>p_losses</code> \u51fd\u6570\u3002\u5728 <code>p_losses</code> \u51fd\u6570\u4e2d\uff0c\u9996\u5148\u751f\u6210\u4e00\u4e2a\u4e0e\u8f93\u5165\u56fe\u50cf\u5f62\u72b6\u76f8\u540c\u7684\u968f\u673a\u566a\u58f0\uff0c\u7136\u540e\u8c03\u7528 <code>q_sample</code> \u51fd\u6570\uff0c\u8be5\u51fd\u6570\u5c06\u566a\u58f0\u6dfb\u52a0\u5230\u8f93\u5165\u56fe\u50cf\u4e2d\uff0c\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u56fe\u50cf\u3002\u8fd9\u5c31\u662f\u5728\u524d\u5411\u8fc7\u7a0b\u4e2d\u6dfb\u52a0\u566a\u58f0\u7684\u90e8\u5206\u3002</p> <p>\u5728\u540e\u5411\u8fc7\u7a0b\u4e2d\uff0c<code>p_losses</code> \u51fd\u6570\u7ee7\u7eed\u6267\u884c\uff0c\u5b83\u8c03\u7528\u6a21\u578b\u7684 <code>model_predictions</code> \u51fd\u6570\u6765\u9884\u6d4b\u566a\u58f0\uff0c\u7136\u540e\u8ba1\u7b97\u9884\u6d4b\u566a\u58f0\u548c\u5b9e\u9645\u566a\u58f0\u4e4b\u95f4\u7684\u5747\u65b9\u8bef\u5dee\u635f\u5931\u3002\u8fd9\u4e2a\u635f\u5931\u88ab\u4e58\u4ee5\u4e00\u4e2a\u635f\u5931\u6743\u91cd\uff0c\u7136\u540e\u8fd4\u56de\u7ed9 <code>forward</code> \u51fd\u6570\uff0c<code>forward</code> \u51fd\u6570\u5c06\u8fd9\u4e2a\u635f\u5931\u8fd4\u56de\u7ed9\u8c03\u7528\u8005\u3002\u8fd9\u5c31\u662f\u5728\u540e\u5411\u8fc7\u7a0b\u4e2d\u8ba1\u7b97\u635f\u5931\u7684\u90e8\u5206\u3002</p>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#\u4e00\u7ef4_3","title":"\u4e00\u7ef4","text":"<pre><code>class GaussianDiffusion1D(nn.Module):\n    def __init__(\n        self,\n        model,\n        *,\n        seq_length,\n        timesteps = 1000,\n        sampling_timesteps = None,\n        objective = 'pred_noise',\n        beta_schedule = 'cosine',\n        ddim_sampling_eta = 0.,\n        auto_normalize = True\n    ):\n        super().__init__()\n        self.model = model\n        self.channels = self.model.channels\n        self.self_condition = self.model.self_condition\n\n        self.seq_length = seq_length\n\n        self.objective = objective\n\n        assert objective in {'pred_noise', 'pred_x0', 'pred_v'}, 'objective must be either pred_noise (predict noise) or pred_x0 (predict image start) or pred_v (predict v [v-parameterization as defined in appendix D of progressive distillation paper, used in imagen-video successfully])'\n\n        if beta_schedule == 'linear':\n            betas = linear_beta_schedule(timesteps)\n        elif beta_schedule == 'cosine':\n            betas = cosine_beta_schedule(timesteps)\n        else:\n            raise ValueError(f'unknown beta schedule {beta_schedule}')\n\n        alphas = 1. - betas\n        alphas_cumprod = torch.cumprod(alphas, dim=0)\n        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n\n        # sampling related parameters\n        self.sampling_timesteps = default(sampling_timesteps, timesteps) # default num sampling timesteps to number of timesteps at training\n\n        assert self.sampling_timesteps &lt;= timesteps\n        self.is_ddim_sampling = self.sampling_timesteps &lt; timesteps\n        self.ddim_sampling_eta = ddim_sampling_eta\n\n        # helper function to register buffer from float64 to float32\n        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n        register_buffer('betas', betas)\n        register_buffer('alphas_cumprod', alphas_cumprod)\n        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        register_buffer('posterior_variance', posterior_variance)\n\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))\n        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n\n        # calculate loss weight\n\n        snr = alphas_cumprod / (1 - alphas_cumprod)\n\n        if objective == 'pred_noise':\n            loss_weight = torch.ones_like(snr)\n        elif objective == 'pred_x0':\n            loss_weight = snr\n        elif objective == 'pred_v':\n            loss_weight = snr / (snr + 1)\n\n        register_buffer('loss_weight', loss_weight)\n\n        # whether to autonormalize\n\n        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity\n        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else identity\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def predict_noise_from_start(self, x_t, t, x0):\n        return (\n            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n        )\n\n    def predict_v(self, x_start, t, noise):\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n        )\n\n    def predict_start_from_v(self, x_t, t, v):\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def model_predictions(self, x, t, x_self_cond = None, clip_x_start = False, rederive_pred_noise = False):\n        model_output = self.model(x, t, x_self_cond)\n        maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity\n\n        if self.objective == 'pred_noise':\n            pred_noise = model_output\n            x_start = self.predict_start_from_noise(x, t, pred_noise)\n            x_start = maybe_clip(x_start)\n\n            if clip_x_start and rederive_pred_noise:\n                pred_noise = self.predict_noise_from_start(x, t, x_start)\n\n        elif self.objective == 'pred_x0':\n            x_start = model_output\n            x_start = maybe_clip(x_start)\n            pred_noise = self.predict_noise_from_start(x, t, x_start)\n\n        elif self.objective == 'pred_v':\n            v = model_output\n            x_start = self.predict_start_from_v(x, t, v)\n            x_start = maybe_clip(x_start)\n            pred_noise = self.predict_noise_from_start(x, t, x_start)\n\n        return ModelPrediction(pred_noise, x_start)\n\n    def p_mean_variance(self, x, t, x_self_cond = None, clip_denoised = True):\n        preds = self.model_predictions(x, t, x_self_cond)\n        x_start = preds.pred_x_start\n\n        if clip_denoised:\n            x_start.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)\n        return model_mean, posterior_variance, posterior_log_variance, x_start\n\n    @torch.no_grad()\n    def p_sample(self, x, t: int, x_self_cond = None, clip_denoised = True):\n        b, *_, device = *x.shape, x.device\n        batched_times = torch.full((b,), t, device = x.device, dtype = torch.long)\n        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, x_self_cond = x_self_cond, clip_denoised = clip_denoised)\n        noise = torch.randn_like(x) if t &gt; 0 else 0. # no noise if t == 0\n        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n        return pred_img, x_start\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape):\n        batch, device = shape[0], self.betas.device\n\n        img = torch.randn(shape, device=device)\n\n        x_start = None\n\n        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n            self_cond = x_start if self.self_condition else None\n            img, x_start = self.p_sample(img, t, self_cond)\n\n        img = self.unnormalize(img)\n        return img\n\n    @torch.no_grad()\n    def ddim_sample(self, shape, clip_denoised = True):\n        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.betas.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n\n        times = torch.linspace(-1, total_timesteps - 1, steps=sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n        times = list(reversed(times.int().tolist()))\n        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n\n        img = torch.randn(shape, device = device)\n\n        x_start = None\n\n        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n            time_cond = torch.full((batch,), time, device=device, dtype=torch.long)\n            self_cond = x_start if self.self_condition else None\n            pred_noise, x_start, *_ = self.model_predictions(img, time_cond, self_cond, clip_x_start = clip_denoised)\n\n            if time_next &lt; 0:\n                img = x_start\n                continue\n\n            alpha = self.alphas_cumprod[time]\n            alpha_next = self.alphas_cumprod[time_next]\n\n            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n            c = (1 - alpha_next - sigma ** 2).sqrt()\n\n            noise = torch.randn_like(img)\n\n            img = x_start * alpha_next.sqrt() + \\\n                  c * pred_noise + \\\n                  sigma * noise\n\n        img = self.unnormalize(img)\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size = 16):\n        seq_length, channels = self.seq_length, self.channels\n        sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n        return sample_fn((batch_size, channels, seq_length))\n\n    @torch.no_grad()\n    def interpolate(self, x1, x2, t = None, lam = 0.5):\n        b, *_, device = *x1.shape, x1.device\n        t = default(t, self.num_timesteps - 1)\n\n        assert x1.shape == x2.shape\n\n        t_batched = torch.full((b,), t, device = device)\n        xt1, xt2 = map(lambda x: self.q_sample(x, t = t_batched), (x1, x2))\n\n        img = (1 - lam) * xt1 + lam * xt2\n\n        x_start = None\n\n        for i in tqdm(reversed(range(0, t)), desc = 'interpolation sample time step', total = t):\n            self_cond = x_start if self.self_condition else None\n            img, x_start = self.p_sample(img, i, self_cond)\n\n        return img\n\n    @autocast(enabled = False)\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n        )\n\n    def p_losses(self, x_start, t, noise = None):\n        b, c, n = x_start.shape\n        noise = default(noise, lambda: torch.randn_like(x_start))\n\n        # noise sample\n\n        x = self.q_sample(x_start = x_start, t = t, noise = noise)\n\n        # if doing self-conditioning, 50% of the time, predict x_start from current set of times\n        # and condition with unet with that\n        # this technique will slow down training by 25%, but seems to lower FID significantly\n\n        x_self_cond = None\n        if self.self_condition and random() &lt; 0.5:\n            with torch.no_grad():\n                x_self_cond = self.model_predictions(x, t).pred_x_start\n                x_self_cond.detach_()\n\n        # predict and take gradient step\n\n        model_out = self.model(x, t, x_self_cond)\n\n        if self.objective == 'pred_noise':\n            target = noise\n        elif self.objective == 'pred_x0':\n            target = x_start\n        elif self.objective == 'pred_v':\n            v = self.predict_v(x_start, t, noise)\n            target = v\n        else:\n            raise ValueError(f'unknown objective {self.objective}')\n\n        loss = F.mse_loss(model_out, target, reduction = 'none')\n        loss = reduce(loss, 'b ... -&gt; b (...)', 'mean')\n\n        loss = loss * extract(self.loss_weight, t, loss.shape)\n        return loss.mean()\n\n    def forward(self, img, *args, **kwargs):\n        b, c, n, device, seq_length, = *img.shape, img.device, self.seq_length\n        assert n == seq_length, f'seq length must be {seq_length}'\n        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n\n        img = self.normalize(img)\n        return self.p_losses(img, t, *args, **kwargs)\n</code></pre>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#\u591a\u7ef4_3","title":"\u591a\u7ef4","text":"<pre><code>class GaussianDiffusion(nn.Module):\n    def __init__(\n        self,\n        model,\n        *,\n        image_size,\n        timesteps = 1000,\n        sampling_timesteps = None,\n        objective = 'pred_v',\n        beta_schedule = 'sigmoid',\n        schedule_fn_kwargs = dict(),\n        ddim_sampling_eta = 0.,\n        auto_normalize = True,\n        offset_noise_strength = 0.,  # https://www.crosslabs.org/blog/diffusion-with-offset-noise\n        min_snr_loss_weight = False, # https://arxiv.org/abs/2303.09556\n        min_snr_gamma = 5\n    ):\n        super().__init__()\n        assert not (type(self) == GaussianDiffusion and model.channels != model.out_dim)\n        assert not model.random_or_learned_sinusoidal_cond\n\n        self.model = model\n\n        self.channels = self.model.channels\n        self.self_condition = self.model.self_condition\n\n        self.image_size = image_size\n\n        self.objective = objective\n\n        assert objective in {'pred_noise', 'pred_x0', 'pred_v'}, 'objective must be either pred_noise (predict noise) or pred_x0 (predict image start) or pred_v (predict v [v-parameterization as defined in appendix D of progressive distillation paper, used in imagen-video successfully])'\n\n        if beta_schedule == 'linear':\n            beta_schedule_fn = linear_beta_schedule\n        elif beta_schedule == 'cosine':\n            beta_schedule_fn = cosine_beta_schedule\n        elif beta_schedule == 'sigmoid':\n            beta_schedule_fn = sigmoid_beta_schedule\n        else:\n            raise ValueError(f'unknown beta schedule {beta_schedule}')\n\n        betas = beta_schedule_fn(timesteps, **schedule_fn_kwargs)\n\n        alphas = 1. - betas\n        alphas_cumprod = torch.cumprod(alphas, dim=0)\n        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value = 1.)\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n\n        # sampling related parameters\n        self.sampling_timesteps = default(sampling_timesteps, timesteps) # default num sampling timesteps to number of timesteps at training\n\n        assert self.sampling_timesteps &lt;= timesteps\n        self.is_ddim_sampling = self.sampling_timesteps &lt; timesteps\n        self.ddim_sampling_eta = ddim_sampling_eta\n\n        # helper function to register buffer from float64 to float32\n        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n        register_buffer('betas', betas)\n        register_buffer('alphas_cumprod', alphas_cumprod)\n        register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n        register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1. - alphas_cumprod))\n        register_buffer('log_one_minus_alphas_cumprod', torch.log(1. - alphas_cumprod))\n        register_buffer('sqrt_recip_alphas_cumprod', torch.sqrt(1. / alphas_cumprod))\n        register_buffer('sqrt_recipm1_alphas_cumprod', torch.sqrt(1. / alphas_cumprod - 1))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        register_buffer('posterior_variance', posterior_variance)\n\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        register_buffer('posterior_log_variance_clipped', torch.log(posterior_variance.clamp(min =1e-20)))\n        register_buffer('posterior_mean_coef1', betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod))\n        register_buffer('posterior_mean_coef2', (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod))\n\n        # offset noise strength - in blogpost, they claimed 0.1 was ideal\n        self.offset_noise_strength = offset_noise_strength\n\n        # derive loss weight\n        # snr - signal noise ratio\n        snr = alphas_cumprod / (1 - alphas_cumprod)\n\n        # https://arxiv.org/abs/2303.09556\n        maybe_clipped_snr = snr.clone()\n        if min_snr_loss_weight:\n            maybe_clipped_snr.clamp_(max = min_snr_gamma)\n\n        if objective == 'pred_noise':\n            register_buffer('loss_weight', maybe_clipped_snr / snr)\n        elif objective == 'pred_x0':\n            register_buffer('loss_weight', maybe_clipped_snr)\n        elif objective == 'pred_v':\n            register_buffer('loss_weight', maybe_clipped_snr / (snr + 1))\n\n        # auto-normalization of data [0, 1] -&gt; [-1, 1] - can turn off by setting it to be False\n        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity\n        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else identity\n\n    @property\n    def device(self):\n        return self.betas.device\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def predict_noise_from_start(self, x_t, t, x0):\n        return (\n            (extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0) / \\\n            extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n        )\n\n    def predict_v(self, x_start, t, noise):\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * noise -\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * x_start\n        )\n\n    def predict_start_from_v(self, x_t, t, v):\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_t.shape) * x_t -\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_t.shape) * v\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def model_predictions(self, x, t, x_self_cond = None, clip_x_start = False, rederive_pred_noise = False):\n        model_output = self.model(x, t, x_self_cond)\n        maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity\n\n        if self.objective == 'pred_noise':\n            pred_noise = model_output\n            x_start = self.predict_start_from_noise(x, t, pred_noise)\n            x_start = maybe_clip(x_start)\n\n            if clip_x_start and rederive_pred_noise:\n                pred_noise = self.predict_noise_from_start(x, t, x_start)\n\n        elif self.objective == 'pred_x0':\n            x_start = model_output\n            x_start = maybe_clip(x_start)\n            pred_noise = self.predict_noise_from_start(x, t, x_start)\n\n        elif self.objective == 'pred_v':\n            v = model_output\n            x_start = self.predict_start_from_v(x, t, v)\n            x_start = maybe_clip(x_start)\n            pred_noise = self.predict_noise_from_start(x, t, x_start)\n\n        return ModelPrediction(pred_noise, x_start)\n\n    def p_mean_variance(self, x, t, x_self_cond = None, clip_denoised = True):\n        preds = self.model_predictions(x, t, x_self_cond)\n        x_start = preds.pred_x_start\n\n        if clip_denoised:\n            x_start.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)\n        return model_mean, posterior_variance, posterior_log_variance, x_start\n\n    @torch.inference_mode()\n    def p_sample(self, x, t: int, x_self_cond = None):\n        b, *_, device = *x.shape, self.device\n        batched_times = torch.full((b,), t, device = device, dtype = torch.long)\n        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, x_self_cond = x_self_cond, clip_denoised = True)\n        noise = torch.randn_like(x) if t &gt; 0 else 0. # no noise if t == 0\n        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n        return pred_img, x_start\n\n    @torch.inference_mode()\n    def p_sample_loop(self, shape, return_all_timesteps = False):\n        batch, device = shape[0], self.device\n\n        img = torch.randn(shape, device = device)\n        imgs = [img]\n\n        x_start = None\n\n        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n            self_cond = x_start if self.self_condition else None\n            img, x_start = self.p_sample(img, t, self_cond)\n            imgs.append(img)\n\n        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n\n        ret = self.unnormalize(ret)\n        return ret\n\n    @torch.inference_mode()\n    def ddim_sample(self, shape, return_all_timesteps = False):\n        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n\n        times = torch.linspace(-1, total_timesteps - 1, steps = sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n        times = list(reversed(times.int().tolist()))\n        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n\n        img = torch.randn(shape, device = device)\n        imgs = [img]\n\n        x_start = None\n\n        for time, time_next in tqdm(time_pairs, desc = 'sampling loop time step'):\n            time_cond = torch.full((batch,), time, device = device, dtype = torch.long)\n            self_cond = x_start if self.self_condition else None\n            pred_noise, x_start, *_ = self.model_predictions(img, time_cond, self_cond, clip_x_start = True, rederive_pred_noise = True)\n\n            if time_next &lt; 0:\n                img = x_start\n                imgs.append(img)\n                continue\n\n            alpha = self.alphas_cumprod[time]\n            alpha_next = self.alphas_cumprod[time_next]\n\n            sigma = eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n            c = (1 - alpha_next - sigma ** 2).sqrt()\n\n            noise = torch.randn_like(img)\n\n            img = x_start * alpha_next.sqrt() + \\\n                  c * pred_noise + \\\n                  sigma * noise\n\n            imgs.append(img)\n\n        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n\n        ret = self.unnormalize(ret)\n        return ret\n\n    @torch.inference_mode()\n    def sample(self, batch_size = 16, return_all_timesteps = False):\n        image_size, channels = self.image_size, self.channels\n        sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n        return sample_fn((batch_size, channels, image_size, image_size), return_all_timesteps = return_all_timesteps)\n\n    @torch.inference_mode()\n    def interpolate(self, x1, x2, t = None, lam = 0.5):\n        b, *_, device = *x1.shape, x1.device\n        t = default(t, self.num_timesteps - 1)\n\n        assert x1.shape == x2.shape\n\n        t_batched = torch.full((b,), t, device = device)\n        xt1, xt2 = map(lambda x: self.q_sample(x, t = t_batched), (x1, x2))\n\n        img = (1 - lam) * xt1 + lam * xt2\n\n        x_start = None\n\n        for i in tqdm(reversed(range(0, t)), desc = 'interpolation sample time step', total = t):\n            self_cond = x_start if self.self_condition else None\n            img, x_start = self.p_sample(img, i, self_cond)\n\n        return img\n\n    @autocast(enabled = False)\n    def q_sample(self, x_start, t, noise = None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n\n        return (\n            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n            extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n        )\n\n    def p_losses(self, x_start, t, noise = None, offset_noise_strength = None):\n        b, c, h, w = x_start.shape\n\n        noise = default(noise, lambda: torch.randn_like(x_start))\n\n        # offset noise - https://www.crosslabs.org/blog/diffusion-with-offset-noise\n\n        offset_noise_strength = default(offset_noise_strength, self.offset_noise_strength)\n\n        if offset_noise_strength &gt; 0.:\n            offset_noise = torch.randn(x_start.shape[:2], device = self.device)\n            noise += offset_noise_strength * rearrange(offset_noise, 'b c -&gt; b c 1 1')\n\n        # noise sample\n\n        x = self.q_sample(x_start = x_start, t = t, noise = noise)\n\n        # if doing self-conditioning, 50% of the time, predict x_start from current set of times\n        # and condition with unet with that\n        # this technique will slow down training by 25%, but seems to lower FID significantly\n\n        x_self_cond = None\n        if self.self_condition and random() &lt; 0.5:\n            with torch.inference_mode():\n                x_self_cond = self.model_predictions(x, t).pred_x_start\n                x_self_cond.detach_()\n\n        # predict and take gradient step\n\n        model_out = self.model(x, t, x_self_cond)\n\n        if self.objective == 'pred_noise':\n            target = noise\n        elif self.objective == 'pred_x0':\n            target = x_start\n        elif self.objective == 'pred_v':\n            v = self.predict_v(x_start, t, noise)\n            target = v\n        else:\n            raise ValueError(f'unknown objective {self.objective}')\n\n        loss = F.mse_loss(model_out, target, reduction = 'none')\n        loss = reduce(loss, 'b ... -&gt; b (...)', 'mean')\n\n        loss = loss * extract(self.loss_weight, t, loss.shape)\n        return loss.mean()\n\n    def forward(self, img, *args, **kwargs):\n        b, c, h, w, device, img_size, = *img.shape, img.device, self.image_size\n        assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n\n        img = self.normalize(img)\n        return self.p_losses(img, t, *args, **kwargs)\n</code></pre> <p><code>GaussianDiffusion</code>\u548c<code>GaussianDiffusion1D</code>\u4e24\u4e2a\u7c7b\u7684\u4e3b\u8981\u5dee\u5f02\u5728\u4e8e\u5b83\u4eec\u5904\u7406\u7684\u6570\u636e\u7ef4\u5ea6\u4e0d\u540c\u3002<code>GaussianDiffusion</code>\u7c7b\u53ef\u4ee5\u5904\u7406\u66f4\u9ad8\u7ef4\u5ea6\u7684\u6570\u636e\uff0c\u4f8b\u5982\u4e8c\u7ef4\u7684\u56fe\u50cf\u6570\u636e\uff0c\u800c<code>GaussianDiffusion1D</code>\u7c7b\u53ea\u5904\u7406\u4e00\u7ef4\u7684\u6570\u636e\u3002</p> <p>\u4ee5\u4e0b\u662f\u4e24\u4e2a\u7c7b\u4e2d\u4e00\u4e9b\u4e3b\u8981\u51fd\u6570\u7684\u5dee\u5f02\u548c\u7b80\u5316\uff1a</p> <ol> <li> <p><code>__init__</code>\u51fd\u6570\uff1a\u5728<code>GaussianDiffusion</code>\u7c7b\u4e2d\uff0c\u8fd9\u4e2a\u51fd\u6570\u9700\u8981\u5904\u7406\u7684\u53c2\u6570\u66f4\u591a\uff0c\u5305\u62ec\u56fe\u50cf\u7684\u5bbd\u5ea6\u548c\u9ad8\u5ea6\uff0c\u800c\u5728<code>GaussianDiffusion1D</code>\u7c7b\u4e2d\uff0c\u7531\u4e8e\u53ea\u5904\u7406\u4e00\u7ef4\u6570\u636e\uff0c\u6240\u4ee5\u8fd9\u4e2a\u51fd\u6570\u7684\u53c2\u6570\u66f4\u5c11\uff0c\u53ea\u9700\u8981\u5904\u7406\u6570\u636e\u7684\u957f\u5ea6\u3002</p> </li> <li> <p><code>p_sample</code>\u51fd\u6570\uff1a\u5728<code>GaussianDiffusion</code>\u7c7b\u4e2d\uff0c\u8fd9\u4e2a\u51fd\u6570\u9700\u8981\u5904\u7406\u591a\u4e2a\u65f6\u95f4\u6b65\uff0c\u6bcf\u4e2a\u65f6\u95f4\u6b65\u90fd\u9700\u8981\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u56fe\u50cf\u3002\u800c\u5728<code>GaussianDiffusion1D</code>\u7c7b\u4e2d\uff0c\u8fd9\u4e2a\u51fd\u6570\u53ea\u9700\u8981\u5904\u7406\u4e00\u4e2a\u65f6\u95f4\u6b65\uff0c\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u4e00\u7ef4\u6570\u636e\u3002</p> </li> <li> <p><code>p_sample_loop</code>\u548c<code>ddim_sample</code>\u51fd\u6570\uff1a\u5728<code>GaussianDiffusion</code>\u7c7b\u4e2d\uff0c\u8fd9\u4e24\u4e2a\u51fd\u6570\u9700\u8981\u5728\u4e00\u4e2a\u5faa\u73af\u4e2d\u591a\u6b21\u8c03\u7528<code>p_sample</code>\u51fd\u6570\u8fdb\u884c\u91c7\u6837\uff0c\u6bcf\u6b21\u91c7\u6837\u90fd\u9700\u8981\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u56fe\u50cf\u3002\u800c\u5728<code>GaussianDiffusion1D</code>\u7c7b\u4e2d\uff0c\u8fd9\u4e24\u4e2a\u51fd\u6570\u7684\u5b9e\u73b0\u66f4\u7b80\u5355\uff0c\u53ea\u9700\u8981\u5728\u4e00\u4e2a\u5faa\u73af\u4e2d\u591a\u6b21\u8c03\u7528<code>p_sample</code>\u51fd\u6570\u8fdb\u884c\u91c7\u6837\uff0c\u6bcf\u6b21\u91c7\u6837\u90fd\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u4e00\u7ef4\u6570\u636e\u3002</p> </li> <li> <p><code>sample</code>\u51fd\u6570\uff1a\u5728<code>GaussianDiffusion</code>\u7c7b\u4e2d\uff0c\u8fd9\u4e2a\u51fd\u6570\u9700\u8981\u6839\u636e<code>is_ddim_sampling</code>\u53c2\u6570\u9009\u62e9\u91c7\u6837\u51fd\u6570\uff08<code>p_sample_loop</code>\u6216<code>ddim_sample</code>\uff09\uff0c\u7136\u540e\u5728\u4e00\u4e2a\u5faa\u73af\u4e2d\u591a\u6b21\u8c03\u7528\u9009\u5b9a\u7684\u91c7\u6837\u51fd\u6570\u8fdb\u884c\u91c7\u6837\uff0c\u6bcf\u6b21\u91c7\u6837\u90fd\u9700\u8981\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u56fe\u50cf\u3002\u800c\u5728<code>GaussianDiffusion1D</code>\u7c7b\u4e2d\uff0c\u8fd9\u4e2a\u51fd\u6570\u7684\u5b9e\u73b0\u66f4\u7b80\u5355\uff0c\u53ea\u9700\u8981\u6839\u636e<code>is_ddim_sampling</code>\u53c2\u6570\u9009\u62e9\u91c7\u6837\u51fd\u6570\uff08<code>p_sample_loop</code>\u6216<code>ddim_sample</code>\uff09\uff0c\u7136\u540e\u5728\u4e00\u4e2a\u5faa\u73af\u4e2d\u591a\u6b21\u8c03\u7528\u9009\u5b9a\u7684\u91c7\u6837\u51fd\u6570\u8fdb\u884c\u91c7\u6837\uff0c\u6bcf\u6b21\u91c7\u6837\u90fd\u751f\u6210\u4e00\u4e2a\u65b0\u7684\u4e00\u7ef4\u6570\u636e\u3002</p> </li> </ol>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#5model","title":"5\u3001Model","text":"<p>\u5728DDPM\uff08Denoising Diffusion Probabilistic Models\uff09\u6a21\u578b\u4e2d\uff0c<code>model</code>\u7684\u76ee\u6807\u53ef\u4ee5\u662f<code>pred_noise</code>\u3001<code>pred_x0</code>\u6216<code>pred_v</code>\uff0c\u8fd9\u4e9b\u9009\u9879\u4ee3\u8868\u4e86\u4e0d\u540c\u7684\u9884\u6d4b\u76ee\u6807\uff0c\u5177\u4f53\u5982\u4e0b\uff1a</p> <ol> <li> <p><code>pred_noise</code>\uff1a\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c<code>model</code>\u8bd5\u56fe\u9884\u6d4b\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u957f\u5e94\u8be5\u6dfb\u52a0\u7684\u566a\u58f0\u3002\u8fd9\u662f\u6700\u76f4\u63a5\u7684\u65b9\u6cd5\uff0c\u56e0\u4e3a\u5728\u524d\u5411\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u5b9e\u9645\u4e0a\u662f\u5728\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\u6dfb\u52a0\u566a\u58f0\u3002\u7136\u540e\u5728\u540e\u5411\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u8bd5\u56fe\u9884\u6d4b\u5e76\u53bb\u9664\u8fd9\u4e9b\u566a\u58f0\u3002</p> </li> <li> <p><code>pred_x0</code>\uff1a\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c<code>model</code>\u8bd5\u56fe\u9884\u6d4b\u539f\u59cb\u56fe\u50cf\uff08\u5373\u566a\u58f0\u56fe\u50cf\u5728\u5b8c\u5168\u53bb\u566a\u540e\u7684\u72b6\u6001\uff09\u3002\u8fd9\u662f\u4e00\u4e2a\u66f4\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a<code>model</code>\u9700\u8981\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u957f\u9884\u6d4b\u539f\u59cb\u56fe\u50cf\u7684\u5168\u8c8c\uff0c\u5373\u4f7f\u5728\u65e9\u671f\u6b65\u9aa4\u4e2d\uff0c\u566a\u58f0\u56fe\u50cf\u53ef\u80fd\u4e0e\u539f\u59cb\u56fe\u50cf\u5dee\u8ddd\u5f88\u5927\u3002</p> </li> <li> <p><code>pred_v</code>\uff1a\u8fd9\u662f\u4e00\u4e2a\u66f4\u590d\u6742\u7684\u9884\u6d4b\u76ee\u6807\uff0c\u5b83\u6765\u81ea\u4e8eDDPM\u7684\u4e00\u79cd\u53d8\u4f53\uff0c\u79f0\u4e3aScore-Based Generative Models\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c<code>model</code>\u9884\u6d4b\u7684\u662f\u4e00\u4e2a\"score\"\uff0c\u5b83\u662f\u539f\u59cb\u56fe\u50cf\u7684\u68af\u5ea6\u65b9\u5411\u3002\u8fd9\u4e2a\"score\"\u53ef\u4ee5\u88ab\u770b\u4f5c\u662f\u4e00\u4e2a\u5411\u91cf\u573a\uff0c\u6307\u5411\u539f\u59cb\u56fe\u50cf\u7684\u65b9\u5411\u3002\u5728\u540e\u5411\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u6cbf\u7740\u8fd9\u4e2a\u5411\u91cf\u573a\u7684\u65b9\u5411\uff0c\u9010\u6b65\u4ece\u566a\u58f0\u56fe\u50cf\"\u79fb\u52a8\"\u5411\u539f\u59cb\u56fe\u50cf\u3002</p> </li> </ol> <p>\u8fd9\u4e09\u79cd\u9884\u6d4b\u76ee\u6807\u63d0\u4f9b\u4e86\u4e0d\u540c\u7684\u65b9\u5f0f\u6765\u89e3\u51b3DDPM\u6a21\u578b\u7684\u91cd\u5efa\u4efb\u52a1\u3002\u9009\u62e9\u54ea\u79cd\u9884\u6d4b\u76ee\u6807\u53d6\u51b3\u4e8e\u5177\u4f53\u7684\u5e94\u7528\u9700\u6c42\u548c\u6a21\u578b\u6027\u80fd\u3002</p>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#construct","title":"Construct","text":""},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#\u4e00\u7ef4_4","title":"\u4e00\u7ef4","text":"<pre><code>class Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, *args, **kwargs):\n        return self.fn(x, *args, **kwargs) + x\n\ndef Upsample(dim, dim_out = None):\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv1d(dim, default(dim_out, dim), 3, padding = 1)\n    )\n\ndef Downsample(dim, dim_out = None):\n    return nn.Conv1d(dim, default(dim_out, dim), 4, 2, 1)\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.g = nn.Parameter(torch.ones(1, dim, 1))\n\n    def forward(self, x):\n        return F.normalize(x, dim = 1) * self.g * (x.shape[1] ** 0.5)\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = RMSNorm(dim)\n\n    def forward(self, x):\n        x = self.norm(x)\n        return self.fn(x)\n\n# sinusoidal positional embeds\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        device = x.device\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n        emb = x[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb\n\nclass RandomOrLearnedSinusoidalPosEmb(nn.Module):\n\"\"\" following @crowsonkb 's lead with random (learned optional) sinusoidal pos emb \"\"\"\n\"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim, is_random = False):\n        super().__init__()\n        assert (dim % 2) == 0\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim), requires_grad = not is_random)\n\n    def forward(self, x):\n        x = rearrange(x, 'b -&gt; b 1')\n        freqs = x * rearrange(self.weights, 'd -&gt; 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\n# building block modules\n\nclass Block(nn.Module):\n    def __init__(self, dim, dim_out, groups = 8):\n        super().__init__()\n        self.proj = nn.Conv1d(dim, dim_out, 3, padding = 1)\n        self.norm = nn.GroupNorm(groups, dim_out)\n        self.act = nn.SiLU()\n\n    def forward(self, x, scale_shift = None):\n        x = self.proj(x)\n        x = self.norm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.act(x)\n        return x\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, dim_out, *, time_emb_dim = None, groups = 8):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(time_emb_dim, dim_out * 2)\n        ) if exists(time_emb_dim) else None\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n        self.res_conv = nn.Conv1d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n\n    def forward(self, x, time_emb = None):\n\n        scale_shift = None\n        if exists(self.mlp) and exists(time_emb):\n            time_emb = self.mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -&gt; b c 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x, scale_shift = scale_shift)\n\n        h = self.block2(h)\n\n        return h + self.res_conv(x)\n\nclass LinearAttention(nn.Module):\n    def __init__(self, dim, heads = 4, dim_head = 32):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        hidden_dim = dim_head * heads\n        self.to_qkv = nn.Conv1d(dim, hidden_dim * 3, 1, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Conv1d(hidden_dim, dim, 1),\n            RMSNorm(dim)\n        )\n\n    def forward(self, x):\n        b, c, n = x.shape\n        qkv = self.to_qkv(x).chunk(3, dim = 1)\n        q, k, v = map(lambda t: rearrange(t, 'b (h c) n -&gt; b h c n', h = self.heads), qkv)\n\n        q = q.softmax(dim = -2)\n        k = k.softmax(dim = -1)\n\n        q = q * self.scale        \n\n        context = torch.einsum('b h d n, b h e n -&gt; b h d e', k, v)\n\n        out = torch.einsum('b h d e, b h d n -&gt; b h e n', context, q)\n        out = rearrange(out, 'b h c n -&gt; b (h c) n', h = self.heads)\n        return self.to_out(out)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 4, dim_head = 32):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        hidden_dim = dim_head * heads\n\n        self.to_qkv = nn.Conv1d(dim, hidden_dim * 3, 1, bias = False)\n        self.to_out = nn.Conv1d(hidden_dim, dim, 1)\n\n    def forward(self, x):\n        b, c, n = x.shape\n        qkv = self.to_qkv(x).chunk(3, dim = 1)\n        q, k, v = map(lambda t: rearrange(t, 'b (h c) n -&gt; b h c n', h = self.heads), qkv)\n\n        q = q * self.scale\n\n        sim = einsum('b h d i, b h d j -&gt; b h i j', q, k)\n        attn = sim.softmax(dim = -1)\n        out = einsum('b h i j, b h d j -&gt; b h i d', attn, v)\n\n        out = rearrange(out, 'b h n d -&gt; b (h d) n')\n        return self.to_out(out)\n</code></pre> <p>\u4ee3\u7801\u5b9a\u4e49\u4e86\u4e00\u4e9b\u7528\u4e8e\u6784\u5efa\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u57fa\u7840\u6a21\u5757\u3002\u4e0b\u9762\u662f\u6bcf\u4e2a\u51fd\u6570\u548c\u7c7b\u7684\u4f5c\u7528\uff1a</p> <ol> <li> <p><code>Residual</code>\uff1a\u6b8b\u5dee\u6a21\u5757\uff0c\u5b83\u5c06\u8f93\u5165x\u901a\u8fc7\u4e00\u4e2a\u51fd\u6570fn\u5904\u7406\u540e\uff0c\u518d\u52a0\u4e0a\u539f\u59cb\u7684\u8f93\u5165x\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5\u3002</p> </li> <li> <p><code>Upsample</code> \u548c <code>Downsample</code>\uff1a\u4e24\u4e2a\u51fd\u6570\u5206\u522b\u7528\u4e8e\u4e0a\u91c7\u6837\u548c\u4e0b\u91c7\u6837\u3002\u4e0a\u91c7\u6837\u662f\u5c06\u8f93\u5165\u7684\u7279\u5f81\u56fe\u653e\u5927\uff0c\u4e0b\u91c7\u6837\u5219\u662f\u5c06\u8f93\u5165\u7684\u7279\u5f81\u56fe\u7f29\u5c0f\u3002</p> </li> <li> <p><code>RMSNorm</code>\uff1a\u5f52\u4e00\u5316\u5c42\uff0c\u5b83\u4f7f\u7528RMSNorm\u65b9\u6cd5\u8fdb\u884c\u5f52\u4e00\u5316\u3002\u5f52\u4e00\u5316\u53ef\u4ee5\u5e2e\u52a9\u6a21\u578b\u66f4\u597d\u5730\u5b66\u4e60\u548c\u7406\u89e3\u6570\u636e\u3002</p> </li> <li> <p><code>PreNorm</code>\uff1a\u9884\u5f52\u4e00\u5316\u6a21\u5757\uff0c\u5b83\u5148\u5bf9\u8f93\u5165\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u7136\u540e\u518d\u901a\u8fc7\u4e00\u4e2a\u51fd\u6570\u8fdb\u884c\u5904\u7406\u3002</p> </li> <li> <p><code>SinusoidalPosEmb</code> \u548c <code>RandomOrLearnedSinusoidalPosEmb</code>\uff1a\u4e24\u4e2a\u7c7b\u7528\u4e8e\u751f\u6210\u6b63\u5f26\u4f4d\u7f6e\u5d4c\u5165\u3002\u4f4d\u7f6e\u5d4c\u5165\u662f\u7528\u4e8e\u5904\u7406\u5e8f\u5217\u6570\u636e\u7684\u4e00\u79cd\u6280\u672f\uff0c\u5b83\u53ef\u4ee5\u5e2e\u52a9\u6a21\u578b\u7406\u89e3\u5e8f\u5217\u4e2d\u7684\u5143\u7d20\u7684\u4f4d\u7f6e\u5173\u7cfb\u3002</p> </li> <li> <p><code>Block</code>\uff1a\u57fa\u7840\u7684\u5377\u79ef\u5757\uff0c\u5b83\u5305\u542b\u4e00\u4e2a\u5377\u79ef\u5c42\uff0c\u4e00\u4e2a\u5f52\u4e00\u5316\u5c42\uff0c\u548c\u4e00\u4e2a\u6fc0\u6d3b\u51fd\u6570\u3002</p> </li> <li> <p><code>ResnetBlock</code>\uff1aResNet\u98ce\u683c\u7684\u5377\u79ef\u5757\uff0c\u5b83\u5305\u542b\u4e24\u4e2a\u57fa\u7840\u5377\u79ef\u5757\u548c\u4e00\u4e2a\u6b8b\u5dee\u8fde\u63a5\u3002</p> </li> <li> <p><code>LinearAttention</code> \u548c <code>Attention</code>\uff1a\u4e24\u4e2a\u7c7b\u90fd\u662f\u7528\u4e8e\u5b9e\u73b0\u6ce8\u610f\u529b\u673a\u5236\u7684\u6a21\u5757\u3002\u6ce8\u610f\u529b\u673a\u5236\u662f\u4e00\u79cd\u8ba9\u6a21\u578b\u5728\u5904\u7406\u6570\u636e\u65f6\u80fd\u591f\u5173\u6ce8\u5230\u91cd\u8981\u90e8\u5206\u7684\u6280\u672f\u3002</p> </li> </ol>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#\u591a\u7ef4_4","title":"\u591a\u7ef4","text":"<pre><code>def Upsample(dim, dim_out = None):\n    return nn.Sequential(\n        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n        nn.Conv2d(dim, default(dim_out, dim), 3, padding = 1)\n    )\n\ndef Downsample(dim, dim_out = None):\n    return nn.Sequential(\n        Rearrange('b c (h p1) (w p2) -&gt; b (c p1 p2) h w', p1 = 2, p2 = 2),\n        nn.Conv2d(dim * 4, default(dim_out, dim), 1)\n    )\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n\n    def forward(self, x):\n        return F.normalize(x, dim = 1) * self.g * (x.shape[1] ** 0.5)\n\n# sinusoidal positional embeds\n\nclass SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        device = x.device\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n        emb = x[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb\n\nclass RandomOrLearnedSinusoidalPosEmb(nn.Module):\n\"\"\" following @crowsonkb 's lead with random (learned optional) sinusoidal pos emb \"\"\"\n\"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n\n    def __init__(self, dim, is_random = False):\n        super().__init__()\n        assert divisible_by(dim, 2)\n        half_dim = dim // 2\n        self.weights = nn.Parameter(torch.randn(half_dim), requires_grad = not is_random)\n\n    def forward(self, x):\n        x = rearrange(x, 'b -&gt; b 1')\n        freqs = x * rearrange(self.weights, 'd -&gt; 1 d') * 2 * math.pi\n        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n        fouriered = torch.cat((x, fouriered), dim = -1)\n        return fouriered\n\n# building block modules\n\nclass Block(nn.Module):\n    def __init__(self, dim, dim_out, groups = 8):\n        super().__init__()\n        self.proj = nn.Conv2d(dim, dim_out, 3, padding = 1)\n        self.norm = nn.GroupNorm(groups, dim_out)\n        self.act = nn.SiLU()\n\n    def forward(self, x, scale_shift = None):\n        x = self.proj(x)\n        x = self.norm(x)\n\n        if exists(scale_shift):\n            scale, shift = scale_shift\n            x = x * (scale + 1) + shift\n\n        x = self.act(x)\n        return x\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, dim_out, *, time_emb_dim = None, groups = 8):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(time_emb_dim, dim_out * 2)\n        ) if exists(time_emb_dim) else None\n\n        self.block1 = Block(dim, dim_out, groups = groups)\n        self.block2 = Block(dim_out, dim_out, groups = groups)\n        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n\n    def forward(self, x, time_emb = None):\n\n        scale_shift = None\n        if exists(self.mlp) and exists(time_emb):\n            time_emb = self.mlp(time_emb)\n            time_emb = rearrange(time_emb, 'b c -&gt; b c 1 1')\n            scale_shift = time_emb.chunk(2, dim = 1)\n\n        h = self.block1(x, scale_shift = scale_shift)\n\n        h = self.block2(h)\n\n        return h + self.res_conv(x)\n\nclass LinearAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        heads = 4,\n        dim_head = 32\n    ):\n        super().__init__()\n        self.scale = dim_head ** -0.5\n        self.heads = heads\n        hidden_dim = dim_head * heads\n\n        self.norm = RMSNorm(dim)\n        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Conv2d(hidden_dim, dim, 1),\n            RMSNorm(dim)\n        )\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n\n        x = self.norm(x)\n\n        qkv = self.to_qkv(x).chunk(3, dim = 1)\n        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -&gt; b h c (x y)', h = self.heads), qkv)\n\n        q = q.softmax(dim = -2)\n        k = k.softmax(dim = -1)\n\n        q = q * self.scale\n\n        context = torch.einsum('b h d n, b h e n -&gt; b h d e', k, v)\n\n        out = torch.einsum('b h d e, b h d n -&gt; b h e n', context, q)\n        out = rearrange(out, 'b h c (x y) -&gt; b (h c) x y', h = self.heads, x = h, y = w)\n        return self.to_out(out)\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        heads = 4,\n        dim_head = 32,\n        flash = False\n    ):\n        super().__init__()\n        self.heads = heads\n        hidden_dim = dim_head * heads\n\n        self.norm = RMSNorm(dim)\n        self.attend = Attend(flash = flash)\n\n        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n\n        x = self.norm(x)\n\n        qkv = self.to_qkv(x).chunk(3, dim = 1)\n        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -&gt; b h (x y) c', h = self.heads), qkv)\n\n        out = self.attend(q, k, v)\n\n        out = rearrange(out, 'b h (x y) d -&gt; b (h d) x y', x = h, y = w)\n        return self.to_out(out)\n</code></pre> <p>\u8be5\u6bb5\u4ee3\u7801\u4e0e\u4e00\u7ef4\u4ee3\u7801\u5b9e\u73b0\u7684\u529f\u80fd\u5927\u81f4\u76f8\u540c\uff0c\u4f46\u56e0\u4e3a\u4e3b\u8981\u662f\u9488\u5bf9\u4e8c\u7ef4\u6570\u636e\u8bbe\u8ba1\u7684\uff0c\u56e0\u6b64\u5728\u90e8\u5206\u65b9\u6cd5\u4e0a\u6709\u7740\u533a\u522b\uff1a</p> <ol> <li> <p><code>Upsample</code> \u548c <code>Downsample</code>\uff1a\u4e0e\u4e00\u7ef4\u4ee3\u7801\u6bb5\u4e2d\u7684\u51fd\u6570\u76f8\u6bd4\uff0c\u4f7f\u7528\u4e86\u4e8c\u7ef4\u5377\u79ef\u548c\u4e8c\u7ef4\u91cd\u6392\u5217\u3002</p> </li> <li> <p><code>RMSNorm</code>\uff1a\u4e0e\u4e00\u7ef4\u4ee3\u7801\u6bb5\u4e2d\u7684\u51fd\u6570\u76f8\u6bd4\uff0c\u589e\u52a0\u4e86\u4e00\u4e2a\u7ef4\u5ea6\u3002</p> </li> <li> <p><code>SinusoidalPosEmb</code> \u548c <code>RandomOrLearnedSinusoidalPosEmb</code>\uff1a\u8fd9\u4e24\u4e2a\u7c7b\u7528\u4e8e\u751f\u6210\u6b63\u5f26\u4f4d\u7f6e\u5d4c\u5165\u3002\u4f4d\u7f6e\u5d4c\u5165\u662f\u7528\u4e8e\u5904\u7406\u5e8f\u5217\u6570\u636e\u7684\u4e00\u79cd\u6280\u672f\uff0c\u5b83\u53ef\u4ee5\u5e2e\u52a9\u6a21\u578b\u7406\u89e3\u5e8f\u5217\u4e2d\u7684\u5143\u7d20\u7684\u4f4d\u7f6e\u5173\u7cfb\u3002</p> </li> <li> <p><code>Block</code>\uff1a\u4e0e\u4e00\u7ef4\u4ee3\u7801\u6bb5\u4e2d\u7684\u51fd\u6570\u76f8\u6bd4\uff0c\u4f7f\u7528\u4e86\u4e8c\u7ef4\u5377\u79ef\u3002</p> </li> <li> <p><code>ResnetBlock</code>\uff1a\u4e0e\u4e00\u7ef4\u4ee3\u7801\u6bb5\u4e2d\u7684\u51fd\u6570\u76f8\u6bd4\uff0c\u4f7f\u7528\u4e86\u4e8c\u7ef4\u5377\u79ef\u3002</p> </li> <li> <p><code>LinearAttention</code> \u548c <code>Attention</code>\uff1a\u4e0e\u4e00\u7ef4\u4ee3\u7801\u6bb5\u4e2d\u7684\u51fd\u6570\u76f8\u6bd4\uff0c\u4f7f\u7528\u4e86\u4e8c\u7ef4\u5377\u79ef\uff0c\u5e76\u4e14\u5728\u5904\u7406\u6570\u636e\u65f6\u8003\u8651\u4e86\u6570\u636e\u7684\u4e8c\u7ef4\u7ed3\u6784\u3002</p> </li> </ol>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#u-net","title":"U-Net","text":"<p>U-Net\u662f\u4e00\u79cd\u5e38\u7528\u4e8e\u56fe\u50cf\u5206\u5272\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5b83\u7684\u7279\u70b9\u662f\u5728\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e4b\u95f4\u6709\u5f88\u591a\u7684\u8de8\u5c42\u8fde\u63a5\u3002\u5728\u8fd9\u4e2a\u6a21\u578b\u4e2d\uff0c\u9996\u5148\u901a\u8fc7\u4e00\u7cfb\u5217\u7684\u4e0b\u91c7\u6837\u5c42\u5c06\u6570\u636e\u7684\u7ef4\u5ea6\u9010\u6e10\u51cf\u5c0f\uff0c\u7136\u540e\u5728\u4e2d\u95f4\u5c42\u8fdb\u884c\u5904\u7406\uff0c\u6700\u540e\u901a\u8fc7\u4e00\u7cfb\u5217\u7684\u4e0a\u91c7\u6837\u5c42\u5c06\u6570\u636e\u7684\u7ef4\u5ea6\u9010\u6e10\u6062\u590d\uff0c\u540c\u65f6\u5728\u6bcf\u4e00\u5c42\u90fd\u4f1a\u6709\u6b8b\u5dee\u8fde\u63a5\u548c\u6ce8\u610f\u529b\u673a\u5236\u3002</p>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#\u4e00\u7ef4_5","title":"\u4e00\u7ef4","text":"<pre><code>class Unet1D(nn.Module):\n    def __init__(\n        self,\n        dim,\n        init_dim = None,\n        out_dim = None,\n        dim_mults=(1, 2, 4, 8),\n        channels = 3,\n        self_condition = False,\n        resnet_block_groups = 8,\n        learned_variance = False,\n        learned_sinusoidal_cond = False,\n        random_fourier_features = False,\n        learned_sinusoidal_dim = 16,\n        attn_dim_head = 32,\n        attn_heads = 4\n    ):\n        super().__init__()\n\n        # determine dimensions\n\n        self.channels = channels\n        self.self_condition = self_condition\n        input_channels = channels * (2 if self_condition else 1)\n\n        init_dim = default(init_dim, dim)\n        self.init_conv = nn.Conv1d(input_channels, init_dim, 7, padding = 3)\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        block_klass = partial(ResnetBlock, groups = resnet_block_groups)\n\n        # time embeddings\n\n        time_dim = dim * 4\n\n        self.random_or_learned_sinusoidal_cond = learned_sinusoidal_cond or random_fourier_features\n\n        if self.random_or_learned_sinusoidal_cond:\n            sinu_pos_emb = RandomOrLearnedSinusoidalPosEmb(learned_sinusoidal_dim, random_fourier_features)\n            fourier_dim = learned_sinusoidal_dim + 1\n        else:\n            sinu_pos_emb = SinusoidalPosEmb(dim)\n            fourier_dim = dim\n\n        self.time_mlp = nn.Sequential(\n            sinu_pos_emb,\n            nn.Linear(fourier_dim, time_dim),\n            nn.GELU(),\n            nn.Linear(time_dim, time_dim)\n        )\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        for ind, (dim_in, dim_out) in enumerate(in_out):\n            is_last = ind &gt;= (num_resolutions - 1)\n\n            self.downs.append(nn.ModuleList([\n                block_klass(dim_in, dim_in, time_emb_dim = time_dim),\n                block_klass(dim_in, dim_in, time_emb_dim = time_dim),\n                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n                Downsample(dim_in, dim_out) if not is_last else nn.Conv1d(dim_in, dim_out, 3, padding = 1)\n            ]))\n\n        mid_dim = dims[-1]\n        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim, dim_head = attn_dim_head, heads = attn_heads)))\n        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n\n        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n            is_last = ind == (len(in_out) - 1)\n\n            self.ups.append(nn.ModuleList([\n                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n                Upsample(dim_out, dim_in) if not is_last else  nn.Conv1d(dim_out, dim_in, 3, padding = 1)\n            ]))\n\n        default_out_dim = channels * (1 if not learned_variance else 2)\n        self.out_dim = default(out_dim, default_out_dim)\n\n        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim = time_dim)\n        self.final_conv = nn.Conv1d(dim, self.out_dim, 1)\n\n    def forward(self, x, time, x_self_cond = None):\n        if self.self_condition:\n            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))\n            x = torch.cat((x_self_cond, x), dim = 1)\n\n        x = self.init_conv(x)\n        r = x.clone()\n\n        t = self.time_mlp(time)\n\n        h = []\n\n        for block1, block2, attn, downsample in self.downs:\n            x = block1(x, t)\n            h.append(x)\n\n            x = block2(x, t)\n            x = attn(x)\n            h.append(x)\n\n            x = downsample(x)\n\n        x = self.mid_block1(x, t)\n        x = self.mid_attn(x)\n        x = self.mid_block2(x, t)\n\n        for block1, block2, attn, upsample in self.ups:\n            x = torch.cat((x, h.pop()), dim = 1)\n            x = block1(x, t)\n\n            x = torch.cat((x, h.pop()), dim = 1)\n            x = block2(x, t)\n            x = attn(x)\n\n            x = upsample(x)\n\n        x = torch.cat((x, r), dim = 1)\n\n        x = self.final_res_block(x, t)\n        return self.final_conv(x)\n</code></pre> <p>\u4e0a\u8ff0\u4ee3\u7801\u6784\u5efa\u4e86\u4e00\u4e2a\u4e00\u7ef4\u7684U-Net\u6a21\u578b\uff0c\u4e3b\u8981\u7528\u4e8e\u5904\u7406\u4e00\u7ef4\u5e8f\u5217\u6570\u636e\uff0c\u5982\u97f3\u9891\u6216\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002\u56e0\u6b64\uff0c\u5b83\u7684\u8f93\u5165\u6570\u636e\u901a\u5e38\u662f\u5f62\u72b6\u4e3a[batch_size, channels, length]\u7684\u4e09\u7ef4\u5f20\u91cf\u3002\u5728U-Net1D\u6a21\u578b\u4e2d\uff0c\u5377\u79ef\u3001\u4e0a\u91c7\u6837\u548c\u4e0b\u91c7\u6837\u7b49\u64cd\u4f5c\u90fd\u662f\u4e00\u7ef4\u7684\u3002</p> <p>\u5728<code>Unet1D</code>\u7684<code>forward</code>\u51fd\u6570\u4e2d\uff0c\u9996\u5148\u68c0\u67e5\u662f\u5426\u8bbe\u7f6e\u4e86\u81ea\u6211\u6761\u4ef6\u6a21\u5f0f\uff0c\u5982\u679c\u8bbe\u7f6e\u4e86\uff0c\u90a3\u4e48\u4f1a\u5c06\u8f93\u5165\u6570\u636e\u548c\u81ea\u6211\u6761\u4ef6\u6570\u636e\u8fdb\u884c\u62fc\u63a5\u3002\u7136\u540e\u901a\u8fc7\u521d\u59cb\u5377\u79ef\u5c42\u8fdb\u884c\u5904\u7406\uff0c\u63a5\u7740\u5c06\u5904\u7406\u540e\u7684\u6570\u636e\u901a\u8fc7\u4e00\u7cfb\u5217\u7684\u4e0b\u91c7\u6837\u5c42\u3001\u4e2d\u95f4\u5c42\u548c\u4e0a\u91c7\u6837\u5c42\u8fdb\u884c\u5904\u7406\uff0c\u6700\u540e\u901a\u8fc7\u6700\u7ec8\u7684\u6b8b\u5dee\u5757\u548c\u5377\u79ef\u5c42\u5f97\u5230\u8f93\u51fa\u3002\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u8fd8\u4f1a\u4f7f\u7528\u65f6\u95f4\u5d4c\u5165\u5c42\u5bf9\u8f93\u5165\u7684\u65f6\u95f4\u8fdb\u884c\u5904\u7406\uff0c\u5e76\u5c06\u5904\u7406\u540e\u7684\u65f6\u95f4\u5d4c\u5165\u4f5c\u4e3a\u989d\u5916\u7684\u8f93\u5165\u4f20\u9012\u7ed9\u5404\u4e2a\u5377\u79ef\u5757\u548c\u6b8b\u5dee\u5757\u3002</p>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#\u591a\u7ef4_5","title":"\u591a\u7ef4","text":"<pre><code>class Unet(nn.Module):\n    def __init__(\n        self,\n        dim,\n        init_dim = None,\n        out_dim = None,\n        dim_mults = (1, 2, 4, 8),\n        channels = 3,\n        self_condition = False,\n        resnet_block_groups = 8,\n        learned_variance = False,\n        learned_sinusoidal_cond = False,\n        random_fourier_features = False,\n        learned_sinusoidal_dim = 16,\n        attn_dim_head = 32,\n        attn_heads = 4,\n        full_attn = (False, False, False, True),\n        flash_attn = False\n    ):\n        super().__init__()\n\n        # determine dimensions\n\n        self.channels = channels\n        self.self_condition = self_condition\n        input_channels = channels * (2 if self_condition else 1)\n\n        init_dim = default(init_dim, dim)\n        self.init_conv = nn.Conv2d(input_channels, init_dim, 7, padding = 3)\n\n        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n        in_out = list(zip(dims[:-1], dims[1:]))\n\n        block_klass = partial(ResnetBlock, groups = resnet_block_groups)\n\n        # time embeddings\n\n        time_dim = dim * 4\n\n        self.random_or_learned_sinusoidal_cond = learned_sinusoidal_cond or random_fourier_features\n\n        if self.random_or_learned_sinusoidal_cond:\n            sinu_pos_emb = RandomOrLearnedSinusoidalPosEmb(learned_sinusoidal_dim, random_fourier_features)\n            fourier_dim = learned_sinusoidal_dim + 1\n        else:\n            sinu_pos_emb = SinusoidalPosEmb(dim)\n            fourier_dim = dim\n\n        self.time_mlp = nn.Sequential(\n            sinu_pos_emb,\n            nn.Linear(fourier_dim, time_dim),\n            nn.GELU(),\n            nn.Linear(time_dim, time_dim)\n        )\n\n        # attention\n\n        num_stages = len(dim_mults)\n        full_attn  = cast_tuple(full_attn, num_stages)\n        attn_heads = cast_tuple(attn_heads, num_stages)\n        attn_dim_head = cast_tuple(attn_dim_head, num_stages)\n\n        assert len(full_attn) == len(dim_mults)\n\n        FullAttention = partial(Attention, flash = flash_attn)\n\n        # layers\n\n        self.downs = nn.ModuleList([])\n        self.ups = nn.ModuleList([])\n        num_resolutions = len(in_out)\n\n        for ind, ((dim_in, dim_out), layer_full_attn, layer_attn_heads, layer_attn_dim_head) in enumerate(zip(in_out, full_attn, attn_heads, attn_dim_head)):\n            is_last = ind &gt;= (num_resolutions - 1)\n\n            attn_klass = FullAttention if layer_full_attn else LinearAttention\n\n            self.downs.append(nn.ModuleList([\n                block_klass(dim_in, dim_in, time_emb_dim = time_dim),\n                block_klass(dim_in, dim_in, time_emb_dim = time_dim),\n                attn_klass(dim_in, dim_head = layer_attn_dim_head, heads = layer_attn_heads),\n                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding = 1)\n            ]))\n\n        mid_dim = dims[-1]\n        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n        self.mid_attn = FullAttention(mid_dim, heads = attn_heads[-1], dim_head = attn_dim_head[-1])\n        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim = time_dim)\n\n        for ind, ((dim_in, dim_out), layer_full_attn, layer_attn_heads, layer_attn_dim_head) in enumerate(zip(*map(reversed, (in_out, full_attn, attn_heads, attn_dim_head)))):\n            is_last = ind == (len(in_out) - 1)\n\n            attn_klass = FullAttention if layer_full_attn else LinearAttention\n\n            self.ups.append(nn.ModuleList([\n                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n                block_klass(dim_out + dim_in, dim_out, time_emb_dim = time_dim),\n                attn_klass(dim_out, dim_head = layer_attn_dim_head, heads = layer_attn_heads),\n                Upsample(dim_out, dim_in) if not is_last else  nn.Conv2d(dim_out, dim_in, 3, padding = 1)\n            ]))\n\n        default_out_dim = channels * (1 if not learned_variance else 2)\n        self.out_dim = default(out_dim, default_out_dim)\n\n        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim = time_dim)\n        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n\n    @property\n    def downsample_factor(self):\n        return 2 ** (len(self.downs) - 1)\n\n    def forward(self, x, time, x_self_cond = None):\n        assert all([divisible_by(d, self.downsample_factor) for d in x.shape[-2:]]), f'your input dimensions {x.shape[-2:]} need to be divisible by {self.downsample_factor}, given the unet'\n\n        if self.self_condition:\n            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))\n            x = torch.cat((x_self_cond, x), dim = 1)\n\n        x = self.init_conv(x)\n        r = x.clone()\n\n        t = self.time_mlp(time)\n\n        h = []\n\n        for block1, block2, attn, downsample in self.downs:\n            x = block1(x, t)\n            h.append(x)\n\n            x = block2(x, t)\n            x = attn(x) + x\n            h.append(x)\n\n            x = downsample(x)\n\n        x = self.mid_block1(x, t)\n        x = self.mid_attn(x) + x\n        x = self.mid_block2(x, t)\n\n        for block1, block2, attn, upsample in self.ups:\n            x = torch.cat((x, h.pop()), dim = 1)\n            x = block1(x, t)\n\n            x = torch.cat((x, h.pop()), dim = 1)\n            x = block2(x, t)\n            x = attn(x) + x\n\n            x = upsample(x)\n\n        x = torch.cat((x, r), dim = 1)\n\n        x = self.final_res_block(x, t)\n        return self.final_conv(x)\n</code></pre> <p>\u4e0e\u4fee\u6539\u540e\u7684 U-Net1D \u76f8\u6bd4\uff0c\u539f\u59cb\u7684U-Net\u6a21\u578b\u662f\u7528\u4e8e\u5904\u7406\u4e8c\u7ef4\u56fe\u50cf\u6570\u636e\u7684\uff0c\u56e0\u6b64\u5b83\u7684\u8f93\u5165\u6570\u636e\u901a\u5e38\u662f\u5f62\u72b6\u4e3a[batch_size, channels, height, width]\u7684\u56db\u7ef4\u5f20\u91cf\u3002\u5728U-Net\u6a21\u578b\u4e2d\uff0c\u5377\u79ef\u3001\u4e0a\u91c7\u6837\u548c\u4e0b\u91c7\u6837\u7b49\u64cd\u4f5c\u90fd\u662f\u4e8c\u7ef4\u7684\u3002</p>"},{"location":"AI/Model%20Analysis/Diffusion/Diffusion%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%28Pytorch%29/#\u5bf9\u6bd4","title":"\u5bf9\u6bd4","text":"<p>\u7531\u4e8e\u4e0a\u8ff0 U-Net \u6a21\u578b\u9488\u5bf9\u5904\u7406\u7684\u6570\u636e\u7ef4\u5ea6\u6709\u533a\u522b\uff0c\u5bfc\u81f4\u5176\u4f7f\u7528\u7684\u64cd\u4f5c\u4e5f\u5b58\u5728\u4e0d\u540c\u3002\u5176\u4e3b\u8981\u533a\u522b\u5982\u4e0b\uff1a</p> <ol> <li> <p>\u6570\u636e\u7ef4\u5ea6\uff1aUnet\u7c7b\u5904\u7406\u7684\u662f\u4e8c\u7ef4\u6570\u636e\uff0c\u56e0\u6b64\u5b83\u4f7f\u7528\u7684\u662f\u4e8c\u7ef4\u5377\u79ef\uff08nn.Conv2d\uff09\uff0c\u4e8c\u7ef4\u4e0a\u91c7\u6837\u548c\u4e8c\u7ef4\u4e0b\u91c7\u6837\u3002\u800cUnet1D\u7c7b\u5904\u7406\u7684\u662f\u4e00\u7ef4\u6570\u636e\uff0c\u56e0\u6b64\u5b83\u4f7f\u7528\u7684\u662f\u4e00\u7ef4\u5377\u79ef\uff08nn.Conv1d\uff09\uff0c\u4e00\u7ef4\u4e0a\u91c7\u6837\u548c\u4e00\u7ef4\u4e0b\u91c7\u6837\u3002</p> </li> <li> <p>\u6a21\u578b\u7ed3\u6784\uff1aUnet\u548cUnet1D\u7c7b\u7684\u6a21\u578b\u7ed3\u6784\u57fa\u672c\u76f8\u540c\uff0c\u90fd\u5305\u542b\u4e86\u4e0b\u91c7\u6837\uff08downsampling\uff09\u3001\u4e0a\u91c7\u6837\uff08upsampling\uff09\u548c\u8df3\u8dc3\u8fde\u63a5\uff08skip connection\uff09\u3002\u4ed6\u4eec\u90fd\u4f7f\u7528\u4e86\u6b8b\u5dee\u5757\uff08ResnetBlock\uff09\u548c\u6ce8\u610f\u529b\u673a\u5236\uff08Attention\uff09\u3002\u4f46\u662f\u5728\u5b9e\u73b0\u7ec6\u8282\u4e0a\uff0c\u7531\u4e8e\u5904\u7406\u7684\u6570\u636e\u7ef4\u5ea6\u4e0d\u540c\uff0c\u6240\u4ee5\u4ed6\u4eec\u4f7f\u7528\u7684\u64cd\u4f5c\u4e5f\u4e0d\u540c\u3002</p> </li> <li> <p>\u6ce8\u610f\u529b\u673a\u5236\uff1a\u5728Unet\u7c7b\u4e2d\uff0c\u4f7f\u7528\u4e86\u5168\u6ce8\u610f\u529b\uff08FullAttention\uff09\u548c\u7ebf\u6027\u6ce8\u610f\u529b\uff08LinearAttention\uff09\u3002\u800c\u5728Unet1D\u7c7b\u4e2d\uff0c\u53ea\u4f7f\u7528\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\uff08LinearAttention\uff09\u3002</p> </li> <li> <p>\u65f6\u95f4\u5d4c\u5165\uff1aUnet\u548cUnet1D\u7c7b\u90fd\u4f7f\u7528\u4e86\u65f6\u95f4\u5d4c\u5165\uff08time embeddings\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5904\u7406\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u4f4d\u7f6e\u4fe1\u606f\u7684\u65b9\u6cd5\u3002\u4ed6\u4eec\u90fd\u4f7f\u7528\u4e86\u6b63\u5f26\u4f4d\u7f6e\u5d4c\u5165\uff08SinusoidalPosEmb\uff09\u6216\u8005\u968f\u673a\u6216\u5b66\u4e60\u7684\u6b63\u5f26\u4f4d\u7f6e\u5d4c\u5165\uff08RandomOrLearnedSinusoidalPosEmb\uff09\u3002</p> </li> <li> <p>\u8f93\u5165\u6570\u636e\u7684\u5f62\u72b6\uff1aUnet\u7c7b\u7684\u8f93\u5165\u6570\u636e\u7684\u5f62\u72b6\u901a\u5e38\u662f[batch_size, channels, height, width]\uff0c\u800cUnet1D\u7c7b\u7684\u8f93\u5165\u6570\u636e\u7684\u5f62\u72b6\u901a\u5e38\u662f[batch_size, channels, length]\u3002</p> </li> <li> <p>\u81ea\u6211\u6761\u4ef6\uff08self-condition\uff09\uff1aUnet\u548cUnet1D\u7c7b\u90fd\u652f\u6301\u81ea\u6211\u6761\u4ef6\uff08self-condition\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u8ba9\u6a21\u578b\u80fd\u591f\u5904\u7406\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u7684\u65b9\u6cd5\u3002\u5982\u679c\u542f\u7528\u4e86\u81ea\u6211\u6761\u4ef6\uff0c\u90a3\u4e48\u6a21\u578b\u7684\u8f93\u5165\u6570\u636e\u4f1a\u5305\u542b\u4e24\u90e8\u5206\uff1a\u4e00\u90e8\u5206\u662f\u539f\u59cb\u7684\u8f93\u5165\u6570\u636e\uff0c\u53e6\u4e00\u90e8\u5206\u662f\u81ea\u6211\u6761\u4ef6\u7684\u6570\u636e\u3002</p> </li> <li> <p>\u8f93\u51fa\u6570\u636e\u7684\u5f62\u72b6\uff1aUnet\u7c7b\u7684\u8f93\u51fa\u6570\u636e\u7684\u5f62\u72b6\u662f[batch_size, out_dim, height, width]\uff0c\u800cUnet1D\u7c7b\u7684\u8f93\u51fa\u6570\u636e\u7684\u5f62\u72b6\u662f[batch_size, out_dim, length]\u3002</p> </li> </ol> <p>\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cU-Net1D\u6a21\u578b\u8fd8\u5305\u542b\u4e86\u4e00\u4e9b\u7279\u6b8a\u7684\u8bbe\u8ba1\uff0c\u5982\u4f7f\u7528\u4e86\u4f4d\u7f6e\u5d4c\u5165\uff08Position Embedding\uff09\u6765\u5904\u7406\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4f7f\u7528\u4e86\u6ce8\u610f\u529b\u673a\u5236\uff08Attention\uff09\u6765\u8ba9\u6a21\u578b\u80fd\u591f\u5173\u6ce8\u5230\u5e8f\u5217\u4e2d\u7684\u91cd\u8981\u90e8\u5206\uff0c\u4ee5\u53ca\u4f7f\u7528\u4e86\u6b8b\u5dee\u8fde\u63a5\uff08Residual Connection\uff09\u548c\u5f52\u4e00\u5316\uff08Normalization\uff09\u7b49\u6280\u672f\u6765\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\u3002</p>"},{"location":"AI/Multimodal/Paper%20Reading/A%20Review%20on%20Methods%20and%20Applications%20in%20Multimodal%20Deep%20Learning/","title":"A Review on Methods and Applications in Multimodal Deep Learning","text":"<p>\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u548c\u5e94\u7528\u7684\u56de\u987e</p> <p>Abstract</p> <p>\u6df1\u5ea6\u5b66\u4e60\u5df2\u7ecf\u5b9e\u73b0\u4e86\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u5e76\u5728\u8fd1\u5e74\u6765\u53d8\u5f97\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\u3002\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\uff08MMDL\uff09\u7684\u76ee\u6807\u662f\u521b\u5efa\u80fd\u591f\u5904\u7406\u548c\u94fe\u63a5\u4f7f\u7528\u5404\u79cd\u6a21\u6001\u7684\u4fe1\u606f\u7684\u6a21\u578b\u3002\u5c3d\u7ba1\u5bf9\u5355\u6a21\u6001\u5b66\u4e60\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5f00\u53d1\uff0c\u4f46\u5b83\u4ecd\u7136\u65e0\u6cd5\u6db5\u76d6\u4eba\u7c7b\u5b66\u4e60\u7684\u6240\u6709\u65b9\u9762\u3002\u5f53\u5404\u79cd\u611f\u5b98\u53c2\u4e0e\u4fe1\u606f\u5904\u7406\u65f6\uff0c\u591a\u6a21\u6001\u5b66\u4e60\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u548c\u5206\u6790\u3002\u672c\u6587\u5173\u6ce8\u591a\u79cd\u7c7b\u578b\u7684\u6a21\u6001\uff0c\u5373\u56fe\u50cf\u3001\u89c6\u9891\u3001\u6587\u672c\u3001\u97f3\u9891\u3001\u8eab\u4f53\u624b\u52bf\u3001\u9762\u90e8\u8868\u60c5\u548c\u751f\u7406\u4fe1\u53f7\u3002\u63d0\u4f9b\u4e86\u5bf9\u57fa\u7ebf\u65b9\u6cd5\u7684\u8be6\u7ec6\u5206\u6790\u548c\u5bf9\u8fc7\u53bb\u4e94\u5e74\uff082017\u5e74\u81f32021\u5e74\uff09\u5728\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\u7684\u6df1\u5165\u7814\u7a76\u3002\u63d0\u51fa\u4e86\u5404\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff0c\u8be6\u7ec6\u9610\u8ff0\u4e86\u4e0d\u540c\u5e94\u7528\u3002\u6700\u540e\uff0c\u5206\u522b\u7a81\u51fa\u4e86\u6bcf\u4e2a\u9886\u57df\u7684\u4e3b\u8981\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u80fd\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002</p> <p>Motivation</p> <p>\u6df1\u5ea6\u5b66\u4e60\u5df2\u7ecf\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u7684\u5b9e\u65bd\uff0c\u5e76\u5728\u8fd1\u5e74\u6765\u53d8\u5f97\u8d8a\u6765\u8d8a\u6d41\u884c\u3002\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\uff08MMDL\uff09\u7684\u76ee\u6807\u662f\u521b\u5efa\u80fd\u591f\u5904\u7406\u548c\u94fe\u63a5\u4f7f\u7528\u5404\u79cd\u6a21\u6001\u7684\u4fe1\u606f\u7684\u6a21\u578b\u3002\u5c3d\u7ba1\u5bf9\u5355\u6a21\u6001\u5b66\u4e60\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5f00\u53d1\uff0c\u4f46\u5b83\u4ecd\u7136\u65e0\u6cd5\u6db5\u76d6\u4eba\u7c7b\u5b66\u4e60\u7684\u6240\u6709\u65b9\u9762\u3002\u591a\u6a21\u6001\u5b66\u4e60\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u548c\u5206\u6790\uff0c\u5f53\u5404\u79cd\u611f\u5b98\u53c2\u4e0e\u4fe1\u606f\u5904\u7406\u65f6\u3002</p> <p>Brief Outline Of Topic</p> <ol> <li>\u7b80\u4ecb (Introduction)</li> <li>\u672c\u6587\u5173\u6ce8\u591a\u79cd\u7c7b\u578b\u7684\u6a21\u6001\uff0c\u5373\u56fe\u50cf\u3001\u89c6\u9891\u3001\u6587\u672c\u3001\u97f3\u9891\u3001\u8eab\u4f53\u624b\u52bf\u3001\u9762\u90e8\u8868\u60c5\u548c\u751f\u7406\u4fe1\u53f7\u3002\u63d0\u4f9b\u4e86\u5bf9\u57fa\u7ebf\u65b9\u6cd5\u7684\u8be6\u7ec6\u5206\u6790\u548c\u5bf9\u8fc7\u53bb\u4e94\u5e74\uff082017\u5e74\u81f32021\u5e74\uff09\u5728\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\u7684\u6df1\u5165\u7814\u7a76\u3002</li> <li>\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5 (Multimodal Deep Learning Methods)</li> <li>\u63d0\u51fa\u4e86\u5404\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff0c\u8be6\u7ec6\u9610\u8ff0\u4e86\u4e0d\u540c\u5e94\u7528\u3002</li> <li>\u4e3b\u8981\u95ee\u9898 (Main Issues)</li> <li>\u5206\u522b\u7a81\u51fa\u4e86\u6bcf\u4e2a\u9886\u57df\u7684\u4e3b\u8981\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u80fd\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002</li> </ol> <p>Contents</p> <ol> <li>\u591a\u6a21\u6001\u56fe\u50cf\u63cf\u8ff0 (Multimodal Image Description, MMID)\uff1a</li> </ol> <p>\u56fe\u50cf\u63cf\u8ff0\u4e3b\u8981\u7528\u4e8e\u751f\u6210\u8f93\u5165\u56fe\u50cf\u7684\u89c6\u89c9\u5185\u5bb9\u7684\u6587\u672c\u63cf\u8ff0\u3002\u5728\u6df1\u5ea6\u5b66\u4e60\u65f6\u4ee3\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\uff08CV\uff09\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e24\u4e2a\u4e0d\u540c\u7684\u9886\u57df\u88ab\u5408\u5e76\u6765\u8fdb\u884c\u56fe\u50cf\u63cf\u8ff0\u3002\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u4e3b\u8981\u4f7f\u7528\u4e86\u4e24\u79cd\u6a21\u6001\uff0c\u5373\u56fe\u50cf\u548c\u6587\u672c\u3002\u56fe\u50cf\u63cf\u8ff0\u6846\u67b6\u88ab\u5206\u7c7b\u4e3a\u57fa\u4e8e\u68c0\u7d22\u7684\u3001\u57fa\u4e8e\u6a21\u677f\u7684\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u63cf\u8ff0\u3002\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u8be6\u7ec6\u89e3\u91ca\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u63cf\u8ff0\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u8fdb\u4e00\u6b65\u88ab\u5206\u7c7b\u4e3a\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7684\u3001\u57fa\u4e8e\u8bed\u4e49\u6982\u5ff5\u7684\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u56fe\u50cf\u63cf\u8ff0\u3002</p> <ul> <li>\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7684\u56fe\u50cf\u63cf\u8ff0 (Encoder-Decoder based Image Description, EDID)\uff1aEDID\u5728\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u56fe\u50cf\u6807\u6ce8\u4efb\u52a1\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002CNN\u67b6\u6784\u4e3b\u8981\u7528\u4f5c\u7f16\u7801\u5668\u90e8\u5206\uff0c\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u548c\u7f16\u7801\u6570\u636e\uff0c\u800cRNN\u67b6\u6784\u7528\u4f5c\u89e3\u7801\u5668\u90e8\u5206\uff0c\u89e3\u7801\u5e76\u751f\u6210\u6807\u6ce8\u3002<ul> <li>J. Wu\u548cH.hu [106] \u63d0\u51fa\u4e86\u4e00\u4e2a \u7ea7\u8054\u9012\u5f52\u795e\u7ecf\u7f51\u7edc (Cascade Recurrent Neural Network, CRNN) \u7528\u4e8e\u56fe\u50cf\u63cf\u8ff0\u3002CRNN\u91c7\u7528\u7ea7\u8054\u7f51\u7edc\u4ece\u524d\u5411\u548c\u540e\u5411\u65b9\u5411\u5b66\u4e60\u89c6\u89c9\u8bed\u8a00\u4ea4\u4e92\u3002</li> <li>M. Chen\u7b49\u4eba [21] \u63d0\u51fa\u4e86\u4e00\u4e2a \u57fa\u4e8e\u53c2\u8003\u7684LSTM\u6a21\u578b \u7528\u4e8e\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u3002\u5728\u8fd9\u4e2a\u6a21\u578b\u4e2d\uff0c\u8bad\u7ec3\u56fe\u50cf\u88ab\u7528\u4f5c\u63d0\u51fa\u7684\u6846\u67b6\u7684\u53c2\u8003\uff0c\u4ee5\u6700\u5c0f\u5316\u63cf\u8ff0\u4efb\u52a1\u7684\u8bef\u8bc6\u522b\u3002</li> <li>W. Jiang\u7b49\u4eba [49] \u63d0\u51fa\u4e86\u4e00\u4e2a \u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7684\u9012\u5f52\u878d\u5408\u7f51\u7edc \u7528\u4e8e\u56fe\u50cf\u6807\u6ce8\u4efb\u52a1\u3002\u5728\u8fd9\u4e2a\u7f51\u7edc\u4e2d\uff0cCNN\u67b6\u6784\u88ab\u7528\u6765\u4ece\u8f93\u5165\u56fe\u50cf\u4e2d\u63d0\u53d6\u4fe1\u606f\uff0cRNN\u67b6\u6784\u88ab\u7528\u6765\u751f\u6210\u6587\u672c\u5f62\u5f0f\u7684\u63cf\u8ff0\u3002</li> <li>L. Guo\u7b49\u4eba [34] \u63d0\u51fa\u4e86\u4e00\u4e2a \u4f7f\u7528CNN\u7684\u591a\u6837\u5f0f\u56fe\u50cf\u6807\u6ce8\u6846\u67b6\u3002</li> </ul> </li> <li>\u57fa\u4e8e\u8bed\u4e49\u6982\u5ff5\u7684\u56fe\u50cf\u63cf\u8ff0 (Semantic Concept-based Image Description, SCID)\uff1aSCID\u65b9\u6cd5\u9009\u62e9\u6027\u5730\u5904\u7406\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684\u4e00\u7ec4\u8bed\u4e49\u6982\u5ff5\u3002\u8fd9\u4e9b\u6982\u5ff5\u5728\u7f16\u7801\u9636\u6bb5\u4e0e\u56fe\u50cf\u7684\u5176\u4ed6\u7279\u5f81\u4e00\u8d77\u88ab\u63d0\u53d6\uff0c\u7136\u540e\u88ab\u5408\u5e76\u5230\u8bed\u8a00\u6a21\u578b\u7684\u9690\u85cf\u72b6\u6001\u4e2d\uff0c\u8f93\u51fa\u88ab\u7528\u6765\u751f\u6210\u57fa\u4e8e\u8bed\u4e49\u6982\u5ff5\u7684\u56fe\u50cf\u63cf\u8ff0\u3002<ul> <li>W. Wang\u7b49\u4eba[101]\u63d0\u51fa\u4e86\u4e00\u4e2a \u57fa\u4e8e\u5c5e\u6027\u7684\u56fe\u50cf\u5b57\u5e55\u751f\u6210\u6846\u67b6 \u3002\u901a\u8fc7\u4f7f\u7528\u663e\u8457\u7684\u8bed\u4e49\u5c5e\u6027\u6765\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3aLSTM\u7f16\u7801\u5668\u7684\u8f93\u5165\u3002</li> <li>Z. Zhang\u7b49\u4eba[118]\u63d0\u51fa\u4e86\u4e00\u4e2a \u57fa\u4e8e\u8bed\u4e49\u5f15\u5bfc\u7684\u89c6\u89c9\u6ce8\u610f\u673a\u5236\u7684\u56fe\u50cf\u5b57\u5e55\u6a21\u578b \u3002\u5168\u5377\u79ef\u7f51\u7edc\uff08FCN\uff09\u4e3b\u8981\u7528\u4e8e\u8bed\u4e49\u5206\u5272\uff0c\u7279\u522b\u662f\u7528\u4e8e\u5bc6\u96c6\u50cf\u7d20\u7ea7\u7279\u5f81\u63d0\u53d6\u548c\u7a7a\u95f4\u7f51\u683c\u5f62\u5f0f\u7684\u8bed\u4e49\u6807\u7b7e\u3002</li> <li>P. Cao\u7b49\u4eba[17]\u63d0\u51fa\u4e86\u4e00\u4e2a \u57fa\u4e8e\u8bed\u4e49\u7684\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b \u3002\u5728\u8fd9\u4e2a\u6a21\u578b\u4e2d\uff0c\u4f7f\u7528\u57fa\u4e8e\u8bed\u4e49\u6ce8\u610f\u529b\u7684\u5f15\u5bfc\u5bf9LSTM\u67b6\u6784\u8fdb\u884c\u63cf\u8ff0\u56fe\u50cf\u3002</li> <li>L. Cheng\u7b49\u4eba[23]\u63d0\u51fa\u4e86\u4e00\u4e2a \u57fa\u4e8e\u591a\u9636\u6bb5\u89c6\u89c9\u8bed\u4e49\u6ce8\u610f\u673a\u5236\u7684\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b \u3002\u5728\u8fd9\u79cd\u65b9\u6cd5\u4e2d\uff0c\u5c06\u81ea\u4e0a\u800c\u4e0b\u548c\u81ea\u4e0b\u800c\u4e0a\u7684\u6ce8\u610f\u6a21\u5757\u7ed3\u5408\u8d77\u6765\uff0c\u63a7\u5236\u89c6\u89c9\u548c\u8bed\u4e49\u7ea7\u522b\u7684\u4fe1\u606f\uff0c\u4ee5\u4ea7\u751f\u7ec6\u7c92\u5ea6\u7684\u56fe\u50cf\u63cf\u8ff0\u3002</li> <li>L. Chen\u7b49\u4eba[20]\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u578b\uff0c\u901a\u8fc7 \u5f15\u5165\u52a8\u8bcd\u7279\u5b9a\u7684\u8bed\u4e49\u89d2\u8272\uff08VSR\uff09\uff0c\u63d0\u9ad8\u4e86\u56fe\u50cf\u5b57\u5e55\u7684\u51c6\u786e\u6027\u3002\u8be5\u6a21\u578b\u9488\u5bf9\u7279\u5b9a\u52a8\u4f5c\u4e2d\u7684\u6d3b\u52a8\u548c\u5b9e\u4f53\u89d2\u8272\uff0c\u4ee5\u63d0\u53d6\u548c\u751f\u6210\u56fe\u50cf\u4e2d\u6700\u5177\u4f53\u7684\u4fe1\u606f\u3002</li> </ul> </li> <li> <p>\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u56fe\u50cf\u63cf\u8ff0 (Attention-based Image Description, AID)\uff1aAID\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u56e0\u4e3a\u5b83\u901a\u8fc7\u6839\u636e\u5b83\u4eec\u7684\u4e0a\u4e0b\u6587\u5173\u6ce8\u56fe\u50cf\u7684\u4e0d\u540c\u533a\u57df\u6765\u5e2e\u52a9\u56fe\u50cf\u63cf\u8ff0\u8fc7\u7a0b\u3002\u8fd1\u5e74\u6765\uff0c\u5df2\u7ecf\u63d0\u51fa\u4e86\u5404\u79cd\u6280\u672f\u6765\u901a\u8fc7\u5e94\u7528\u6ce8\u610f\u529b\u673a\u5236\u66f4\u597d\u5730\u63cf\u8ff0\u56fe\u50cf\u3002</p> <ul> <li>L. Li\u7b49\u4eba[58]\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528 \u5c40\u90e8\u548c\u5168\u5c40\u6ce8\u610f\u673a\u5236\u6765\u63cf\u8ff0\u56fe\u50cf\u3002\u6839\u636e\u4e0a\u4e0b\u6587\uff0c\u5c06\u9009\u62e9\u6027\u7684**\u5bf9\u8c61\u7ea7\u7279\u5f81\u4e0e\u56fe\u50cf\u7ea7\u7279\u5f81\u7ed3\u5408**\u8d77\u6765\u3002</li> <li>P. Anderson\u7b49\u4eba[3]\u63d0\u51fa\u4e86\u4e00\u4e2a \u57fa\u4e8e\u81ea\u4e0b\u800c\u4e0a\u548c\u81ea\u4e0a\u800c\u4e0b\u7684\u6ce8\u610f\u529b\u7684\u56fe\u50cf\u63cf\u8ff0\u6846\u67b6\uff0c\u4ee5\u4fc3\u8fdb\u5bf9\u56fe\u50cf\u7684\u66f4\u6df1\u5165\u7406\u89e3\u548c\u63a8\u7406\u3002</li> <li>M. Liu\u7b49\u4eba\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e \u53cc\u91cd\u6ce8\u610f\u673a\u5236\u7684\u6846\u67b6 \u6765\u63cf\u8ff0\u4e2d\u6587[62]\u548c\u82f1\u6587[63]\u7684\u56fe\u50cf\u3002\u6587\u672c\u6ce8\u610f\u673a\u5236\u7528\u4e8e\u63d0\u9ad8\u6570\u636e\u7684\u53ef\u4fe1\u5ea6\uff0c\u89c6\u89c9\u6ce8\u610f\u673a\u5236\u7528\u4e8e\u6df1\u5165\u7406\u89e3\u56fe\u50cf\u7279\u5f81\u3002</li> <li>B. Wang\u7b49\u4eba[98]\u63d0\u51fa\u4e86\u4e00\u4e2a \u4f7f\u7528\u8bed\u4e49\u6ce8\u610f\u673a\u5236\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5 \u8fdb\u884c\u56fe\u50cf\u63cf\u8ff0\u3002\u5728\u8fd9\u79cd\u65b9\u6cd5\u4e2d\uff0c\u4f7f\u7528\u6ce8\u610f\u673a\u5236\u4ece\u7279\u5b9a\u7684\u56fe\u50cf\u533a\u57df\u63d0\u53d6\u7279\u5f81\uff0c\u4ee5\u751f\u6210\u76f8\u5e94\u7684\u63cf\u8ff0\u3002</li> <li>Y. Wei\u7b49\u4eba[105]\u63d0\u51fa\u4e86\u4e00\u4e2a\u56fe\u50cf\u63cf\u8ff0\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528 \u591a\u6ce8\u610f\u529b\u673a\u5236\u6765\u63d0\u53d6\u5c40\u90e8\u548c\u975e\u5c40\u90e8\u7279\u5f81\u8868\u793a \u3002</li> <li>W. Jiang\u7b49\u4eba[48]\u63d0\u51fa\u4e86\u4e00\u4e2a \u591a\u95e8\u81ea\u6211\u6ce8\u610f\u673a\u5236\u7684\u6269\u5c55\u3002\u5728\u8fd9\u4e2a\u7f51\u7edc\u4e2d\uff0c\u901a\u8fc7\u6dfb\u52a0\u81ea\u6211\u95e8\u63a7\u6a21\u5757\u548c\u6ce8\u610f\u529b\u6743\u91cd\u95e8\u63a7\u6a21\u5757\u6765\u6269\u5c55\u6ce8\u610f\u673a\u5236\uff0c\u4ee5\u6d88\u9664\u63cf\u8ff0\u4e2d\u7684\u65e0\u5173\u4fe1\u606f\u3002</li> </ul> </li> <li> <p>\u591a\u6a21\u6001\u89c6\u9891\u63cf\u8ff0\uff08MMVD\uff09\uff1a</p> </li> </ul> <p>\u4e0e\u56fe\u50cf\u63cf\u8ff0\u7c7b\u4f3c\uff0c\u89c6\u9891\u63cf\u8ff0\u7528\u4e8e\u751f\u6210\u8f93\u5165\u89c6\u9891\u7684\u89c6\u89c9\u5185\u5bb9\u7684\u6587\u672c\u63cf\u8ff0\u3002\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u4e3b\u8981\u4f7f\u7528\u4e24\u79cd\u6a21\u6001\uff0c\u5373\u89c6\u9891\u6d41\u548c\u6587\u672c\u3002\u89c6\u9891\u63cf\u8ff0\u65b9\u6cd5\u6839\u636e\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u548c\u6587\u672c\u751f\u6210\u7684\u4e0d\u540c\u67b6\u6784\u7ec4\u5408\u8fdb\u884c\u5206\u7c7b\u3002</p> <ul> <li>CNN-RNN\u67b6\u6784\uff1a\u5728\u89c6\u9891\u63cf\u8ff0\u9886\u57df\uff0cCNN-RNN\u662f\u6700\u5e7f\u6cdb\u4f7f\u7528\u7684\u67b6\u6784\u7ec4\u5408\u3002\u5728\u89c6\u89c9\u63d0\u53d6\uff08\u7f16\u7801\u5668\uff09\u9636\u6bb5\u4f7f\u7528CNN\u67b6\u6784\u7684\u53d8\u4f53\uff0c\u5728\u53e5\u5b50\u751f\u6210\uff08\u89e3\u7801\u5668\uff09\u9636\u6bb5\u4f7f\u7528RNN\u67b6\u6784\u7684\u53d8\u4f53\u3002\u5728\u6df1\u5ea6\u5b66\u4e60\u65f6\u4ee3\uff0c\u8bb8\u591a\u4f5c\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u8fd9\u79cd\u7f16\u7801\u5668\u3001\u89e3\u7801\u5668\u7ec4\u5408\u7684\u89c6\u9891\u63cf\u8ff0\u6280\u672f\u3002<ul> <li>R Krishna\u7b49\u4eba[52]\u63d0\u51fa\u4e86\u4e00\u79cd \u4f7f\u7528\u5bc6\u96c6\u5b57\u5e55\u673a\u5236\u8fdb\u884c\u52a8\u4f5c/\u4e8b\u4ef6\u68c0\u6d4b\u7684\u89c6\u9891\u63cf\u8ff0\u6280\u672f\u3002B. Wang\u7b49\u4eba[97]\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668-\u91cd\u6784\u5668\u67b6\u6784\u7684\u89c6\u9891\u63cf\u8ff0\u91cd\u6784\u7f51\u7edc\u3002</li> <li>W. Pei\u7b49\u4eba[81]\u63d0\u51fa\u4e86\u4e00\u4e2a \u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6 \u8fdb\u884c\u89c6\u9891\u63cf\u8ff0\u3002</li> <li>N Aafaq\u7b49\u4eba[1]\u63d0\u51fa\u4e86\u4e00\u4e2a\u89c6\u9891\u5b57\u5e55\u6846\u67b6\uff0c\u4f7f\u75282D\u548c3D\u7684CNNs\u5c42\u6b21\u5316\u5730\u63d0\u53d6\u89c6\u9891\u7684\u65f6\u7a7a\u52a8\u6001 \u4ee5\u83b7\u53d6\u9ad8\u7ea7\u8bed\u4e49\uff0cGRU\u7528\u4e8e\u6587\u672c\u751f\u6210\u90e8\u5206\u3002</li> <li>S. Liu\u7b49\u4eba[64]\u63d0\u51fa\u4e86SibNet\uff0c\u4e00\u4e2a \u7528\u4e8e\u89c6\u9891\u63cf\u8ff0\u7684\u5144\u5f1f\u5377\u79ef\u7f51\u7edc\u3002</li> <li>J. Perez-Martin\u7b49\u4eba[82]\u901a\u8fc7\u5b9e\u65bd \u89c6\u89c9\u53e5\u6cd5\u5d4c\u5165 \u6765\u63d0\u9ad8\u89c6\u89c9\u5b57\u5e55\u7684\u8d28\u91cf\u3002</li> </ul> </li> <li>RNN-RNN\u67b6\u6784\uff1a\u5728\u6df1\u5ea6\u5b66\u4e60\u65f6\u4ee3\uff0cRNN-RNN\u4e5f\u662f\u4e00\u79cd\u6d41\u884c\u7684\u67b6\u6784\u7ec4\u5408\uff0c\u56e0\u4e3a\u8bb8\u591a\u4f5c\u8005\u901a\u8fc7\u4f7f\u7528\u8fd9\u79cd\u7ec4\u5408\u63d0\u51fa\u4e86\u5404\u79cd\u65b9\u6cd5\u3002\u4f5c\u8005\u4f7f\u7528RNN\u67b6\u6784\u800c\u4e0d\u662fCNN\u6765\u63d0\u53d6\u89c6\u9891\u7684\u89c6\u89c9\u5185\u5bb9\u3002\u5728\u89c6\u89c9\u63d0\u53d6\uff08\u7f16\u7801\u5668\uff09\u548c\u53e5\u5b50\u751f\u6210\uff08\u89e3\u7801\u5668\uff09\u9636\u6bb5\u90fd\u4f7f\u7528RNN\u67b6\u6784\u7684\u53d8\u4f53\u3002<ul> <li>M. Rahman\u7b49\u4eba[85]\u63d0\u51fa\u4e86\u4e00\u4e2a\u89c6\u9891\u5b57\u5e55\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528 \u7a7a\u95f4\u786c\u62c9\u548c\u5806\u53e0\u6ce8\u610f\u529b\u673a\u5236 \u4fee\u6539\u751f\u6210\u7684\u4e0a\u4e0b\u6587\u3002</li> <li>Z. Fang\u7b49\u4eba[29]\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u751f\u6210\u8f93\u5165\u89c6\u9891\u7684 \u5e38\u8bc6\u5b57\u5e55\u3002</li> <li>Z. Zhang\u7b49\u4eba[119]\u63d0\u51fa\u4e86\u4e00\u4e2a \u57fa\u4e8e\u7f16\u7801\u5668\u89e3\u7801\u5668 \u7684\u5bc6\u96c6\u89c6\u9891\u5b57\u5e55\u6846\u67b6\u3002</li> </ul> </li> <li> <p>\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u67b6\u6784\uff1aDRL\u662f\u4e00\u79cd\u5b66\u4e60\u673a\u5236\uff0c\u5176\u4e2d\u673a\u5668\u53ef\u4ee5\u50cf\u4eba\u7c7b\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u4e00\u6837\u4ece\u884c\u52a8\u4e2d\u5b66\u4e60\u667a\u80fd\u3002\u5728\u8fd9\u79cd\u673a\u5236\u4e2d\uff0c\u5982\u679c\u4e00\u4e2a\u4ee3\u7406\u7684\u884c\u52a8\u4f7f\u6a21\u578b\u66f4\u63a5\u8fd1\u76ee\u6807\u7ed3\u679c\uff0c\u90a3\u4e48\u5c31\u4f1a\u5956\u52b1\u6216\u60e9\u7f5a\u8be5\u4ee3\u7406\u3002\u4f7f\u7528DRL\u67b6\u6784\u7684\u4f5c\u8005\u7684\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a</p> <ul> <li>X. Wang\u7b49\u4eba[102]\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u63cf\u8ff0\u89c6\u9891\u7684 \u57fa\u4e8e\u5c42\u6b21\u7684\u5f3a\u5316\u5b66\u4e60\uff08HRL\uff09\u6a21\u578b\u3002</li> <li>Y. Chen\u7b49\u4eba[22]\u63d0\u51fa\u4e86\u4e00\u4e2a \u57fa\u4e8eRL\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u8f93\u5165\u89c6\u9891\u4e2d\u9009\u62e9\u4fe1\u606f\u5e27\u3002L. Li\u548cB. Gong[57]\u63d0\u51fa\u4e86\u4e00\u4e2aE2E\u591a\u4efb\u52a1RL\u6846\u67b6\u8fdb\u884c\u89c6\u9891\u63cf\u8ff0\u3002</li> <li>J. Mun\u7b49\u4eba[74]\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5176\u4e2d\u4f7f\u7528 \u4e8b\u4ef6\u5e8f\u5217\u751f\u6210\u7f51\u7edc\u6765\u76d1\u63a7 \u4e3a\u89c6\u9891\u751f\u6210\u7684\u5b57\u5e55\u7684\u4e00\u7cfb\u5217\u4e8b\u4ef6\u3002</li> <li>W. Zhang\u7b49\u4eba[117]\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u63cf\u8ff0\u89c6\u89c9\u5185\u5bb9\u7684 \u91cd\u6784\u7f51\u7edc\u3002</li> <li>W. Xu\u7b49\u4eba[108]\u63d0\u51fa\u4e86\u4e00\u4e2a \u4f7f\u7528RL\u6280\u672f\u7cbe\u70bc\u751f\u6210\u5b57\u5e55\u7684\u629b\u5149\u7f51\u7edc\u3002</li> <li>R. Wei\u7b49\u4eba[104]\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u597d\u5730\u63a2\u7d22 RL\u4e8b\u4ef6\uff0c\u4ee5\u751f\u6210\u66f4\u51c6\u786e\u548c\u8be6\u7ec6\u7684\u89c6\u9891\u5b57\u5e55\u3002</li> </ul> </li> <li> <p>\u591a\u6a21\u6001\u89c6\u89c9\u95ee\u9898\u56de\u7b54\uff08MMVQA\uff09\uff1a</p> </li> </ul> <p>VQA\u662f\u4e00\u4e2a\u65b0\u5174\u7684\u6280\u672f\uff0c\u5b83\u5f15\u8d77\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u56e2\u961f\u7684\u5174\u8da3\u3002\u5b83\u662f\u5173\u4e8e\u521b\u5efa\u4e00\u4e2a\u80fd\u591f\u56de\u7b54\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u7684AI\u7cfb\u7edf\u7684\u7814\u7a76\u9886\u57df\u3002\u4ece\u8f93\u5165\u7684\u56fe\u50cf/\u89c6\u9891\u548c\u95ee\u9898\u4e2d\u63d0\u53d6\u7684\u7279\u5f81\u88ab\u5904\u7406\u548c\u7ec4\u5408\uff0c\u4ee5\u56de\u7b54\u5173\u4e8e\u56fe\u50cf\u7684\u95ee\u9898\u3002VQA\u6bd4\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u3001\u89c6\u9891\u5b57\u5e55\u3001\u56fe\u50cf\u5b57\u5e55\u7b49\u5176\u4ed6\u89c6\u89c9\u548c\u8bed\u8a00\u529f\u80fd\u66f4\u590d\u6742\uff0c\u56e0\u4e3a\uff1a</p> <p>\uff081\uff09\u5728VQA\u4e2d\u63d0\u51fa\u7684\u95ee\u9898\u4e0d\u662f\u7279\u5b9a\u7684\u6216\u9884\u5148\u786e\u5b9a\u7684\u3002 </p> <p>\uff082\uff09VQA\u4e2d\u7684\u89c6\u89c9\u4fe1\u606f\u7ef4\u5ea6\u8f83\u9ad8\u3002\u901a\u5e38\uff0cVQA\u9700\u8981\u5bf9\u56fe\u50cf/\u89c6\u9891\u6709\u66f4\u6df1\u5165\u548c\u8be6\u7ec6\u7684\u7406\u89e3\u3002 </p> <p>\uff083\uff09VQA\u89e3\u51b3\u4e86\u591a\u4e2a\u8ba1\u7b97\u673a\u89c6\u89c9\u5b50\u4efb\u52a1\u3002</p> <p>\u8bb8\u591a\u4f5c\u8005\u4f7f\u7528\u5404\u79cd\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u4e3aVQA\u9886\u57df\u505a\u51fa\u4e86\u8d21\u732e\u3002\u8fd9\u4e9b\u65b9\u6cd5\u88ab\u5206\u4e3a\u4e09\u7ec4\uff0c\u5373\u591a\u6a21\u6001\u8054\u5408\u5d4c\u5165\u6a21\u578b\u3001\u591a\u6a21\u6001\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u548c\u591a\u6a21\u6001\u5916\u90e8\u77e5\u8bc6\u5e93\u6a21\u578b\u3002</p> <ul> <li>\u591a\u6a21\u6001\u8054\u5408\u5d4c\u5165\u6a21\u578b\uff08MMJEM\uff09\uff1aMMJEM\u5728\u4e00\u4e2a\u5171\u4eab\u7279\u5f81\u7a7a\u95f4\u4e2d\u8054\u5408\u5e76\u5b66\u4e60\u591a\u6a21\u6001\u7684\u8868\u793a\u3002\u5728VQA\u4e2d\uff0c\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u5728\u6a21\u6001\u4e0a\u8fdb\u884c\u66f4\u591a\u7684\u63a8\u7406\uff0c\u6bd4\u56fe\u50cf/\u89c6\u9891\u63cf\u8ff0\u8fdb\u4e00\u6b65\u6539\u8fdb\u4e86\u8fd9\u79cd\u7406\u5ff5\u3002<ul> <li>H. Ben-Younes\u7b49\u4eba[12]\u63d0\u51fa\u4e86\u4e00\u4e2a \u7528\u4e8eVQA\u7684MUTAN\u6846\u67b6\u3002\u4f7f\u7528\u57fa\u4e8e\u5f20\u91cf\u7684tucker\u5206\u89e3\u6a21\u578b\uff0c\u901a\u8fc7\u4f4e\u79e9\u77e9\u9635\u7ea6\u675f\u6765\u53c2\u6570\u5316\u89c6\u89c9\u548c\u6587\u672c\u89e3\u91ca\u4e4b\u95f4\u7684\u53cc\u7ebf\u6027\u5173\u7cfb\u3002</li> <li>MT Desta\u7b49\u4eba[27]\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06 \u89c6\u89c9\u7279\u5f81\u548c\u8bed\u8a00\u4e0e\u62bd\u8c61\u63a8\u7406\u5408\u5e76\u3002\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684\u9ad8\u7ea7\u62bd\u8c61\u4e8b\u5b9e\u4f18\u5316\u4e86\u63a8\u7406\u8fc7\u7a0b\u3002</li> <li>R. Cadene\u7b49\u4eba[16]\u63d0\u51fa\u4e86\u4e00\u4e2a \u7528\u4e8eVQA\u7684\u7aef\u5230\u7aef\u63a8\u7406\u7f51\u7edc\u3002\u8fd9\u9879\u7814\u7a76\u7684\u4e3b\u8981\u8d21\u732e\u662f\u5f15\u5165\u4e86MuRel\u5355\u5143\uff0c\u8be5\u5355\u5143\u4ea7\u751f\u4e86\u95ee\u9898\u548c\u76f8\u5e94\u56fe\u50cf\u533a\u57df\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002</li> <li>B. Patro\u7b49\u4eba[80]\u63d0\u51fa\u4e86\u4e00\u4e2a \u8054\u5408\u7b54\u6848\u548c\u6587\u672c\u89e3\u91ca\u751f\u6210\u6a21\u578b\u3002\u4f7f\u7528\u534f\u4f5c\u76f8\u5173\uff08\u7f16\u7801\u5668\uff0c\u751f\u6210\u5668\u548c\u76f8\u5173\uff09\u6a21\u5757\u6765\u786e\u4fdd\u7b54\u6848\u53ca\u5176\u751f\u6210\u7684\u89e3\u91ca\u662f\u6b63\u786e\u548c\u8fde\u8d2f\u7684\u3002</li> <li>S. Lobry\u7b49\u4eba[65]\u63d0\u51fa\u4e86\u4e00\u4e2a \u7528\u4e8e\u9065\u611f\u6570\u636e\u7684VQA\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u7528\u4e8e\u571f\u5730\u8986\u76d6\u5206\u7c7b\u4efb\u52a1\u3002</li> <li>Z. Fang\u7b49\u4eba[29]\u63d0\u51fa\u4e86\u4e00\u4e2a \u4f7f\u7528\u5e38\u8bc6\u63a8\u7406\u7684\u5f00\u653e\u5f0fVQA\u6846\u67b6\uff0c\u5176\u4e2d\u95ee\u9898\u662f\u5173\u4e8e\u5f71\u54cd\u3001\u610f\u56fe\u548c\u5c5e\u6027\u7684\u3002</li> </ul> </li> <li>\u591a\u6a21\u6001\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\uff08MMAM\uff09\uff1a\u5728\u7f16\u7801\u9636\u6bb5\uff0c\u4e00\u822c\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u53ef\u80fd\u4f1a\u5728\u9884\u6d4b\u9636\u6bb5\u63d0\u4f9b\u4e00\u4e9b\u566a\u58f0\u548c\u4e0d\u5fc5\u8981\u7684\u4fe1\u606f\u3002MMAM\u88ab\u8bbe\u8ba1\u7528\u6765\u6539\u8fdb\u901a\u7528\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u4ee5\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\u3002\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e3b\u8981\u76ee\u6807\u662f\u4f7f\u7528\u56fe\u50cf/\u89c6\u9891\u7684\u5c40\u90e8\u7279\u5f81\uff0c\u5e76\u5141\u8bb8\u7cfb\u7edf\u4e3a\u4ece\u4e0d\u540c\u533a\u57df\u63d0\u53d6\u7684\u7279\u5f81\u5206\u914d\u4f18\u5148\u7ea7\u3002\u8fd9\u4e2a\u6982\u5ff5\u4e5f\u88ab\u7528\u4e8eVQA\uff0c\u901a\u8fc7\u5173\u6ce8\u56fe\u50cf\u7684\u7279\u5b9a\u90e8\u5206\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u3002<ul> <li>P. Wang\u7b49\u4eba[100]\u63d0\u51fa\u4e86\u4e00\u4e2a \u57fa\u4e8e\u5171\u6ce8\u610f\u673a\u5236\u7684VQA\u6846\u67b6\u3002\u5728\u8fd9\u4e2a\u6846\u67b6\u4e2d\uff0c\u5171\u6ce8\u610f\u673a\u5236\u53ef\u4ee5\u5904\u7406\u4e8b\u5b9e\u3001\u56fe\u50cf\u548c\u95ee\u9898\u7684\u66f4\u9ad8\u9636\u3002</li> <li>Z. Yu\u7b49\u4eba[113]\u63d0\u51fa\u4e86\u4e00\u4e2a \u4f7f\u7528\u5171\u6ce8\u610f\u5b66\u4e60\u7684\u56e0\u5b50\u5316\u53cc\u7ebf\u6027\u6c60\u5316\u65b9\u6cd5 \u8fdb\u884cVQA\u4efb\u52a1\u3002\u53cc\u7ebf\u6027\u6c60\u5316\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684\u7ebf\u6027\u65b9\u6cd5\uff0c\u4f46\u7531\u4e8e\u5176\u9ad8\u8ba1\u7b97\u590d\u6742\u6027\u548c\u9ad8\u7ef4\u8868\u793a\uff0c\u5176\u5b9e\u9645\u5e94\u7528\u6027\u53d7\u5230\u9650\u5236\u3002</li> <li>P. Anderson\u7b49\u4eba[3]\u63d0\u51fa\u4e86\u4e00\u4e2a \u57fa\u4e8e\u81ea\u4e0b\u800c\u4e0a\u548c\u81ea\u4e0a\u800c\u4e0b\u7684\u6ce8\u610f\u529b\u7684VQA\u6846\u67b6\u3002\u8fd9\u79cd\u6ce8\u610f\u529b\u673a\u5236\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u5bf9\u8c61\u6216\u663e\u8457\u7684\u56fe\u50cf\u533a\u57df\u8ba1\u7b97\u7279\u5f81\u3002</li> <li>Z. Yu\u7b49\u4eba[112]\u63d0\u51fa\u4e86\u4e00\u4e2a \u7528\u4e8eVQA\u4efb\u52a1\u7684\u6df1\u5ea6\u6a21\u5757\u5316\u5171\u6ce8\u610f\u7f51\u7edc\u3002\u6bcf\u4e2a\u6a21\u5757\u5316\u5171\u6ce8\u610f\u5c42\u90fd\u5305\u542b\u4e86\u95ee\u9898\u5f15\u5bfc\u7684\u56fe\u50cf\u81ea\u6211\u6ce8\u610f\u673a\u5236\uff0c\u4f7f\u7528\u56fe\u50cf\u548c\u95ee\u9898\u6ce8\u610f\u5355\u5143\u7684\u6a21\u5757\u5316\u7ec4\u5408\u3002</li> <li>L. Li\u7b49\u4eba[56]\u63d0\u51fa\u4e86\u4e00\u4e2a \u5173\u7cfb\u611f\u77e5\u56fe\u6ce8\u610f\u673a\u5236 \u8fdb\u884cVQA\u3002\u8fd9\u4e2a\u6846\u67b6\u5c06\u56fe\u50cf\u7684\u89c6\u89c9\u7279\u5f81\u7f16\u7801\u6210\u4e00\u4e2a\u56fe\uff0c\u4e3a\u4e86\u5b66\u4e60\u95ee\u9898\u9002\u5e94\u7684\u5173\u7cfb\u8868\u793a\uff0c\u4e00\u4e2a\u56fe\u6ce8\u610f\u673a\u5236\u6a21\u62df\u4e86\u5bf9\u8c61\u95f4\u7684\u5173\u7cfb\u3002</li> <li>W. Guo\u7b49\u4eba[36]\u63d0\u51fa\u4e86\u4e00\u4e2a \u57fa\u4e8e\u91cd\u65b0\u6ce8\u610f\u7684\u673a\u5236\u8fdb\u884cVQA\u3002\u6ce8\u610f\u6a21\u5757\u5c06\u5bf9\u8c61-\u8bcd\u7684\u5bf9\u5e94\u5173\u7cfb\u76f8\u5173\u8054\uff0c\u5e76\u5728\u5f7c\u6b64\u7684\u5f15\u5bfc\u4e0b\u4e3a\u95ee\u9898\u548c\u56fe\u50cf\u751f\u6210\u6ce8\u610f\u56fe\u3002</li> </ul> </li> <li> <p>\u591a\u6a21\u6001\u5916\u90e8\u77e5\u8bc6\u5e93\u6a21\u578b\uff08MMEKM\uff09\uff1a\u4f20\u7edf\u7684\u591a\u6a21\u6001\u8054\u5408\u5d4c\u5165\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u53ea\u4ece\u8bad\u7ec3\u96c6\u4e2d\u5b58\u5728\u7684\u4fe1\u606f\u4e2d\u5b66\u4e60\u3002\u73b0\u6709\u7684\u6570\u636e\u96c6\u5e76\u672a\u8986\u76d6\u6240\u6709\u7684\u771f\u5b9e\u4e16\u754c\u4e8b\u4ef6/\u6d3b\u52a8\u3002\u56e0\u6b64\uff0cMMEKM\u5bf9\u4e8e\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u7684\u573a\u666f\u81f3\u5173\u91cd\u8981\u3002\u901a\u8fc7\u5c06\u77e5\u8bc6\u5e93\uff08KB\uff09\u6570\u636e\u5e93\u94fe\u63a5\u5230VQA\u4efb\u52a1\uff0c\u53ef\u4ee5\u66f4\u5927\u5730\u63d0\u9ad8VQA\u4efb\u52a1\u7684\u6027\u80fd\u3002Freebase [15]\uff0cDBPedia [7]\uff0cWordNet [69]\uff0cConceptNet [61]\u548cWebChild [92]\u662f\u5e7f\u6cdb\u4f7f\u7528\u7684KB\u3002\u4e00\u4e2a\u5f3a\u5927\u7684VQA\u6846\u67b6\u9700\u8981\u8bbf\u95ee\u6765\u81eaKB\u7684\u5e7f\u6cdb\u4fe1\u606f\u5185\u5bb9\u3002\u5b83\u5df2\u7ecf\u88ab\u6709\u6548\u5730\u6574\u5408\u5230VQA\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u5d4c\u5165\u5404\u79cd\u5b9e\u4f53\u548c\u5173\u7cfb\u3002\u5728\u6df1\u5ea6\u5b66\u4e60\u65f6\u4ee3\uff0c\u4e3aVQA\u4efb\u52a1\u63d0\u51fa\u4e86\u5404\u79cd\u5916\u90e8KB\u65b9\u6cd5\u3002</p> <ul> <li>P. Wang\u7b49\u4eba[99]\u63d0\u51fa\u4e86\u53e6\u4e00\u4e2a\u7528\u4e8eVQA\u4efb\u52a1\u7684\u6846\u67b6\uff0c\u540d\u4e3a\"\u57fa\u4e8e\u4e8b\u5b9e\u7684VQA\uff08FVQA\uff09\"\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u548cLSTM\u67b6\u6784\u6765\u6620\u5c04\u56fe\u50cf/\u95ee\u9898\u67e5\u8be2\u3002FVQA\u6846\u67b6\u4f7f\u7528\u4e86DBPedia\uff0cConceptNet\u548cWebChild KB\u3002</li> <li>M. Narasimhan\u548cAG. Schwing [75]\u63d0\u51fa\u4e86\u4e00\u4e2a \u4f7f\u7528\u5916\u90e8\u77e5\u8bc6\u8d44\u6e90\u7684VQA\u6846\u67b6\uff0c\u8be5\u8d44\u6e90\u5305\u542b\u4e00\u7ec4\u4e8b\u5b9e\u3002\u8fd9\u4e2a\u6846\u67b6\u53ef\u4ee5\u56de\u7b54\u57fa\u4e8e\u4e8b\u5b9e\u548c\u57fa\u4e8e\u89c6\u89c9\u7684\u95ee\u9898\u3002</li> <li>K. Marino\u7b49\u4eba[67]\u63d0\u51fa\u4e86\u4e00\u4e2a \u7528\u4e8eVQA\u7684\u5916\u90e8\u77e5\u8bc6\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc714,000\u4e2a\u95ee\u9898\u3002\u8fd9\u4e2a\u6570\u636e\u96c6\u5305\u542b\u4e86\u8bb8\u591a\u7c7b\u522b\uff0c\u5982\u4f53\u80b2\u3001\u79d1\u5b66\u548c\u6280\u672f\u3001\u5386\u53f2\u7b49\u3002\u8fd9\u4e2a\u6570\u636e\u96c6\u9700\u8981\u4f7f\u7528\u5916\u90e8\u8d44\u6e90\u6765\u56de\u7b54\u95ee\u9898\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u7406\u89e3\u95ee\u9898\u548c\u56fe\u50cf\u7279\u5f81\u3002</li> <li>K. Basu\u7b49\u4eba[11]\u63d0\u51fa\u4e86\u4e00\u4e2a \u57fa\u4e8e\u5e38\u8bc6\u7684VQA\u6846\u67b6\u3002\u5728\u8fd9\u4e2a\u6846\u67b6\u4e2d\uff0c\u56fe\u50cf\u7684\u89c6\u89c9\u5185\u5bb9\u88abYOLO\u6846\u67b6\u63d0\u53d6\u548c\u7406\u89e3\uff0c\u5e76\u5728\u7b54\u6848\u96c6\u7a0b\u5e8f\u4e2d\u8868\u793a\u3002\u8bed\u4e49\u5173\u7cfb\u7279\u5f81\u548c\u989d\u5916\u7684\u5e38\u8bc6\u77e5\u8bc6\u56de\u7b54\u4e86\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u7684\u590d\u6742\u95ee\u9898\u3002</li> <li>J. Yu\u7b49\u4eba[111]\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5728\u8be5\u6846\u67b6\u4e2d\uff0c\u56fe\u50cf\u7684 \u89c6\u89c9\u5185\u5bb9 \u88ab\u63d0\u53d6\u5e76\u5728\u77e5\u8bc6\u56fe\u7684\u591a\u4e2a\u89c6\u89d2\u4e0b\u5904\u7406\uff0c\u5982\u8bed\u4e49\u3001\u89c6\u89c9\u548c\u4e8b\u5b9e\u89c6\u89d2\u3002</li> </ul> </li> <li> <p>\u591a\u6a21\u6001\u8bed\u97f3\u5408\u6210\uff08MMSS\uff09\uff1a </p> </li> </ul> <p>MMSS\u662f\u4eba\u7c7b\u884c\u4e3a\u4e2d\u6700\u91cd\u8981\u7684\u4e00\u90e8\u5206\uff0c\u5373\u901a\u4fe1\uff08\u5199\u4f5c/\u8bb2\u8bdd\uff09\u3002\u4eba\u7c7b\u53ef\u4ee5\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u7684\u6587\u672c\u548c\u8bed\u97f3\u8fdb\u884c\u4ea4\u6d41\uff0c\u5206\u522b\u4ee3\u8868\u81ea\u7136\u8bed\u8a00\u7684\u4e66\u9762\u5f62\u5f0f\u548c\u53d1\u58f0\u5f62\u5f0f\u3002\u6700\u65b0\u7684\u8bed\u8a00\u548c\u8bed\u97f3\u5904\u7406\u7814\u7a76\u5e2e\u52a9\u7cfb\u7edf\u50cf\u4eba\u7c7b\u4e00\u6837\u4ea4\u8c08\u3002\u8bed\u97f3\u5408\u6210\u662f\u751f\u6210\u673a\u5668\u53d1\u51fa\u7684\u81ea\u7136\u8bed\u8a00\u7684\u590d\u6742\u8fc7\u7a0b\u3002\u6587\u672c\u8f6c\u8bed\u97f3\uff08TTS\uff09\u7cfb\u7edf\u5c06\u81ea\u7136\u8bed\u8a00\u6587\u672c\u6a21\u6001\u5b9e\u65f6\u8f6c\u6362\u4e3a\u5176\u76f8\u5e94\u7684\u8bed\u97f3\u6ce2\u5f62\u6a21\u6001\u3002\u4f7f\u7528\u8bed\u97f3\u5408\u6210\u5f15\u5165\u4e86\u5404\u79cd\u5b9e\u9645\u5e94\u7528\uff0c\u5982\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\uff0c\u5c4f\u5e55\u9605\u8bfb\u5668\uff0c\u7535\u4fe1\u548c\u591a\u5a92\u4f53\u5e94\u7528\uff0c\u8bf4\u8bdd\u7684\u73a9\u5177\u6e38\u620f\u7b49\u3002</p> <p>\u76ee\u524dTTS\u7cfb\u7edf\u7684\u4e3b\u8981\u7814\u7a76\u76ee\u6807\u662f\u4ea7\u751f\u50cf\u4eba\u7c7b\u4e00\u6837\u7684\u58f0\u97f3\u3002\u56e0\u6b64\uff0c\u7528\u4e8e\u8bc4\u4f30TTS\u7cfb\u7edf\u8d28\u91cf\u7684\u5404\u79cd\u65b9\u9762\uff0c\u5982\u81ea\u7136\u6027\uff08\u4ece\u751f\u6210\u7684\u8bed\u97f3\u65f6\u5e8f\u7ed3\u6784\uff0c\u6e32\u67d3\u60c5\u7eea\u548c\u53d1\u97f3\u7684\u89d2\u5ea6\u770b\u7684\u8d28\u91cf\uff09\uff0c\u53ef\u7406\u89e3\u6027\uff08\u5728\u53e5\u5b50\u4e2d\u4ea7\u751f\u7684\u6bcf\u4e2a\u5355\u8bcd\u7684\u8d28\u91cf\uff09\uff0c\u5408\u6210\u8bed\u97f3\u504f\u597d\uff08\u542c\u8005\u5728\u8bed\u97f3\u548c\u4fe1\u53f7\u8d28\u91cf\u65b9\u9762\u7684\u9009\u62e9\uff0c\u4ee5\u4fbf\u66f4\u597d\u7684TTS\u7cfb\u7edf\uff09\u548c\u4eba\u7c7b\u611f\u77e5\u56e0\u7d20\uff0c\u5982\u53ef\u7406\u89e3\u6027\uff08\u63a5\u6536\u5230\u7684\u6d88\u606f\u7684\u7406\u89e3\u8d28\u91cf\uff09\u3002\u8bed\u97f3\u5408\u6210\u8fc7\u7a0b\u7684\u4e3b\u8981\u7c7b\u522b\u5305\u62ec\u53d1\u97f3TTS\uff0c\u8fde\u63a5TTS\uff0c\u5171\u632f\u5cf0TTS\uff0c\u53c2\u6570TTS\u548c\u6df1\u5ea6\u5b66\u4e60TTS\u3002</p> <ul> <li> <p>\u6df1\u5ea6\u5b66\u4e60TTS\uff08DLTTS\uff09\uff1a\u5728DLTTS\u6846\u67b6\u4e2d\uff0cDNN\u67b6\u6784\u6a21\u62df\u4e86\u6587\u672c\u53ca\u5176\u58f0\u5b66\u5b9e\u73b0\u4e4b\u95f4\u7684\u5173\u7cfb\u3002DLTTS\u7684\u4e3b\u8981\u4f18\u70b9\u662f\u5728\u6ca1\u6709\u4eba\u7c7b\u9884\u5904\u7406\u7684\u60c5\u51b5\u4e0b\u5f00\u53d1\u5176\u5e7f\u6cdb\u7684\u7279\u6027\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u8fd9\u4e9b\u7cfb\u7edf\u53ef\u4ee5\u63d0\u9ad8\u8bed\u97f3\u7684\u81ea\u7136\u6027\u548c\u53ef\u7406\u89e3\u6027\u3002\u6df1\u5ea6\u5b66\u4e60\u6587\u672c\u8f6c\u8bed\u97f3\u6846\u67b6\u4f7f\u7528DNN\u67b6\u6784\u89e3\u91ca\u4e86\u6587\u672c\u8f6c\u8bed\u97f3\u5408\u6210\u8fc7\u7a0b\u3002</p> <ul> <li>Y. Wang\u7b49\u4eba[103]\u63d0\u51fa\u4e86\"Tacotron\"\uff0c\u8fd9\u662f\u4e00\u4e2a \u5e8f\u5217\u5230\u5e8f\u5217\u7684TTS\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u6587\u672c\u548c\u97f3\u9891\u5bf9\u5408\u6210\u8bed\u97f3\u3002\u7f16\u7801\u5668\u5d4c\u5165\u4e86\u63d0\u53d6\u5176\u987a\u5e8f\u8868\u793a\u7684\u6587\u672c\u3002\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u89e3\u7801\u5668\u5904\u7406\u8fd9\u4e9b\u8868\u793a\uff0c\u7136\u540e\u540e\u5904\u7406\u67b6\u6784\u751f\u6210\u5408\u6210\u7684\u6ce2\u5f62\u3002</li> <li>SO Arik\u7b49\u4eba[5]\u63d0\u51fa\u4e86 \u4f7f\u7528DNN\u67b6\u6784\u7684\"Deep Voice\"\u6a21\u578b \u6765\u5408\u6210\u6765\u81ea\u5b57\u7b26\u7684\u97f3\u9891\u3002\u8fd9\u4e2a\u6a21\u578b\u7531\u4e94\u4e2a\u4e3b\u8981\u7684\u5757\u7ec4\u6210\uff0c\u7528\u4e8e\u4ece\u6587\u672c\u751f\u6210\u5408\u6210\u8bed\u97f3\u3002\u4e0e\u73b0\u6709\u7684\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u8ba1\u7b97\u901f\u5ea6\u589e\u52a0\uff0c\u56e0\u4e3a\u8be5\u6a21\u578b\u53ef\u4ee5\u5728\u6ca1\u6709\u4eba\u7c7b\u53c2\u4e0e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u8bad\u7ec3\u3002</li> <li>A. Gibiansky\u7b49\u4eba[33]\u63d0\u51fa\u4e86 Deep Voice-2\u67b6\u6784\u3002\u8be5\u6846\u67b6\u65e8\u5728\u901a\u8fc7\u6269\u5c55\u591a\u626c\u58f0\u5668TTS\u6765\u6539\u8fdb\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5373Tacotron\u548cDeep Voice-1\uff0c\u901a\u8fc7\u4f4e\u7ef4\u53ef\u8bad\u7ec3\u7684\u626c\u58f0\u5668\u5d4c\u5165\u3002\u5728Deep Voice\u7684\u7b2c\u4e09\u4e2a\u7248\u672c\u4e2d\uff0c</li> <li>W. Ping\u7b49\u4eba[84]\u63d0\u51fa\u4e86\u4e00\u4e2a \u57fa\u4e8e\u5168\u5377\u79ef\u6a21\u578b\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u795e\u7ecfTTS\u7cfb\u7edf\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u9002\u5e94Griffin-Lim\u5149\u8c31\u53cd\u8f6c\uff0cWORLD\u548cWaveNet\u58f0\u7801\u5668\u8bed\u97f3\u5408\u6210\u6765\u6267\u884c\u5e76\u884c\u8ba1\u7b97\u3002</li> <li>A. Oord\u7b49\u4eba[78]\u63d0\u51fa\u4e86 WaveNet\u7684\u9ad8\u7ea7\u7248\u672c\"Parallel WaveNet\"\uff0c\u4f7f\u7528\u6982\u7387\u5bc6\u5ea6\u5206\u5e03\u65b9\u6cd5\u6765\u8bad\u7ec3\u7f51\u7edc\u3002\u5728\u8fd9\u4e2a\u6a21\u578b\u4e2d\uff0c\u6559\u5e08\u548c\u5b66\u751fWaveNet\u5e76\u884c\u4f7f\u7528\u3002</li> <li>SO Arik\u7b49\u4eba[4]\u63d0\u51fa\u4e86\u4e00\u4e2a \u795e\u7ecf\u8bed\u97f3\u514b\u9686\u7cfb\u7edf\uff0c\u53ef\u4ee5\u4ece\u5c11\u91cf\u6837\u672c\u4e2d\u5b66\u4e60\u4eba\u7c7b\u7684\u58f0\u97f3\u3002\u4e3a\u6b64\uff0c\u4e00\u8d77\u4f7f\u7528\u4e86\u4e24\u79cd\u6280\u672f\uff0c\u5373\u626c\u58f0\u5668\u9002\u5e94\u548c\u7f16\u7801\u3002</li> <li>Y. Taigman\u7b49\u4eba[91]\u63d0\u51fa\u4e86\u4e00\u4e2a VoiceLoop\u6846\u67b6 \u7528\u4e8eTTS\u7cfb\u7edf\u3002\u8fd9\u4e2a\u6a21\u578b\u53ef\u4ee5\u5904\u7406\u65e0\u7ea6\u675f\u7684\u8bed\u97f3\u6837\u672c\uff0c\u800c\u4e0d\u9700\u8981\u8bed\u8a00\u7279\u6027\u6216\u5bf9\u9f50\u7684\u97f3\u7d20\u3002\u8fd9\u4e2a\u6846\u67b6\u4f7f\u7528\u77ed\u79fb\u52a8\u5185\u5b58\u7f13\u51b2\u533a\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u8bed\u97f3\u3002</li> <li>J. Shen\u7b49\u4eba[88]\u63d0\u51fa\u4e86\"Tacotron2\"\u3002\u8fd9\u662f\u4e00\u4e2a \u795e\u7ecfTTS\u67b6\u6784\uff0c\u7528\u4e8e\u76f4\u63a5\u4ece\u6587\u672c\u5408\u6210\u8bed\u97f3\u3002\u4e00\u4e2a\u57fa\u4e8e\u5faa\u73af\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u7279\u6027\u9884\u6d4b\u7f51\u7edc\u53ef\u4ee5\u5c06\u5b57\u7b26\u6620\u5c04\u5230\u5149\u8c31\u56fe\uff0c\u7136\u540e\u8fd9\u4e9b\u5149\u8c31\u56fe\u88ab\u7528\u6765\u901a\u8fc7\u4f7f\u7528WaveNet\u58f0\u7801\u5668\u7684\u4fee\u6539\u7248\u672c\u6765\u5408\u6210\u6ce2\u5f62\u3002</li> <li>F. Tao\u548cC. Busso[93]\u63d0\u51fa\u4e86\u4e00\u4e2a \u4f7f\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u673a\u5236\u7684\u8bed\u97f3\u8bc6\u522b \u7cfb\u7edf\u3002\u6240\u63d0\u51fa\u7684\u8bbe\u8ba1\u8003\u8651\u4e86\u8de8\u6a21\u6001\u548c\u6a21\u6001\u5185\u7684\u65f6\u95f4\u52a8\u6001\uff0c\u4ece\u800c\u4ea7\u751f\u4e86\u4e00\u4e2a\u5438\u5f15\u4eba\u7684\u548c\u53ef\u884c\u7684\u878d\u5408\u65b9\u6cd5\u3002</li> <li>Parallel Tacotron \u662f\u6700\u8fd1\u4e00\u6b21\u5bf9\u795e\u7ecfTTS\u65b9\u6cd5\u7684\u51fa\u8272\u53d1\u660e\uff0c\u7531I. Elias\u7b49\u4eba[28]\u63d0\u51fa\u3002\u5728\u63a8\u7406\u548c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u8fd9\u79cd\u65b9\u6cd5\u662f\u9ad8\u5ea6\u5e76\u884c\u5316\u7684\uff0c\u4ee5\u5728\u73b0\u4ee3\u786c\u4ef6\u4e0a\u5b9e\u73b0\u6700\u4f73\u5408\u6210\u3002VAE\u7684\u4e00\u5bf9\u591a\u6620\u5c04\u6027\u8d28\u63d0\u9ad8\u4e86TTS\u7684\u6027\u80fd\uff0c\u4e5f\u63d0\u9ad8\u4e86\u5176\u81ea\u7136\u6027\u3002</li> </ul> </li> <li> <p>Other MMDL Applications</p> </li> <li> <p>\u591a\u6a21\u6001\u60c5\u7eea\u8bc6\u522b\uff08MMER\uff09\uff1aMMER\u662f\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u7684\u91cd\u8981\u65b9\u5f0f\uff0c\u5b83\u4f7f\u8ba1\u7b97\u673a\u80fd\u591f\u901a\u8fc7\u8bad\u7ec3\u6570\u636e\u96c6\u5b66\u4e60\u548c\u8bc6\u522b\u65b0\u8f93\u5165\uff0c\u4ece\u800c\u6709\u6548\u5730\u68c0\u6d4b\u3001\u5904\u7406\u3001\u54cd\u5e94\u3001\u7406\u89e3\u548c\u8bc6\u522b\u4eba\u7c7b\u7684\u60c5\u7eea\u3002\u60c5\u611f\u8ba1\u7b97\u7684\u4e3b\u8981\u76ee\u6807\u662f\u8d4b\u4e88\u673a\u5668/\u7cfb\u7edf\u60c5\u7eea\u667a\u80fd\u80fd\u529b\u3002\u591a\u6a21\u6001\u60c5\u7eea\u8bc6\u522b\u6846\u67b6\u53ef\u4ee5\u57fa\u4e8eAI/ML\u539f\u578b\uff0c\u4ece\u5404\u79cd\u6a21\u6001\uff08\u5982\u8bed\u97f3\u3001\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u3001\u9762\u90e8\u8868\u60c5\u3001\u8eab\u4f53\u59ff\u52bf\u548c\u751f\u7406\u4fe1\u53f7\uff09\u4e2d\u63d0\u53d6\u548c\u5904\u7406\u60c5\u7eea\u4fe1\u606f\u3002</p> </li> <li>Y. Huang\u7b49\u4eba[45]\u63d0\u51fa\u4e86\u4e00\u79cd \u4f7f\u7528\u9762\u90e8\u8868\u60c5\u548c\u8111\u7535\u56fe\uff08EEG\uff09\u4fe1\u53f7\u7684\u60c5\u7eea\u8bc6\u522b\u878d\u5408\u65b9\u6cd5\u3002</li> <li>D. Nguyen\u7b49\u4eba[77]\u548c[76]\u63d0\u51fa\u4e86 \u4f7f\u7528\u97f3\u9891\u548c\u89c6\u9891\u6d41\u8fdb\u884c\u60c5\u7eea\u8bc6\u522b\u7684\u65b9\u6cd5\u3002</li> <li>S. Tripathi\u7b49\u4eba[94]\u63d0\u51fa\u4e86\u4e00\u4e2a \u4f7f\u7528\u6587\u672c\u3001\u8bed\u97f3\u3001\u9762\u90e8\u8868\u60c5\u548c\u624b\u90e8\u52a8\u4f5c\u7b49\u591a\u79cd\u6a21\u6001\u7684\u60c5\u7eea\u8bc6\u522b\u6846\u67b6\u3002</li> <li>D. Hazarika\u7b49\u4eba[39]\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ece \u89c6\u9891\u5bf9\u8bdd\u4e2d\u8bc6\u522b\u60c5\u7eea \u7684\u6846\u67b6\uff0c\u4ed6\u4eec\u8fd8\u63d0\u51fa\u4e86\u53e6\u4e00\u4e2a \u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u60c5\u7eea\u68c0\u6d4b \u7684\u6846\u67b6[38]\u3002</li> <li>M. Jaiswal\u7b49\u4eba[46]\u5206\u6790\u4e86\u4e2a\u4f53\u5728\u5404\u79cd\u538b\u529b\u6c34\u5e73\u4e0b \u60c5\u7eea\u8868\u8fbe\u7684\u53d8\u5316\u3002</li> <li>L. Chong\u7b49\u4eba[24]\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684 \u5728\u7ebf\u804a\u5929\u7cfb\u7edf\"EmoChat\"\uff0c\u53ef\u4ee5 \u81ea\u52a8\u8bc6\u522b\u7528\u6237\u7684\u60c5\u7eea \u5e76\u5728\u77ed\u65f6\u95f4\u5185\u9644\u52a0\u8bc6\u522b\u7684\u60c5\u7eea\u548c\u53d1\u9001\u7684\u6d88\u606f\u3002</li> <li>M. Li\u7b49\u4eba[59]\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6b65\u6df1\u5ea6\u7cfb\u7edf\uff0c\u7528\u4e8e\u901a\u8fc7 \u6536\u96c6\u5305\u542b\u65e0\u6548\u6570\u636e\u7684\u6570\u636e\u6765\u53ef\u9760\u5730\u68c0\u6d4b\u60c5\u7eea\u3002</li> <li>H. Lai\u7b49\u4eba[53]\u63d0\u51fa\u4e86\u4e00\u4e2a \u7528\u4e8e\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u4e2d\u7684\u60c5\u7eea\u8bc6\u522b \u7684\u6a21\u578b\u3002</li> <li>RH Huan\u7b49\u4eba[43]\u63d0\u51fa\u4e86\u4e00\u4e2a \u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236 \u7684\u6a21\u578b\u3002</li> <li>Y Cimtay\u7b49\u4eba[25]\u63d0\u51fa\u4e86\u4e00\u79cd \u4f7f\u7528\u9762\u90e8\u8868\u60c5\u3001\u76ae\u80a4\u7535\u53cd\u5e94\uff08GSR\uff09\u548cEEG\u4fe1\u53f7\u7684\u4e09\u79cd\u6a21\u6001\u7684\u60c5\u7eea\u8bc6\u522b\u7684\u6df7\u5408\u878d\u5408\u65b9\u6cd5\u3002</li> <li>\u591a\u6a21\u6001\u4e8b\u4ef6\u68c0\u6d4b\uff08MMED\uff09\uff1a\u7531\u4e8e\u4e92\u8054\u7f51\u4e0a\u7684\u5a92\u4f53\u5171\u4eab\u7684\u666e\u53ca\uff0c\u7528\u6237\u53ef\u4ee5\u968f\u65f6\u5206\u4eab\u4ed6\u4eec\u7684\u4e8b\u4ef6\u3001\u6d3b\u52a8\u548c\u60f3\u6cd5\u3002MMED\u7cfb\u7edf\u7684\u76ee\u6807\u662f\u4ece\u591a\u79cd\u6a21\u6001\uff08\u5982\u56fe\u50cf\u3001\u89c6\u9891\u3001\u97f3\u9891\u3001\u6587\u672c\u7b49\uff09\u4e2d\u627e\u5230\u52a8\u4f5c\u548c\u4e8b\u4ef6\u3002\u81ea\u52a8\u4ece\u5927\u91cf\u7528\u6237\u751f\u6210\u7684\u89c6\u9891\u4e2d\u68c0\u6d4b\u4e8b\u4ef6\u548c\u52a8\u4f5c\u7684\u673a\u5236\u5728\u8bb8\u591a\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u4e2d\u662f\u5fc5\u9700\u7684\u3002\u4ece\u8fd9\u5927\u91cf\u7684\u6570\u636e\u4e2d\u627e\u5230\u4e8b\u4ef6\u548c\u52a8\u4f5c\u662f\u4e00\u9879\u590d\u6742\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u5b83\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6709\u5404\u79cd\u5e94\u7528\uff0c\u5982\u75be\u75c5\u76d1\u6d4b\u3001\u6cbb\u7406\u3001\u5546\u4e1a\u7b49\uff0c\u4e5f\u5e2e\u52a9\u4e92\u8054\u7f51\u7528\u6237\u7406\u89e3\u548c\u6355\u6349\u4e16\u754c\u5404\u5730\u7684\u53d1\u751f\u7684\u4e8b\u60c5\u3002</li> <li>Y. Gao\u7b49\u4eba[32]\u63d0\u51fa\u4e86\u4e00\u79cd \u4f7f\u7528\u5fae\u535a\u4e2d\u7684\u56fe\u50cf\u548c\u6587\u672c\u5a92\u4f53\u8fdb\u884c\u4e8b\u4ef6\u5206\u7c7b \u7684\u65b9\u6cd5\u3002</li> <li>S. Huang\u7b49\u4eba[44]\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784 \u4ece\u62e5\u6324\u573a\u666f\u4e2d\u68c0\u6d4b\u5f02\u5e38\u4e8b\u4ef6\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u3002</li> <li>P. Koutras\u7b49\u4eba[51]\u63d0\u51fa\u4e86\u4e00\u79cd \u4f7f\u7528\u97f3\u9891\u548c\u89c6\u9891\u6a21\u6001\u68c0\u6d4b\u89c6\u9891\u4e2d\u663e\u8457\u4e8b\u4ef6\u7684\u65b9\u6cd5\u3002</li> <li>Z. Yang\u7b49\u4eba[109]\u63d0\u51fa\u4e86\u4e00\u4e2a \u4ece\u591a\u4e2a\u6570\u636e\u9886\u57df\uff08\u5982\u793e\u4ea4\u548c\u65b0\u95fb\u5a92\u4f53\uff09\u68c0\u6d4b\u771f\u5b9e\u4e16\u754c\u4e8b\u4ef6 \u7684\u6846\u67b6\u3002</li> </ul>"},{"location":"AI/NLP/Other/","title":"Other","text":"<p>DGL \u5e93\u6761\u4ef6\u4e0b\u8fdb\u884c\u77e5\u8bc6\u62bd\u53d6\uff1a</p> <pre><code>import dgl\n\ndef extract_multi_hop(graph: dgl.DGLGraph, node_id: int, num_hops: int) -&gt; dgl.DGLGraph:\n\"\"\"\n    \u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u63d0\u53d6\u6307\u5b9a\u8282\u70b9\u7684\u591a\u8df3\u5173\u7cfb\u3002\n\n    Args:\n        graph (dgl.DGLGraph): \u5b58\u50a8\u77e5\u8bc6\u56fe\u8c31\u7684\u56fe\u6570\u636e\u7ed3\u6784\u3002\n        node_id (int): \u5f00\u59cb\u7ed3\u70b9 ID\u3002\n        num_hops (int): \u6307\u5b9a\u7684\u8df3\u6570\u3002\n\n    Returns:\n        dgl.DGLGraph: \u5b58\u50a8\u63d0\u53d6\u51fa\u7684\u5b50\u56fe\u7684\u56fe\u6570\u636e\u7ed3\u6784\u3002\n    \"\"\"\n    # \u5b9a\u4e49\u4e00\u4e2a\u5b50\u56fe\u5bf9\u8c61\uff0c\u7528\u4e8e\u5b58\u50a8\u63d0\u53d6\u51fa\u7684\u5b50\u56fe\u3002\n    subgraph = dgl.DGLGraph()\n\n    # \u5b9a\u4e49\u4e00\u4e2a\u961f\u5217\uff0c\u7528\u4e8e\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u3002\n    queue = [node_id]\n\n    # \u5b9a\u4e49\u4e00\u4e2a\u96c6\u5408\uff0c\u7528\u4e8e\u5b58\u50a8\u5df2\u7ecf\u88ab\u8bbf\u95ee\u8fc7\u7684\u8282\u70b9\u3002\n    visited = set([node_id])\n\n    # \u8fdb\u884c\u591a\u8df3\u641c\u7d22\u3002\n    for i in range(num_hops):\n        # \u4fdd\u5b58\u5f53\u524d\u961f\u5217\u7684\u957f\u5ea6\u3002\n        cur_len = len(queue)\n\n        # \u9010\u4e2a\u8bbf\u95ee\u5f53\u524d\u961f\u5217\u4e2d\u7684\u7ed3\u70b9\u3002\n        for j in range(cur_len):\n            # \u4ece\u961f\u5217\u4e2d\u53d6\u51fa\u4e00\u4e2a\u7ed3\u70b9\u3002\n            cur_node = queue.pop(0)\n\n            # \u5c06\u5f53\u524d\u7ed3\u70b9\u52a0\u5165\u5230\u5b50\u56fe\u4e2d\u3002\n            subgraph.add_nodes(1)\n            subgraph.nodes[i].data['id'] = torch.tensor([cur_node])\n\n            # \u904d\u5386\u5f53\u524d\u7ed3\u70b9\u7684\u90bb\u5c45\u7ed3\u70b9\u3002\n            neighbors = graph.successors(cur_node)\n            for neighbor in neighbors:\n                # \u5982\u679c\u5f53\u524d\u90bb\u5c45\u7ed3\u70b9\u8fd8\u6ca1\u6709\u88ab\u8bbf\u95ee\u8fc7\uff0c\u5219\u5c06\u5176\u52a0\u5165\u5230\u961f\u5217\u548c\u96c6\u5408\u4e2d\u3002\n                if neighbor not in visited:\n                    visited.add(neighbor)\n                    queue.append(neighbor)\n\n                    # \u5728\u5b50\u56fe\u4e2d\u6dfb\u52a0\u90bb\u5c45\u7ed3\u70b9\u3002\n                    subgraph.add_nodes(1)\n                    subgraph.nodes[i+1].data['id'] = torch.tensor([neighbor])\n                    subgraph.add_edge(i, i+1)\n\n    return subgraph\n</code></pre> <p>\u5728\u5305\u542b\u591a\u8df3\u5173\u7cfb\u7684\u77e5\u8bc6\u56fe\u8c31\u4e2d\u62bd\u53d6\u5b50\u56fe\uff1a</p> <pre><code>import torch\nimport torch.nn.functional as F\n\n# \u5b9a\u4e49\u4e09\u5143\u7ec4\u8868\u793a\u6cd5\uff0c\u5373(head, relation, tail)\nclass TripletsDataset(torch.utils.data.Dataset):\n    def __init__(self, triplets):\n        self.triplets = triplets\n\n    def __getitem__(self, index):\n        return self.triplets[index]\n\n    def __len__(self):\n        return len(self.triplets)\n\n# \u5b9a\u4e49\u57fa\u4e8e\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u7684\u51fd\u6570\uff0c\u7528\u4e8e\u63d0\u53d6\u6307\u5b9a\u7ed3\u70b9\u7684\u5b50\u56fe\ndef extract_subgraph(start_node, max_depth, triplets):\n    # \u521d\u59cb\u5316\u76f8\u5173\u6570\u636e\u7ed3\u6784\n    subgraph_triplets = []\n    visited_nodes = set()\n    queue = [(start_node, 0)]\n\n    # \u5f00\u59cb\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\n    while queue:\n        node, depth = queue.pop(0)\n        visited_nodes.add(node)\n\n        # \u5224\u65ad\u662f\u5426\u8fbe\u5230\u6307\u5b9a\u6df1\u5ea6\uff0c\u5982\u679c\u8fbe\u5230\u4e86\u5c31\u8df3\u8fc7\n        if depth &gt;= max_depth:\n            continue\n\n        # \u904d\u5386\u6240\u6709\u4e09\u5143\u7ec4\n        for head, relation, tail in triplets:\n            # \u627e\u5230\u6240\u6709\u4e0e\u5f53\u524d\u7ed3\u70b9\u76f8\u8fde\u7684\u4e09\u5143\u7ec4\n            if head == node:\n                subgraph_triplets.append((head, relation, tail))\n\n                # \u5c06\u4e0e\u5f53\u524d\u7ed3\u70b9\u76f8\u8fde\u7684\u672a\u8bbf\u95ee\u7ed3\u70b9\u52a0\u5165\u961f\u5217\u4e2d\n                if tail not in visited_nodes:\n                    queue.append((tail, depth + 1))\n            elif tail == node:\n                subgraph_triplets.append((head, relation, tail))\n\n                # \u5c06\u4e0e\u5f53\u524d\u7ed3\u70b9\u76f8\u8fde\u7684\u672a\u8bbf\u95ee\u7ed3\u70b9\u52a0\u5165\u961f\u5217\u4e2d\n                if head not in visited_nodes:\n                    queue.append((head, depth + 1))\n\n    # \u8fd4\u56de\u5b50\u56fe\u7684\u4e09\u5143\u7ec4\u5217\u8868\n    return subgraph_triplets\n\n# \u5b9a\u4e49\u6a21\u578b\u7c7b\uff0c\u7528\u4e8e\u8868\u793a\u77e5\u8bc6\u56fe\u8c31\nclass KnowledgeGraph(torch.nn.Module):\n    def __init__(self, triplets):\n        super(KnowledgeGraph, self).__init__()\n\n           # \u5c06\u4e09\u5143\u7ec4\u8f6c\u5316\u4e3aPyTorch\u5f20\u91cf\n    self.triplets = torch.tensor(triplets)\n\n    # \u63d0\u53d6\u6240\u6709\u7ed3\u70b9\u7684\u7f16\u53f7\n    nodes = set(self.triplets[:, [0, 2]].flatten().tolist())\n    self.node2index = {node: index for index, node in enumerate(nodes)}\n\n    # \u5c06\u4e09\u5143\u7ec4\u7684\u5934\u5c3e\u7ed3\u70b9\u6620\u5c04\u4e3a\u7f16\u53f7\n    self.triplets[:, [0, 2]] = self.triplets[:, [0, 2]].apply(lambda x: self.node2index[x])\n\n# \u5b9a\u4e49\u524d\u5411\u4f20\u64ad\u51fd\u6570\ndef forward(self, nodes, depth):\n    # \u63d0\u53d6\u6307\u5b9a\u7ed3\u70b9\u7684\u5b50\u56fe\n    subgraph_triplets = []\n    for node in nodes:\n        subgraph_triplets += extract_subgraph(node, depth, self.triplets.tolist())\n\n    # \u5c06\u5b50\u56fe\u8f6c\u5316\u4e3aPyTorch\u5f20\u91cf\n    subgraph_triplets = torch.tensor(subgraph_triplets)\n\n    # \u5c06\u5b50\u56fe\u7684\u5934\u5c3e\u7ed3\u70b9\u6620\u5c04\u4e3a\u7f16\u53f7\n    subgraph_triplets[:, [0, 2]] = subgraph_triplets[:, [0, 2]].apply(lambda x: self.node2index[x])\n\n    # \u5b9a\u4e49\u5173\u7cfb\u77e9\u9635\n    num_nodes = len(self.node2index)\n    num_relations = len(set(self.triplets[:, 1]))\n    relation_matrix = torch.zeros((num_relations, num_nodes, num_nodes))\n\n    # \u5c06\u5b50\u56fe\u7684\u5173\u7cfb\u77e9\u9635\u8d4b\u503c\u7ed9\u603b\u7684\u5173\u7cfb\u77e9\u9635\n    for head, relation, tail in subgraph_triplets:\n        relation_matrix[relation, tail, head] = 1\n\n    # \u8ba1\u7b97\u7ed3\u70b9\u7684\u5d4c\u5165\u5411\u91cf\n    node_embeddings = torch.zeros((num_nodes, 128))\n    for i in range(num_relations):\n        relation_embeddings = torch.randn(128, 128)\n        node_embeddings = torch.matmul(relation_matrix[i], relation_embeddings)\n\n    # \u8fd4\u56de\u6307\u5b9a\u7ed3\u70b9\u7684\u5d4c\u5165\u5411\u91cf\n    node_embeddings = F.normalize(node_embeddings, p=2, dim=1)\n    return node_embeddings[nodes, :]\n</code></pre> <ol> <li>\u5b9a\u4e49\u4e86\u4e09\u5143\u7ec4\u8868\u793a\u6cd5\uff0c\u5373(head, relation, tail)\uff0c\u7528\u4e8e\u8868\u793a\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u5173\u7cfb\u3002</li> <li>\u5b9a\u4e49\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u7684\u51fd\u6570\uff0c\u7528\u4e8e\u63d0\u53d6\u6307\u5b9a\u7ed3\u70b9\u7684\u5b50\u56fe\u3002</li> <li>\u5b9a\u4e49\u4e86\u4e00\u4e2a\u6a21\u578b\u7c7bKnowledgeGraph\uff0c\u7528\u4e8e\u8868\u793a\u6574\u4e2a\u77e5\u8bc6\u56fe\u8c31\u3002\u5176\u4e2d\uff0c\u6a21\u578b\u7684forward\u51fd\u6570\u63a5\u53d7\u4e24\u4e2a\u53c2\u6570\uff0c\u5373\u5f85\u63d0\u53d6\u5b50\u56fe\u7684\u7ed3\u70b9\u548c\u6307\u5b9a\u7684\u6df1\u5ea6\uff0c\u8fd4\u56de\u6307\u5b9a\u7ed3\u70b9\u7684\u5d4c\u5165\u5411\u91cf\u3002</li> </ol>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/","title":"\u4fe1\u606f\u62bd\u53d6","text":"<p>\u4fe1\u606f\u62bd\u53d6\u65e8\u5728\u4ece\u65e0\u7ed3\u6784\u7684\u81ea\u7136\u8bed\u8a00\u6587\u672c\u4e2d\u62bd\u53d6\u51fa\u7ed3\u6784\u5316\u7684\u4fe1\u606f</p> <ol> <li> <p>\u77e5\u8bc6\u62bd\u53d6\u6700\u91cd\u8981\u7684 4 \u4e2a\u5b50\u4efb\u52a1\uff1a</p> <ul> <li> <p>\u5b9e\u4f53\u62bd\u53d6 (\u8bc6\u522b)\uff1a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff0c\u5305\u62ec\u5b9e\u4f53\u7684\u68c0\u6d4b\u4e0e\u5206\u7c7b\uff0c\u4e00\u822c span \u53ca\u5176\u5b9e\u4f53\u7c7b\u522b\u8868\u793a</p> </li> <li> <p>\u5173\u7cfb\u62bd\u53d6\uff1a\u4e00\u822c triplet \u8868\u793a</p> </li> <li> <p>\u4e8b\u4ef6\u62bd\u53d6\uff1a\u591a\u5143\u5173\u7cfb\uff0c\u4e00\u822c record\u8868\u793a</p> </li> <li> <p>\u89c2\u70b9\u62bd\u53d6\uff1a\u4e00\u822c triplet \u8868\u793a</p> </li> </ul> </li> </ol> <p>\u77e5\u8bc6\u62bd\u53d6\u4efb\u52a1</p> <ol> <li> <p>\u76f8\u5173\u7ade\u8d5b\u3001\u6570\u636e\u96c6\uff1a</p> <ul> <li>ACE\uff1a\u5bf9 MUC \u5b9a\u4e49\u4efb\u52a1\u8fdb\u884c\u878d\u5408</li> <li>KBP\uff1a\u56db\u4e2a\u72ec\u7acb\u4efb\u52a1\u3001\u4e00\u4e2a\u6574\u5408\u4efb\u52a1</li> <li>SemEval\uff1a\u8bcd\u4e49\u6d88\u6b67\u8bc4\u6d4b</li> </ul> </li> </ol> <p>\u76f8\u5173\u4f1a\u8bae/\u6570\u636e\u96c6\uff1aMUC\u3001ACE\u3001KBP\u3001SemEval\u3001TREC etc.</p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u5b9e\u4f53\u62bd\u53d6","title":"\u5b9e\u4f53\u62bd\u53d6","text":"<p>\u5b9e\u4f53\u62bd\u53d6/\u547d\u540d\u5b9e\u4f53\u8bc6\u522b (NER) \uff0c\u4e3b\u8981\u62bd\u53d6\u7684\u6587\u672c\u4e2d\u7684\u539f\u5b50\u4fe1\u606f\u5143\u7d20 (\u4eba\u540d\u3001\u7ec4\u7ec7/\u673a\u6784\u540d\u3001\u5730\u7406\u4f4d\u7f6e\u3001\u4e8b\u4ef6/\u65e5\u671f\u3001\u5b57\u7b26\u503c\u3001\u91d1\u989d\u503c etc.)\u3002\u5176\u5173\u952e\u8bcd\u53ef\u4ee5\u603b\u7ed3\u4e3a\u4e24\u4e2a\uff1afind &amp; classify\uff0c\u5373 find \u6587\u672c\u4e2d\u7684 entity\uff0c\u518d\u5bf9\u5176\u8fdb\u884c\u5206\u7c7b\u3002</p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u4e3b\u8981\u5e94\u7528","title":"\u4e3b\u8981\u5e94\u7528","text":"<ul> <li>\u547d\u540d\u5b9e\u4f53\u7528\u4e8e\u7d22\u5f15\u4e0e\u8d85\u94fe\u63a5</li> <li>\u6570\u636e\u5206\u6790\u7684\u51c6\u5907\u6b65\u9aa4</li> <li>\u5173\u7cfb\u62bd\u53d6\u7684\u51c6\u5907\u6b65\u9aa4</li> <li>QA \u7cfb\u7edf\u7684\u7b54\u6848\u5f62\u5f0f\u5927\u591a\u90fd\u4e3a\u547d\u540d\u5b9e\u4f53</li> </ul>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u62bd\u53d6\u65b9\u6cd5","title":"\u62bd\u53d6\u65b9\u6cd5","text":""},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#traditional-machine-learning","title":"Traditional Machine Learning","text":"<p>\u5927\u81f4\u6d41\u7a0b</p> <p>Training</p> <ol> <li>\u6536\u96c6\u5bf9\u5e94\u7684\u8bad\u7ec3\u6587\u6863</li> <li>\u4e3a\u6bcf\u4e2a token \u6807\u8bb0\u547d\u540d\u5b9e\u4f53</li> <li>\u8bbe\u8ba1\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5</li> <li>\u8bad\u7ec3 sequence classifier \u9884\u6d4b\u6570\u636e\u7684 label</li> </ol> <p>Testing</p> <ol> <li>\u6536\u96c6\u5bf9\u5e94\u6d4b\u8bd5\u6587\u6863</li> <li>\u8fd0\u884c sequence classfier \u6807\u8bb0 token</li> <li>\u8f93\u51fa\u547d\u540d\u5b9e\u4f53</li> </ol> <p>\u6807\u6ce8\u4e0e\u7f16\u7801</p> <p>\u8fdb\u884c\u6570\u636e\u6807\u6ce8\u65f6\uff0c\u6700\u5e38\u7528\u7684\u6709\u4e24\u79cd sequence labeling \u7684\u7f16\u7801\u65b9\u5f0f\uff1aIO/IOB encoding</p> <p>\u5bf9\u4e8e IO\uff0c\u82e5\u5355\u8bcd\u4e0d\u4e3a\u547d\u540d\u5b9e\u4f53 (NE) \u5c31\u6807\u4e3a O (other)</p> <p>\u5bf9\u4e8e IOB\uff0c\u4f7f\u7528 I-PER \u627f\u63a5\u4e0a\u4e00\u4e2a NE\uff0c\u82e5\u8fde\u7eed\u51fa\u73b0\u4e24\u4e2aB-PER\uff0c\u5219\u4e0a\u4e00\u4e2a B \u7ed3\u675f</p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#sequence-model","title":"Sequence model","text":"<p>\u5e38\u89c1\u7684\u5e8f\u5217\u6a21\u578b\u6709\uff1a</p> <ul> <li>\u6709\u5411\u56fe\u6a21\u578b\uff1a\u5982 HMM\uff0c\u751f\u6210\u5f0f\u6a21\u578b\uff0c\u5047\u8bbe\u7279\u5f81\u4e4b\u95f4\u76f8\u4e92\u72ec\u7acb\uff0c\u627e\u5230\u4f7f\u5f97 P(X|Y) \u6700\u5927\u7684\u53c2\u6570</li> <li>\u65e0\u5411\u56fe\u6a21\u578b\uff1a\u5982 CRF\uff0c\u5224\u522b\u5f0f\u6a21\u578b\uff0c\u6ca1\u6709\u7279\u5f81\u72ec\u7acb\u7684\u5047\u8bbe\uff0c\u627e\u5230\u4f7f\u5f97 P(X|YP(X|Y) \u6700\u5927\u7684\u53c2\u6570</li> </ul>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#deep-learning","title":"Deep Learning","text":"<p>LSTM + CRF</p> <p>\u7aef\u5230\u7aef\u7684\u5224\u522b\u5f0f\u6a21\u578b\uff0cLSTM \u5229\u7528\u8fc7\u53bb\u7684\u8f93\u5165\u7279\u5f81\uff0cCRF \u5229\u7528\u53e5\u5b50\u7ea7\u7684\u6807\u6ce8\u4fe1\u606f\uff0c\u6709\u6548\u5229\u7528\u8fc7\u53bb\u4e0e\u672a\u6765\u7684\u6807\u6ce8\u9884\u6d4b\u5f53\u524d\u6807\u6ce8\u3002</p> <p>LSTM + CRF \u793a\u610f\u56fe</p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u5173\u7cfb\u62bd\u53d6","title":"\u5173\u7cfb\u62bd\u53d6","text":"<p>\u5173\u7cfb\u62bd\u53d6 \u9700\u8981\u4ece\u6587\u672c\u4e2d\u62bd\u53d6\u4e24\u4e2a/\u591a\u4e2a\u5b9e\u4f53\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u4e3b\u8981\u65b9\u6cd5\u6709\uff1a</p> <ul> <li>\u57fa\u4e8e\u6a21\u677f\u7684\u65b9\u6cd5 (hand-written patterns)<ul> <li>\u57fa\u4e8e\u89e6\u53d1\u8bcd/\u5b57\u7b26\u4e32</li> <li>\u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5</li> </ul> </li> <li>\u76d1\u7763\u5b66\u4e60 (supervised machine learning)<ul> <li>Machine Learning</li> <li>Deep Learning</li> </ul> </li> <li>\u534a\u76d1\u7763/\u65e0\u76d1\u7763\u5b66\u4e60 (semi-supervised / unsupervised learning)<ul> <li>Bootstrapping</li> <li>Distant supervision</li> <li>Unsupervised learning from the web  </li> </ul> </li> </ul>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u57fa\u4e8e\u6a21\u677f\u7684\u65b9\u6cd5","title":"\u57fa\u4e8e\u6a21\u677f\u7684\u65b9\u6cd5","text":""},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u57fa\u4e8e\u89e6\u53d1\u8bcd\u5b57\u7b26\u4e32","title":"\u57fa\u4e8e\u89e6\u53d1\u8bcd/\u5b57\u7b26\u4e32","text":"<p>\u9996\u5148\u662f \u57fa\u4e8e\u5b57\u7b26\u4e32\u7684 pattern\uff0c\u4f8b\u5982 IS-A \u5173\u7cfb</p> <p>Agar is a substance prepared from a mixture of red algae, such as Gelidium, for laboratory or industrial use</p> <p>\u57fa\u4e8e such as\uff0c\u53ef\u4ee5\u5224\u65ad\u5176\u4e3a IS-A \u5173\u7cfb\uff0c\u7531\u6b64\u53ef\u4ee5\u5199\u51fa\u7684\u89c4\u5219\u4e3a\uff1a</p> <pre><code>\u201cY such as X ((, X)* (, and|or) X)\u201d\n\u201csuch Y as X\u201d\n\u201cX or other Y\u201d\n\u201cX and other Y\u201d\n\u201cY including X\u201d\n\u201cY, especially X\u201d\n</code></pre> <p>\u5176\u6b21\u5bf9\u4e8e\u7279\u5b9a\u5b9e\u4f53\uff0c\u6709 \u57fa\u4e8e NER \u7684 pattern\uff0c\u4f7f\u7528 NER \u6807\u7b7e\u5e2e\u52a9\u8fdb\u884c\u5173\u7cfb\u62bd\u53d6</p> <pre><code>\u2022  located-in (ORGANIZATION, LOCATION)\n\u2022  founded (PERSON, ORGANIZATION)\n\u2022  cures (DRUG, DISEASE)\n</code></pre> <p>\u5f53\u7136\uff0c\u4e5f\u53ef\u4ee5\u7ed3\u5408\u4e24\u79cd pattern\uff0c\u5bf9\u5e94\u7684\u5de5\u5177\u6709 Stanford CoreNLP \u7684 tokensRegex</p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5","title":"\u57fa\u4e8e\u4f9d\u5b58\u53e5\u6cd5","text":"<p>\u901a\u5e38\u53ef\u4ee5\u4ee5\u52a8\u8bcd\u4e3a\u8d77\u70b9\u6784\u5efa\u89c4\u5219\uff0c\u5bf9\u8282\u70b9\u4e0a\u7684\u8bcd\u6027\u548c\u5176\u8fb9\u4e0a\u7684\u4f9d\u5b58\u5173\u7cfb\u8fdb\u884c\u9650\u5b9a\uff0c\u6d41\u7a0b\u4e3a\uff1a</p> <ol> <li>\u5bf9\u53e5\u5b50\u8fdb\u884c\u5206\u8bcd\u3001\u8bcd\u6027\u6807\u6ce8\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u4f9d\u5b58\u5206\u6790\u7b49\u5904\u7406</li> <li>\u6839\u636e\u53e5\u5b50\u4f9d\u5b58\u8bed\u6cd5\u6811\u7ed3\u6784\u4e0a\u5339\u914d\u89c4\u5219\uff0c\u6bcf\u5339\u914d\u4e00\u6761\u89c4\u5219\u5c31\u751f\u6210\u4e00\u4e2a\u4e09\u5143\u7ec4</li> <li>\u6839\u636e\u62d3\u5c55\u89c4\u5219\u5bf9\u62bd\u53d6\u5230\u7684\u4e09\u5143\u7ec4\u8fdb\u884c\u62d3\u5c55</li> <li>\u5bf9\u4e09\u5143\u7ec4\u5b9e\u4f53\u548c\u89e6\u53d1\u8bcd\u8fdb\u4e00\u6b65\u62bd\u53d6\u51fa\u5173\u7cfb</li> </ol>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u76d1\u7763\u5b66\u4e60","title":"\u76d1\u7763\u5b66\u4e60","text":""},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#machine-learning","title":"Machine Learning","text":"<p>Feature</p> <p>\u5bf9\u4e8e\u6587\u672c\uff1a</p> <p>American Airlines, a unit of AMR, immediately matched the move, spokesman\u00a0Tim Wagner\u00a0said</p> <p>Mention 1 (M1)\uff1aAmerican Airlines</p> <p>Mention 2 (M2)\uff1aTim Wagner</p> <p>\u5e38\u7528\u7684 Feature \u6709\uff1a</p> <p>Word Features</p> <ul> <li>M1 \u4e0e M2 \u7684\u8bcd\u6761 (headwords) \u53ca\u5176\u7ec4\u5408 (combination)<ul> <li>\u4f8b\uff0cM1\uff1aAirlines\uff0cM2\uff1aWagner\uff0cCombination\uff1aAirlines-Wagner</li> </ul> </li> <li>M1 \u4e0e M2 \u4e2d\u7684\u8bcd\u888b (Bag of words) \u4e0e\u4e8c\u5143\u7ec4 (bigrams)<ul> <li>{American, Airlines, Tim, Wagner, American Airlines, Tim Wagner}</li> </ul> </li> <li>M1/M2 \u5de6\u53f3\u7279\u5b9a\u4f4d\u7f6e\u7684\u5355\u8bcd\u6216\u4e8c\u5143\u7ec4<ul> <li>M2: -1 spokesman</li> <li>M2: +1 said</li> </ul> </li> <li>\u4e24\u4e2a\u5b9e\u4f53\u4e4b\u95f4\u7684\u8bcd\u888b\u6216\u4e8c\u5143\u7ec4<ul> <li>{a, AMR, of, immediately, matched, move, spokesman, the, unit}</li> </ul> </li> </ul> <p>Named Entities Type and Mention Level Features</p> <ul> <li>\u547d\u540d\u5b9e\u4f53 (NE) \u7c7b\u578b<ul> <li>M1: ORG  </li> <li>M2: PERSON</li> </ul> </li> <li>2 \u547d\u540d\u5b9e\u4f53\u7684\u5173\u8054 (Concatenation)<ul> <li>ORG-PERSON</li> </ul> </li> <li>M1 \u4e0e M2 \u7684\u5b9e\u4f53\u7ea7\u522b (NAME, NOMINAL, PRONOUN)  <ul> <li>M1: NAME [it or he would be PRONOUN]  </li> <li>M2: NAME [the company would be NOMINAL]</li> </ul> </li> </ul> <p>Parse Features</p> <ul> <li>\u8bcd\u5230\u8bcd\u7684\u57fa\u672c\u53e5\u6cd5\u5757\u5e8f\u5217 (Base syntactic chunk sequence)  <ul> <li>NP NP PP VP NP NP</li> </ul> </li> <li>\u6811\u5230\u6811\u7684\u6784\u6210\u8def\u5f84 (Constituent path) <ul> <li>NP \u2191 NP \u2191 S \u2191 S \u2193 NP</li> </ul> </li> <li>\u4f9d\u8d56\u8def\u5f84 (Dependency path  )<ul> <li>Airlines matched Wagner said</li> </ul> </li> </ul> <p>Gazetteer and trigger word features</p> <ul> <li>\u5bb6\u5ead\u89e6\u53d1\u5217\u8868\uff1a\u4eb2\u5c5e\u5173\u7cfb\u672f\u8bed<ul> <li>parent, wife, husband, grandparent, etc.</li> </ul> </li> <li>\u5730\u540d\u8bcd\u5178<ul> <li>List of useful geo or geopolitical words  </li> <li>Country name list  </li> <li>Other sub-entities</li> </ul> </li> </ul> <p>Classification</p> <p>\u5206\u7c7b\u5668\u6d41\u7a0b\uff1a</p> <ul> <li>\u9884\u5148\u5b9a\u4e49\u597d\u60f3\u63d0\u53d6\u7684\u5173\u7cfb\u96c6\u5408</li> <li>\u9009\u62e9\u76f8\u5173\u7684\u547d\u540d\u5b9e\u4f53\u96c6\u5408</li> <li>\u5bfb\u627e\u5e76\u6807\u6ce8\u6570\u636e<ul> <li>\u9009\u62e9\u6709\u4ee3\u8868\u6027\u7684\u8bed\u6599\u5e93</li> <li>\u6807\u8bb0\u547d\u540d\u5b9e\u4f53</li> <li>\u4eba\u5de5\u6807\u6ce8\u5b9e\u4f53\u95f4\u7684\u5173\u7cfb</li> <li>\u5206\u6210\u8bad\u7ec3\u3001\u5f00\u53d1\u3001\u6d4b\u8bd5\u96c6</li> </ul> </li> <li>\u8bbe\u8ba1\u7279\u5f81</li> <li>\u9009\u62e9\u5e76\u8bad\u7ec3\u5206\u7c7b\u5668</li> </ul> <p>\u4e3a\u4e86\u63d0\u9ad8 efficiency\uff0c\u901a\u5e38\u4f1a\u8bad\u7ec3\u4e24\u4e2a\u5206\u7c7b\u5668\uff0c\u7b2c\u4e00\u4e2a\u5206\u7c7b\u5668\u662f yes/no \u7684\u4e8c\u5206\u7c7b\uff0c\u5224\u65ad\u547d\u540d\u5b9e\u4f53\u95f4\u662f\u5426\u6709\u5173\u7cfb\uff0c\u5982\u679c\u6709\u5173\u7cfb\uff0c\u518d\u9001\u5230\u7b2c\u4e8c\u4e2a\u5206\u7c7b\u5668\uff0c\u7ed9\u5b9e\u4f53\u5206\u914d\u5173\u7cfb\u7c7b\u522b\u3002\u8fd9\u6837\u505a\u7684\u597d\u5904\u662f\u901a\u8fc7\u6392\u9664\u5927\u591a\u6570\u7684\u5b9e\u4f53\u5bf9\u6765\u52a0\u5feb\u5206\u7c7b\u5668\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u53e6\u4e00\u65b9\u9762\uff0c\u5bf9\u6bcf\u4e2a\u4efb\u52a1\u53ef\u4ee5\u4f7f\u7528 task-specific feature-set</p> <p>\u53ef\u4ee5\u91c7\u7528\u7684\u5206\u7c7b\u5668\u53ef\u4ee5\u662f\u00a0MaxEnt\u3001Naive Bayes\u3001SVM\u00a0\u7b49\u3002</p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#deep-learning_1","title":"Deep Learning","text":"<p>DL \u65b9\u6cd5\u5206\u4e3a\u4e24\u7c7b\uff1aPipeline \u4e0e Joint Model</p> <p>\u5e38\u7528\u7279\u5f81\u6709\uff1a</p> <ul> <li>Position embeddings</li> <li>Word embeddings</li> <li>Knlowledge embeddings</li> </ul> <p>\u6a21\u578b\u901a\u5e38\u6709 CNN / RNN + attention\uff0c\u635f\u5931\u51fd\u6570\u4e00\u822c ranking loss \u8981\u4f18\u4e8e\u4ea4\u53c9\u71b5</p> <p>Pipeline</p> <p>\u5c06\u5b9e\u4f53\u8bc6\u522b\u4e0e\u5173\u7cfb\u5206\u7c7b\u4f5c\u4e3a\u4e24\u4e2a\u5b8c\u5168\u72ec\u7acb\u7684\u8fc7\u7a0b\uff0c\u4e0d\u4f1a\u76f8\u4e92\u5f71\u54cd\uff0c\u5173\u7cfb\u7684\u8bc6\u522b\u5224\u65ad\u4f9d\u8d56\u4e8e\u5b9e\u4f53\u8bc6\u522b\u7684\u6548\u679c</p> <p>\u5982\uff1aCR-CNN\u3001Att-CNN\u3001Att-BLSTM</p> <p>Joint Model</p> <p>\u5b9e\u4f53\u8bc6\u522b\u4e0e\u5173\u7cfb\u5206\u7c7b\u7684\u8fc7\u7a0b\u5171\u540c\u4f18\u5316</p> <p>\u5982\uff1aLSTM-RNNs</p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u534a\u76d1\u7763\u65e0\u76d1\u7763\u5b66\u4e60","title":"\u534a\u76d1\u7763/\u65e0\u76d1\u7763\u5b66\u4e60","text":""},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u534a\u76d1\u7763\u5b66\u4e60","title":"\u534a\u76d1\u7763\u5b66\u4e60","text":"<p>\u534a\u76d1\u7763\u5b66\u4e60\u4e3b\u8981\u662f\u5229\u7528\u5c11\u91cf\u7684\u6807\u6ce8\u4fe1\u606f\u8fdb\u884c\u5b66\u4e60\uff0c\u8be5\u65b9\u9762\u4e3b\u8981\u57fa\u4e8e Bootstrap (\u5f15\u5bfc) \u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca distant supervision (\u8fdc\u7a0b\u76d1\u7763) \u65b9\u6cd5\u3002\u57fa\u4e8e Bootstrap \u7684\u65b9\u6cd5\u4e3b\u8981\u662f\u5229\u7528\u5c11\u91cf\u5b9e\u4f8b\u4f5c\u4e3a\u521d\u59cb\u79cd\u5b50 (seed tuple) \u7684\u96c6\u5408\uff0c\u7136\u540e\u5229\u7528 pattern \u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u5b66\u4e60\uff0c\u901a\u8fc7\u4e0d\u65ad\u8fed\u4ee3\u4ece\u975e\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u62bd\u53d6\u5b9e\u4f8b\u3002\u7136\u540e\u4ece\u65b0\u5b66\u5230\u7684\u5b9e\u4f8b\u4e2d\u5b66\u4e60\u65b0\u7684 pattern \uff0c\u540c\u65f6\u6269\u5145 pattern \u96c6\u5408\uff0c\u5bfb\u627e\u4e0e\u53d1\u73b0\u65b0\u7684\u6f5c\u5728\u5173\u7cfb\u4e09\u5143\u7ec4\u3002\u8fdc\u7a0b\u76d1\u7763\u65b9\u6cd5\u4e3b\u8981\u662f\u5bf9\u77e5\u8bc6\u5e93\u4e0e\u975e\u7ed3\u6784\u5316\u6587\u672c\u5bf9\u9f50\u6765\u6784\u5efa\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u51cf\u5c11\u6a21\u578b\u5bf9\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u8de8\u9886\u57df\u9002\u5e94\u80fd\u529b</p> <p>Bootstrapping</p> <ol> <li>\u6536\u96c6\u4e00\u7ec4\u5177\u6709\u5173\u7cfb RR \u7684 seed pair</li> <li>\u8fed\u4ee3\uff1a<ul> <li>\u67e5\u627e\u5305\u542b\u8fd9\u4e9b pair \u7684 sentence</li> <li>\u67e5\u770b pair \u4e4b\u95f4/\u5468\u56f4\u7684\u4e0a\u4e0b\u6587\uff0c\u5e76\u5c06\u5176\u6982\u62ec\u4ee5\u521b\u5efa patterns</li> <li>\u4f7f\u7528 grep \u7684 patterns \u83b7\u53d6\u66f4\u591a pair</li> </ul> </li> </ol> <p>Distant supervision</p> <p>\u57fa\u672c\u5047\u8bbe\uff1a\u4e24\u4e2a\u5b9e\u4f53\u5982\u679c\u5728\u77e5\u8bc6\u5e93\u4e2d\u5b58\u5728\u67d0\u79cd\u5173\u7cfb\uff0c\u5219\u5305\u542b\u4e24\u4e2a\u5b9e\u4f53\u7684\u975e\u7ed3\u6784\u5316\u53e5\u5b50\u5747\u80fd\u8868\u793a\u51fa\u8be5\u79cd\u5173\u7cfb</p> <p>\u6b65\u9aa4\uff1a 1. \u4ece\u77e5\u8bc6\u5e93\u62bd\u53d6\u5b58\u5728\u5173\u7cfb\u7684\u5b9e\u4f53\u5bf9 2. \u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u62bd\u53d6\u542b\u6709\u5b9e\u4f53\u5bf9\u7684\u53e5\u5b50\u4f5c\u4e3a\u8bad\u7ec3\u6837\u4f8b\uff0c\u7136\u540e\u63d0\u53d6\u7279\u5f81\u8bad\u7ec3\u5206\u7c7b\u5668</p> <p>Distant Supervision \u7ed3\u5408\u4e86 Bootstrapping \u548c\u76d1\u7763\u5b66\u4e60\u7684\u957f\u5904\uff0c\u4f7f\u7528\u4e00\u4e2a\u5927\u7684 corpus \u6765\u5f97\u5230\u6d77\u91cf\u7684 seed example\uff0c\u7136\u540e\u4ece\u8fd9\u4e9b example \u4e2d\u521b\u5efa\u7279\u5f81\uff0c\u6700\u540e\u4e0e\u6709\u76d1\u7763\u7684\u5206\u7c7b\u5668\u76f8\u7ed3\u5408\uff1b\u4e0e\u76d1\u7763\u5b66\u4e60\u76f8\u4f3c\u7684\u662f\u8fd9\u79cd\u65b9\u6cd5\u7528\u5927\u91cf\u7279\u5f81\u8bad\u7ec3\u4e86\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u5df2\u6709\u7684\u77e5\u8bc6\u8fdb\u884c\u76d1\u7763\uff0c\u4e0d\u9700\u8981\u7528\u8fed\u4ee3\u7684\u65b9\u6cd5\u6765\u6269\u5145 pattern\u3002  \uff1b\u4e0e\u65e0\u76d1\u7763\u5b66\u4e60\u76f8\u4f3c\u7684\u662f\u8fd9\u79cd\u65b9\u6cd5\u91c7\u7528\u4e86\u5927\u91cf\u6ca1\u6709\u6807\u6ce8\u7684\u6570\u636e\uff0c\u5bf9\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u7684 genre \u5e76\u4e0d\u654f\u611f\uff0c\u9002\u5408\u6cdb\u5316</p> <p>\u5982\uff1aPCNN + Attention</p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u65e0\u76d1\u7763\u5b66\u4e60","title":"\u65e0\u76d1\u7763\u5b66\u4e60","text":"<p>Unsupervised learning from the web </p> <p>\u6d41\u7a0b<pre><code>1. Use parsed data to train a \u201ctrustworthy tuple\u201d classifier\n2. Single-pass extract all relations between NPs, keep if trustworthy\n3. Assessor ranks relations based on text redundancy \n</code></pre> </p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u4e8b\u4ef6\u62bd\u53d6","title":"\u4e8b\u4ef6\u62bd\u53d6","text":"<p>\u4e0e\u5173\u7cfb\u62bd\u53d6\u76f8\u6bd4\uff0c\u4e8b\u4ef6\u62bd\u53d6\u540c\u6837\u9700\u8981\u4ece\u6587\u672c\u4e2d\u62bd\u53d6 predict \u4e0e\u5176\u5bf9\u5e94\u7684 arguments\uff0c\u4e0d\u540c\u7684\u662f\uff0c\u5173\u7cfb\u62bd\u53d6\u7684\u95ee\u9898\u662f binary \u7684\uff0c\u5176\u4e24\u4e2a arguments \u9700\u5728\u540c\u4e00\u53e5\u4e2d\u51fa\u73b0\uff1b\u800c\u4e8b\u4ef6\u62bd\u53d6\u5219\u9762\u5bf9\u4e86\u4e86\u591a\u4e2a arguments \u548c modifiers\uff0c\u4e14\u53ef\u80fd\u5206\u5e03\u5728\u591a\u4e2a\u53e5\u5b50\u4e2d\uff0c\u540c\u65f6\u90e8\u5206 arguments \u53ef\u80fd\u4e0d\u662f\u5fc5\u987b\u7684\uff0c\u8fd9\u4f7f\u5f97\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u51e0\u79cd\u65b9\u6cd5\u5e94\u5bf9\u8be5\u79cd\u60c5\u51b5\u65f6\u90fd\u6bd4\u8f83\u56f0\u96be</p> <p>\u4e8b\u4ef6\u62bd\u53d6\u7684\u4efb\u52a1\u5206\u4e3a\u4e24\u5927\u7c7b\uff1a</p> <ul> <li> <p>\u4e8b\u4ef6\u8bc6\u522b\u548c\u62bd\u53d6</p> <p>\u4ece\u63cf\u8ff0\u4e8b\u4ef6\u4fe1\u606f\u7684\u6587\u672c\u4e2d\u8bc6\u522b\u5e76\u62bd\u53d6\u51fa\u4e8b\u4ef6\u4fe1\u606f\u5e76\u4ee5\u7ed3\u6784\u5316\u7684\u5f62\u5f0f\u5448\u73b0\u51fa\u6765\uff0c\u5305\u62ec\u53d1\u751f\u7684\u65f6\u95f4\u3001\u5730\u70b9\u3001\u53c2\u4e0e\u89d2\u8272\u4ee5\u53ca\u4e0e\u4e4b\u76f8\u5173\u7684\u52a8\u4f5c\u6216\u8005\u72b6\u6001\u7684\u6539\u53d8</p> </li> <li> <p>\u4e8b\u4ef6\u68c0\u6d4b\u4e0e\u8ffd\u8e2a</p> <p>\u4e8b\u4ef6\u68c0\u6d4b\u4e0e\u8ffd\u8e2a\u65e8\u5728\u5c06\u6587\u672c\u65b0\u95fb\u6d41\u6309\u7167\u5176\u62a5\u9053\u7684\u4e8b\u4ef6\u8fdb\u884c\u7ec4\u7ec7\uff0c\u4e3a\u4f20\u7edf\u5a92\u4f53\u591a\u79cd\u6765\u6e90\u7684\u65b0\u95fb\u76d1\u63a7\u63d0\u4f9b\u6838\u5fc3\u6280\u672f\uff0c\u4ee5\u4fbf\u8ba9\u7528\u6237\u4e86\u89e3\u65b0\u95fb\u53ca\u5176\u53d1\u5c55\u3002\u5177\u4f53\u800c\u8a00\uff0c\u4e8b\u4ef6\u53d1\u73b0\u4e0e\u8ddf\u8e2a\u5305\u62ec\u4e09\u4e2a\u4e3b\u8981\u4efb\u52a1\uff1a\u5206\u5272\uff0c\u53d1\u73b0\u548c\u8ddf\u8e2a\uff0c\u5c06\u65b0\u95fb\u6587\u672c\u5206\u89e3\u4e3a\u4e8b\u4ef6\uff0c \u53d1\u73b0\u65b0\u7684\uff08\u4e0d\u53ef\u9884\u89c1\u7684\uff09\u4e8b\u4ef6\uff0c\u5e76\u8ddf\u8e2a\u4ee5\u524d\u62a5\u9053\u4e8b\u4ef6\u7684\u53d1\u5c55;\u4e8b\u4ef6\u53d1\u73b0\u4efb\u52a1\u53c8\u53ef\u7ec6\u5206\u4e3a \u5386\u53f2\u4e8b\u4ef6\u53d1\u73b0 \u548c \u5728\u7ebf\u4e8b\u4ef6\u53d1\u73b0 \u4e24\u79cd\u5f62\u5f0f\uff0c\u524d\u8005\u76ee\u6807\u662f\u4ece\u6309\u65f6\u95f4\u6392\u5e8f\u7684\u65b0\u95fb\u6587\u6863\u4e2d\u53d1\u73b0\u4ee5\u524d\u6ca1\u6709\u8bc6\u522b\u7684\u4e8b\u4ef6\uff0c\u540e\u8005\u5219\u662f\u4ece\u5b9e\u65f6\u65b0\u95fb\u6d41\u4e2d\u5b9e\u65f6\u53d1\u73b0\u65b0\u7684\u4e8b\u4ef6</p> </li> </ul>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u76f8\u5173\u6982\u5ff5","title":"\u76f8\u5173\u6982\u5ff5","text":"<ul> <li>\u4e8b\u4ef6\u63cf\u8ff0 (Event Mention)     \u63cf\u8ff0\u4e8b\u4ef6\u7684\u8bcd\u7ec4/\u53e5\u5b50/\u53e5\u7fa4\uff0c\u5305\u542b\u4e00\u4e2a trigger \u4ee5\u53ca\u4efb\u610f\u6570\u91cf\u7684 arguments</li> <li>\u4e8b\u4ef6\u89e6\u53d1 (Event Trigger)     \u4e8b\u4ef6\u63cf\u8ff0\u4e2d\u6700\u80fd\u4ee3\u8868\u4e8b\u4ef6\u53d1\u751f\u7684\u8bcd\u6c47\uff0c\u51b3\u5b9a\u4e8b\u4ef6\u7c7b\u522b\u7684\u91cd\u8981\u7279\u5f81\uff0c\u4e00\u822c\u4e3a\u52a8\u8bcd\u6216\u540d\u8bcd</li> <li>\u4e8b\u4ef6\u5143\u7d20 (Event Argument)     \u4e8b\u4ef6\u7684\u91cd\u8981\u4fe1\u606f/\u5b9e\u4f53\u63cf\u8ff0 (entity mention)\uff0c\u4e3b\u8981\u7531\u5b9e\u4f53\u3001\u5c5e\u6027\u503c\u7b49\u8868\u8fbe\u5b8c\u6574\u8bed\u4e49\u7684\u7ec6\u7c92\u5ea6\u5355\u4f4d\u7ec4\u6210</li> <li>\u5143\u7d20\u89d2\u8272 (Argument Role)     \u4e8b\u4ef6\u5143\u7d20\u5728\u4e8b\u4ef6\u4e2d\u626e\u6f14\u7684\u89d2\u8272\uff0c\u4e8b\u4ef6\u5143\u7d20\u4e0e\u4e8b\u4ef6\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a slot</li> <li>\u4e8b\u4ef6\u7c7b\u578b (Event Type)</li> </ul>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u4e8b\u4ef6\u8bc6\u522b\u4e0e\u62bd\u53d6","title":"\u4e8b\u4ef6\u8bc6\u522b\u4e0e\u62bd\u53d6","text":"<p>\u7c7b\u4f3c\u4e8e\u4ece\u6587\u672c\u4e2d\u627e\u5230\u7279\u5b9a\u7c7b\u522b\u7684\u4e8b\u4ef6\uff0c\u7136\u540e\u8fdb\u884c\u586b\u8868</p> <p>Given a text document, an event extraction system should\u00a0predict event triggers\u00a0with specific\u00a0sub-types\u00a0and\u00a0their arguments for each sentence.</p> <p>\u4e8b\u4ef6\u62bd\u53d6\u4efb\u52a1\u7684\u57fa\u7840\u90e8\u5206\u5305\u62ec\uff1a</p> <ul> <li>\u8bc6\u522b\u4e8b\u4ef6\u89e6\u53d1\u8bcd\u53ca\u4e8b\u4ef6\u7c7b\u578b</li> <li>\u62bd\u53d6\u4e8b\u4ef6\u5143\u7d20\uff0c\u540c\u65f6\u5224\u65ad\u5176\u89d2\u8272</li> <li>\u62bd\u53d6\u63cf\u8ff0\u4e8b\u4ef6\u7684\u8bcd\u7ec4\u6216\u53e5\u5b50</li> </ul> <p>\u4e8b\u4ef6\u62bd\u53d6\u5927\u591a\u662f\u5206\u9636\u6bb5\u8fdb\u884c\uff0c\u901a\u5e38\u662f\u7531 trigger classifier \u5f00\u59cb\uff0c\u82e5\u6709 trigger\uff0c\u5c31\u5c06 trigger \u53ca\u5176\u4e0a\u4e0b\u6587\u4f5c\u4e3a\u7279\u5f81\u8fdb\u884c\u5206\u7c7b\u5224\u65ad\u4e8b\u4ef6\u7c7b\u578b\uff0c\u518d\u5224\u65ad\u4e0b\u4e00\u6b65\u7684 argument classifier\uff0c\u5bf9\u53e5\u5b50\u4e2d\u7684\u6bcf\u4e2a entity  mention \u8fdb\u884c\u5206\u7c7b\uff0c\u5224\u65ad\u662f\u5426\u662f argument\uff0c\u5982\u679c\u662f\uff0c\u5219\u5224\u5b9a\u5176\u89d2\u8272</p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u57fa\u4e8e\u6a21\u5f0f\u5339\u914d\u7684\u65b9\u6cd5","title":"\u57fa\u4e8e\u6a21\u5f0f\u5339\u914d\u7684\u65b9\u6cd5","text":"<p>\u6838\u5fc3\uff1a\u53e5\u6cd5 (syntactic)\u3001\u8bed\u4e49\u7ea6\u675f (semantic constraints)</p> <p>\u57fa\u4e8e\u4eba\u5de5\u7f16\u5199\u7684\u89c4\u5219/\u8bed\u6cd5\u6811/\u6b63\u5219\u8868\u8fbe\u5f0f</p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u57fa\u4e8e\u4eba\u5de5\u6807\u6ce8\u8bed\u6599","title":"\u57fa\u4e8e\u4eba\u5de5\u6807\u6ce8\u8bed\u6599","text":"<ul> <li>AutoSlog</li> </ul> <p>\u57fa\u672c\u5047\u8bbe<pre><code>1. \u4e8b\u4ef6\u5143\u7d20\u9996\u6b21\u63d0\u53ca\u4e4b\u5904\u5373\u53ef\u786e\u5b9a\u8be5\u5143\u7d20\u4e0e\u4e8b\u4ef6\u95f4\u7684\u5173\u7cfb  \n2. \u4e8b\u4ef6\u5143\u7d20\u5468\u56f4\u7684\u8bed\u53e5\u4e2d\u5305\u542b\u4e86\u4e8b\u4ef6\u5143\u7d20\u5728\u4e8b\u4ef6\u4e2d\u7684\u89d2\u8272\u63cf\u8ff0  \n</code></pre> </p> <p>\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u548c\u4eba\u5de5\u5ba1\u67e5\u6765\u5efa\u7acb\u62bd\u53d6\u89c4\u5219\u3002\u901a\u8fc7\u8bad\u7ec3\u6570\u636e\u4e2d\u5df2\u7ecf\u586b\u5145\u597d\u7684\u69fd\uff08filled slot\uff09\uff0cAutoSlog \u89e3\u6790 slot \u9644\u8fd1\u7684\u53e5\u6cd5\u7ed3\u6784\uff0c\u6765\u81ea\u52a8\u5f62\u6210\u62bd\u53d6\u89c4\u5219\uff0c\u7531\u4e8e\u8fd9\u4e2a\u8fc7\u7a0b\u4ea7\u751f\u7684\u6a21\u677f too-general\uff0c\u6240\u4ee5\u9700\u8981\u4eba\u5de5\u6765\u5ba1\u6838\u3002\u672c\u8d28\u4e0a\u5f62\u6210\u7684\u662f\u4e00\u4e2a\u5b57\u5178</p> <ul> <li>PALKA</li> </ul> <p>\u57fa\u672c\u5047\u8bbe<pre><code>\u7279\u5b9a\u9886\u57df\u4e2d\u9ad8\u9891\u51fa\u73b0\u7684\u8bed\u8a00\u8868\u8fbe\u65b9\u5f0f\u662f\u53ef\u6570\u7684 \n</code></pre> </p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u57fa\u4e8e\u5f31\u76d1\u7763","title":"\u57fa\u4e8e\u5f31\u76d1\u7763","text":"<p>\u4eba\u5de5\u6807\u6ce8\u8017\u65f6\u8017\u529b\uff0c\u4e14\u5b58\u5728\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u800c\u5f31\u76d1\u7763\u65b9\u6cd5\u5219\u65e0\u9700\u5bf9\u8bed\u6599\u8fdb\u884c\u5b8c\u5168\u6807\u6ce8\uff0c\u53ea\u9700\u4eba\u5de5\u5bf9\u8bed\u6599\u8fdb\u884c\u4e00\u5b9a\u7684\u9884\u5206\u7c7b\u6216\u8005\u5236\u5b9a\u79cd\u5b50\u6a21\u677f\uff0c\u7531\u673a\u5668\u6839\u636e\u9884\u5206\u7c7b\u8bed\u6599\u6216\u79cd\u5b50\u6a21\u677f\u81ea\u52a8\u8fdb\u884c\u6a21\u5f0f\u5b66\u4e60</p> <ul> <li>AutoSlog-TS</li> <li>TIMES</li> <li>NEXUS</li> <li>GenPAM</li> </ul> <p>\u57fa\u4e8e\u6a21\u5f0f\u5339\u914d\u7684\u65b9\u6cd5\u5728\u7279\u5b9a\u9886\u57df\u4e2d\u6027\u80fd\u8f83\u597d\uff0c\u77e5\u8bc6\u8868\u793a\u7b80\u6d01\uff0c\u4fbf\u4e8e\u7406\u89e3\u548c\u540e\u7eed\u5e94\u7528\uff0c\u4f46\u5bf9\u4e8e\u8bed\u8a00\u3001\u9886\u57df\u548c\u6587\u6863\u5f62\u5f0f\u90fd\u6709\u4e0d\u540c\u7a0b\u5ea6\u7684\u4f9d\u8d56\uff0c\u8986\u76d6\u5ea6\u548c\u53ef\u79fb\u690d\u6027\u8f83\u5dee</p> <p>\u6a21\u5f0f\u5339\u914d\u7684\u65b9\u6cd5\u4e2d\uff0c\u6a21\u677f\u51c6\u786e\u6027\u662f\u5f71\u54cd\u6574\u4e2a\u65b9\u6cd5\u6027\u80fd\u7684\u91cd\u8981\u56e0\u7d20\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6a21\u5f0f\u5339\u914d\u65b9\u6cd5\u5e94\u7528\u975e\u5e38\u5e7f\u6cdb\uff0c\u4e3b\u8981\u7279\u70b9\u662f\u9ad8\u51c6\u786e\u7387\u4f4e\u53ec\u56de\u7387\uff0c\u8981\u63d0\u9ad8\u53ec\u56de\u7387\uff0c\u4e00\u662f\u8981\u5efa\u7acb\u66f4\u5b8c\u6574\u7684\u6a21\u677f\u5e93\uff0c\u4e8c\u662f\u53ef\u4ee5\u7528\u534a\u76d1\u7763\u7684\u65b9\u6cd5\u6765\u5efa trigger \u5b57\u5178</p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u57fa\u4e8e\u7edf\u8ba1","title":"\u57fa\u4e8e\u7edf\u8ba1","text":"<p>\u5efa\u7acb\u5728\u7edf\u8ba1\u6a21\u578b\u57fa\u7840\u4e0a\uff0c\u4e8b\u4ef6\u62bd\u53d6\u65b9\u6cd5\u53ef\u4ee5\u5206\u4e3a pipeline \u548c joint model \u4e24\u5927\u7c7b</p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u57fa\u4e8e\u7edf\u8ba1---machine-learning","title":"\u57fa\u4e8e\u7edf\u8ba1 - Machine Learning","text":"<p>Pipeline</p> <p>\u5c06\u4e8b\u4ef6\u62bd\u53d6\u4efb\u52a1\u8f6c\u5316\u4e3a\u591a\u9636\u6bb5\u7684\u5206\u7c7b\u95ee\u9898\uff08\u7ba1\u9053\u62bd\u53d6\uff09\uff0c\u9700\u8981\u987a\u5e8f\u6267\u884c\u4e0b\u9762\u7684\u5206\u7c7b\u5668\uff1a</p> <ol> <li>\u4e8b\u4ef6\u89e6\u53d1\u8bcd\u5206\u7c7b\u5668\uff08Trigger Classifier\uff09     \u5224\u65ad\u8bcd\u6c47\u662f\u5426\u662f\u4e8b\u4ef6\u89e6\u53d1\u8bcd\uff0c\u4ee5\u53ca\u4e8b\u4ef6\u7c7b\u522b</li> <li>\u5143\u7d20\u5206\u7c7b\u5668\uff08Argument Classifier\uff09     \u8bcd\u7ec4\u662f\u5426\u662f\u4e8b\u4ef6\u5143\u7d20</li> <li>\u5143\u7d20\u89d2\u8272\u5206\u7c7b\u5668\uff08Role Classifier\uff09     \u5224\u5b9a\u5143\u7d20\u7684\u89d2\u8272\u7c7b\u522b</li> <li>\u5c5e\u6027\u5206\u7c7b\u5668\uff08Attribute Classifier\uff09     \u5224\u5b9a\u4e8b\u4ef6\u5c5e\u6027</li> <li>\u53ef\u62a5\u544a\u6027\u5206\u7c7b\u5668\uff08Reportable-Event Classifier\uff09     \u5224\u5b9a\u662f\u5426\u5b58\u5728\u503c\u5f97\u62a5\u544a\u7684\u4e8b\u4ef6\u5b9e\u4f8b</li> </ol> <p>\u5206\u7c7b\u5668\u53ef\u4ee5\u7528 MaxEnt, SVM\u3002\u91cd\u70b9\u8fd8\u662f\u5728\u4e8e\u63d0\u53d6\u548c\u96c6\u6210\u6709\u533a\u5206\u6027\u7684\u7279\u5f81\uff0c\u5305\u62ec\u00a0\u53e5\u5b50\u7ea7\u4fe1\u606f\u00a0\u548c\u00a0\u7bc7\u7ae0\u7ea7\u4fe1\u606f</p> <p>\u53e5\u5b50\u7ea7\u4fe1\u606f\uff1a\u4e0e\u5019\u9009\u8bcd\u76f8\u5173\u7684\u8bcd\u6cd5\u7279\u5f81\u3001\u4e0a\u4e0b\u6587\u7279\u5f81\u3001\u5b9e\u4f53\u7279\u5f81\u3001\u53e5\u6cd5\u7279\u5f81\u3001\u8bed\u8a00\u5b66\u7279\u5f81\u7b49</p> <p>\u7bc7\u7ae0\u7ea7\u4fe1\u606f\uff1a\u8de8\u6587\u6863\u5229\u7528\u5168\u5c40\u4fe1\u606f\u3002\u5bf9\u4e8e\u4e00\u4e2a\u53e5\u5b50\u7ea7\u7684\u62bd\u53d6\u7ed3\u679c\u4e0d\u4ec5\u8981\u8003\u8651\u5f53\u524d\u7684\u7f6e\u4fe1\u5ea6\uff0c\u8fd8\u8981\u8003\u8651\u4e0e\u5f85\u62bd\u53d6\u6587\u672c\u76f8\u5173\u7684\u6587\u672c\u5bf9\u5b83\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5168\u5c40\u4fe1\u606f\u5982\u4e8b\u4ef6\u4e0e\u8bdd\u9898\u7684\u5173\u7cfb\uff0c\u4e8b\u4ef6\u4e0e\u4e8b\u4ef6\u7684\u5171\u73b0\u4fe1\u606f\u7b49</p> <p>Joint Model</p> <p>\u5206\u4e3a Joint Inference \u548c Joint Modeling \u4e24\u79cd</p> <p>Joint Inference\u00a0\u4f7f\u7528\u96c6\u6210\u5b66\u4e60\u7684\u601d\u8def\uff0c\u5c06\u5404\u6a21\u578b\u901a\u8fc7\u6574\u4f53\u4f18\u5316\u76ee\u6807\u6574\u5408\u8d77\u6765\uff0c\u53ef\u4ee5\u901a\u8fc7\u6574\u6570\u89c4\u5212\u7b49\u65b9\u6cd5\u8fdb\u884c\u4f18\u5316</p> <p>Joint Modeling (Structured)\u00a0\u53c8\u53ef\u4ee5\u79f0\u4e3a\u57fa\u4e8e\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u5c06\u4e8b\u4ef6\u7ed3\u6784\u770b\u4f5c\u4f9d\u5b58\u6811\uff0c\u62bd\u53d6\u4efb\u52a1\u76f8\u5e94\u8f6c\u5316\u4e3a\u4f9d\u5b58\u6811\u7ed3\u6784\u9884\u6d4b\u95ee\u9898\uff0c\u89e6\u53d1\u8bcd\u8bc6\u522b\u548c\u5143\u7d20\u62bd\u53d6\u53ef\u4ee5\u540c\u65f6\u5b8c\u6210\uff0c\u5171\u4eab\u9690\u5c42\u7279\u5f81\uff0c\u4f7f\u7528\u641c\u7d22\u8fdb\u884c\u6c42\u89e3\uff0c\u907f\u514d\u4e86\u8bef\u5dee\u4f20\u64ad\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u53e6\u5916\uff0c\u5168\u5c40\u7279\u5f81\u4e5f\u53ef\u4ee5\u4ece\u6574\u4f53\u7684\u7ed3\u6784\u4e2d\u5b66\u4e60\u5f97\u5230\uff0c\u4ece\u800c\u4f7f\u7528\u5168\u5c40\u7684\u4fe1\u606f\u6765\u63d0\u5347\u5c40\u90e8\u7684\u9884\u6d4b</p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u57fa\u4e8e\u7edf\u8ba1---deep-learning","title":"\u57fa\u4e8e\u7edf\u8ba1 - Deep Learning","text":"<p>\u4e0a\u9762\u7684\u65b9\u6cd5\u5728\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\u4e2d\u8fd8\u662f\u4f1a\u4f9d\u8d56\u4f9d\u5b58\u5206\u6790\u3001\u53e5\u6cd5\u5206\u6790\u3001\u8bcd\u6027\u6807\u6ce8\u7b49\u4f20\u7edf\u7684\u5916\u90e8 NLP \u5de5\u5177\uff0c\u8fd8\u662f\u4f1a\u9020\u6210\u8bef\u5dee\u79ef\u7d2f\uff0c\u53e6\u5916\u6709\u4e9b\u8bed\u8a00\u548c\u9886\u57df\u5e76\u6ca1\u6709\u8fd9\u7c7b\u5904\u7406\u5de5\u5177\uff0c\u52a0\u4e4b\u7279\u5f81\u4e5f\u9700\u8981\u4eba\u5de5\u8bbe\u5b9a\uff0c2015 \u5e74\u8d77\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e8b\u4ef6\u62bd\u53d6\u65b9\u6cd5\u9010\u6e10\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\uff0c\u76f8\u6bd4\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f18\u52bf\u660e\u663e\uff1a</p> <ol> <li>\u51cf\u5c11\u5bf9\u5916\u90e8 NLP \u5de5\u5177\u7684\u4f9d\u8d56 \uff0c \u751a\u81f3\u4e0d\u4f9d\u8d56 NLP \u5de5\u5177 \uff0c \u5efa\u7acb\u6210\u7aef\u5bf9\u7aef\u7684\u7cfb\u7edf</li> <li>\u4f7f\u7528\u8bcd\u5411\u91cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u8574\u542b\u66f4\u4e3a\u4e30\u5bcc\u7684\u8bed\u8a00\u7279\u5f81</li> <li>\u81ea\u52a8\u63d0\u53d6\u53e5\u5b50\u7279\u5f81\uff0c \u907f\u514d\u4e86\u4eba\u5de5\u7279\u5f81\u8bbe\u8ba1\u7684\u7e41\u7410\u5de5\u4f5c</li> </ol> <p>Pipeline</p> <p>DMCNN</p> <p>Joint Model</p> <p>JRNN</p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u5f31\u76d1\u7763--\u9884\u6599\u62d3\u5c55","title":"\u5f31\u76d1\u7763 / \u9884\u6599\u62d3\u5c55","text":"<p>\u6709\u76d1\u7763\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u6807\u6ce8\u6837\u672c\uff0c\u4eba\u5de5\u6807\u6ce8\u8017\u65f6\u8017\u529b\uff0c\u8fd8\u5b58\u5728\u4e00\u81f4\u6027\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u5f31\u76d1\u7763\u65b9\u6cd5\u4e5f\u662f\u4e8b\u4ef6\u62bd\u53d6\u7684\u4e00\u4e2a\u91cd\u8981\u5206\u652f</p> <p>\u5f31\u76d1\u7763/\u8bad\u7ec3\u6570\u636e\u751f\u6210\u65b9\u9762\u6bd4\u8f83\u6d41\u884c\u7684\u65b9\u5411\u6709\u00a0\u5229\u7528\u5916\u90e8\u8d44\u6e90\uff0c\u901a\u8fc7\u8fdc\u7a0b\u76d1\u7763\uff0c\u4ee5\u53ca\u8de8\u8bed\u6599\u8fc1\u79fb\u7684\u65b9\u6cd5</p>"},{"location":"AI/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/#\u4e8b\u4ef6\u68c0\u6d4b\u4e0e\u8ffd\u8e2a","title":"\u4e8b\u4ef6\u68c0\u6d4b\u4e0e\u8ffd\u8e2a","text":"<p>\u4e3b\u6d41\u65b9\u6cd5\u5305\u62ec\u57fa\u4e8e\u76f8\u4f3c\u5ea6\u805a\u7c7b\u548c\u57fa\u4e8e\u6982\u7387\u7edf\u8ba1\u4e24\u7c7b</p>"},{"location":"AI/NLP/%E7%9F%A5%E8%AF%86%E6%8E%A8%E7%90%86/","title":"\u77e5\u8bc6\u63a8\u7406","text":"<p>\u5e38\u7528\u7684\u77e5\u8bc6\u63a8\u7406\u65b9\u6cd5\u6536\u96c6\u5982\u4e0b\uff1a</p>"},{"location":"AI/NLP/%E7%9F%A5%E8%AF%86%E6%8E%A8%E7%90%86/#\u57fa\u4e8e\u89c4\u5219\u7684\u63a8\u7406rule-based-reasoning","title":"\u57fa\u4e8e\u89c4\u5219\u7684\u63a8\u7406\uff08Rule-based Reasoning\uff09","text":"<p>\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e8b\u5148\u5b9a\u4e49\u4e00\u7ec4\u89c4\u5219\uff0c\u7136\u540e\u5e94\u7528\u8fd9\u4e9b\u89c4\u5219\u6765\u63a8\u65ad\u65b0\u7684\u4e8b\u5b9e\u3002\u4f8b\u5982\uff0c\"\u5982\u679cA\u548cB\uff0c\u5219C\"\u7684\u89c4\u5219\u53ef\u4ee5\u7528\u6765\u63a8\u65ad\u5f53A\u548cB\u540c\u65f6\u51fa\u73b0\u65f6\uff0cC\u4e5f\u53ef\u80fd\u51fa\u73b0\u3002\u8fd9\u79cd\u65b9\u6cd5\u9700\u8981\u5bf9\u89c4\u5219\u8fdb\u884c\u4e8b\u5148\u5b9a\u4e49\uff0c\u56e0\u6b64\u901a\u5e38\u9700\u8981\u4e13\u5bb6\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u89c4\u5219\u7684\u7ec4\u5408\u4e5f\u5bb9\u6613\u5bfc\u81f4\u89c4\u5219\u4e4b\u95f4\u7684\u77db\u76fe\u6216\u51b2\u7a81\u3002</p>"},{"location":"AI/NLP/%E7%9F%A5%E8%AF%86%E6%8E%A8%E7%90%86/#\u57fa\u4e8e\u672c\u4f53\u7684\u63a8\u7406ontology-based-reasoning","title":"\u57fa\u4e8e\u672c\u4f53\u7684\u63a8\u7406\uff08Ontology-based Reasoning\uff09","text":"<p>\u8be5\u65b9\u6cd5\u4f7f\u7528\u672c\u4f53\u6765\u63cf\u8ff0\u5b9e\u4f53\u3001\u5c5e\u6027\u548c\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u672c\u4f53\u7684\u63a8\u7406\u80fd\u529b\u6765\u63a8\u65ad\u65b0\u7684\u4e8b\u5b9e\u3002\u672c\u4f53\u53ef\u4ee5\u5b9a\u4e49\u6982\u5ff5\u5c42\u6b21\u7ed3\u6784\u3001\u5c5e\u6027\u548c\u5173\u7cfb\u7684\u9650\u5236\u6761\u4ef6\uff0c\u4ece\u800c\u5b9e\u73b0\u63a8\u7406\u3002\u4f8b\u5982\uff0c\u5982\u679c\u5b9a\u4e49\u4e00\u4e2a\"\u4eba\"\u7c7b\u548c\u4e00\u4e2a\"\u5de5\u7a0b\u5e08\"\u5b50\u7c7b\uff0c\u5e76\u4e14\u5b9a\u4e49\"\u5de5\u7a0b\u5e08\"\u5fc5\u987b\u62e5\u6709\u4e00\u4e2a\"\u5de5\u7a0b\u5b66\u4f4d\"\u5c5e\u6027\uff0c\u90a3\u4e48\u5f53\u4e00\u4e2a\u4eba\u88ab\u6807\u8bb0\u4e3a\"\u5de5\u7a0b\u5e08\"\u65f6\uff0c\u5b83\u5c31\u81ea\u52a8\u5177\u6709\"\u5de5\u7a0b\u5b66\u4f4d\"\u5c5e\u6027\u3002</p>"},{"location":"AI/NLP/%E7%9F%A5%E8%AF%86%E6%8E%A8%E7%90%86/#\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7684\u63a8\u7406semantic-similarity-based-reasoning","title":"\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7684\u63a8\u7406\uff08Semantic Similarity-based Reasoning\uff09","text":"<p>\u8be5\u65b9\u6cd5\u4f7f\u7528\u8bcd\u5411\u91cf\u6a21\u578b\u6765\u8ba1\u7b97\u8bcd\u8bed\u6216\u53e5\u5b50\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff0c\u4ece\u800c\u63a8\u65ad\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u4f8b\u5982\uff0c\u5982\u679c\u6709\u4e00\u4e2a\u95ee\u9898\u662f\"\u8c01\u662f\u4f0a\u4e3d\u838e\u767d\u5973\u738b\u7684\u4e08\u592b\"\uff0c\u53ef\u4ee5\u8ba1\u7b97\"\u4f0a\u4e3d\u838e\u767d\u5973\u738b\"\u548c\"\u4e08\u592b\"\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\uff0c\u5e76\u627e\u5230\u6700\u76f8\u5173\u7684\u7b54\u6848\u3002</p>"},{"location":"AI/NLP/%E7%9F%A5%E8%AF%86%E6%8E%A8%E7%90%86/#\u57fa\u4e8e\u6982\u7387\u63a8\u7406\u7684\u63a8\u7406probabilistic-reasoning","title":"\u57fa\u4e8e\u6982\u7387\u63a8\u7406\u7684\u63a8\u7406\uff08Probabilistic Reasoning\uff09","text":"<p>\u65b9\u6cd5\u4f7f\u7528\u6982\u7387\u6a21\u578b\u6765\u8ba1\u7b97\u4e0d\u540c\u5047\u8bbe\u7684\u6982\u7387\uff0c\u5e76\u6839\u636e\u8fd9\u4e9b\u6982\u7387\u6765\u63a8\u65ad\u65b0\u7684\u4e8b\u5b9e\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u4f7f\u7528\u8d1d\u53f6\u65af\u7f51\u7edc\u6765\u8868\u793a\u53d8\u91cf\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u4f7f\u7528\u6982\u7387\u63a8\u65ad\u6765\u8ba1\u7b97\u6982\u7387\u5206\u5e03\u3002</p>"},{"location":"AI/NLP/%E7%9F%A5%E8%AF%86%E6%8E%A8%E7%90%86/#ir-base-\u548c-sp-base","title":"IR-base \u548c SP-base","text":"<p>IR-base\uff08Information Retrieval-based Reasoning\uff09\u662f\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u68c0\u7d22\u7684\u63a8\u7406\u65b9\u6cd5\u3002\u5b83\u901a\u8fc7\u5bf9\u77e5\u8bc6\u5e93\u4e2d\u7684\u4fe1\u606f\u8fdb\u884c\u67e5\u8be2\u548c\u5339\u914d\uff0c\u6765\u53d1\u73b0\u65b0\u7684\u5173\u8054\u548c\u63a8\u7406\u7ed3\u8bba\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u4f7f\u7528\u57fa\u4e8e\u5173\u952e\u5b57\u7684\u68c0\u7d22\u65b9\u6cd5\u6765\u5bfb\u627e\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u5b9e\u4f53\u6216\u5c5e\u6027\uff0c\u5e76\u4f7f\u7528\u67e5\u8be2\u7ed3\u679c\u6765\u63a8\u65ad\u65b0\u7684\u4e8b\u5b9e\u3002</p> <p>SP-based\uff08Statistical Pattern-based Reasoning\uff09\u662f\u4e00\u79cd\u57fa\u4e8e\u7edf\u8ba1\u6a21\u578b\u7684\u63a8\u7406\u65b9\u6cd5\u3002\u5b83\u4f7f\u7528\u7edf\u8ba1\u6a21\u578b\u6765\u5b66\u4e60\u77e5\u8bc6\u5e93\u4e2d\u7684\u6a21\u5f0f\uff0c\u5e76\u4f7f\u7528\u8fd9\u4e9b\u6a21\u5f0f\u6765\u63a8\u65ad\u65b0\u7684\u4e8b\u5b9e\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u4f7f\u7528\u8d1d\u53f6\u65af\u7f51\u7edc\u6216\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\u7b49\u7edf\u8ba1\u6a21\u578b\u6765\u8868\u793a\u5b9e\u4f53\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u4f7f\u7528\u8fd9\u4e9b\u6a21\u578b\u6765\u63a8\u65ad\u65b0\u7684\u5173\u8054\u548c\u7ed3\u8bba\u3002</p> <p>\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u9009\u62e9\u54ea\u4e00\u79cd\u65b9\u6cd5\u53d6\u51b3\u4e8e\u5177\u4f53\u95ee\u9898\u7684\u6027\u8d28\u548c\u9700\u8981\u89e3\u51b3\u7684\u4efb\u52a1\u3002\u4f8b\u5982\uff0cIR-base\u65b9\u6cd5\u9002\u5408\u4e8e\u5b9e\u4f53\u8bc6\u522b\u548c\u5173\u7cfb\u62bd\u53d6\u7b49\u4efb\u52a1\uff0c\u800cSP-based\u65b9\u6cd5\u9002\u5408\u4e8e\u8bed\u4e49\u89d2\u8272\u6807\u6ce8\u548c\u4e8b\u4ef6\u8bc6\u522b\u7b49\u4efb\u52a1\u3002</p>"},{"location":"AI/NLP/%E7%9F%A5%E8%AF%86%E6%8E%A8%E7%90%86/#\u57fa\u4e8e-pytorch-\u5b9e\u73b0\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u548c\u63a8\u7406\u7684\u4ee3\u7801\u793a\u4f8b","title":"\u57fa\u4e8e PyTorch \u5b9e\u73b0\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u548c\u63a8\u7406\u7684\u4ee3\u7801\u793a\u4f8b","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nclass KnowledgeGraphDataset(Dataset):\n    def __init__(self, triples):\n        self.triples = triples\n\n    def __len__(self):\n        return len(self.triples)\n\n    def __getitem__(self, idx):\n        return self.triples[idx]\n\nclass TransE(nn.Module):\n    def __init__(self, num_entities, num_relations, emb_dim):\n        super(TransE, self).__init__()\n        self.entity_embeddings = nn.Embedding(num_entities, emb_dim)\n        self.relation_embeddings = nn.Embedding(num_relations, emb_dim)\n\n    def forward(self, h, r, t):\n        emb_h = self.entity_embeddings(h)\n        emb_r = self.relation_embeddings(r)\n        emb_t = self.entity_embeddings(t)\n\n        score = torch.norm(emb_h + emb_r - emb_t, p=2, dim=-1)\n        return score\n\ndef train(model, optimizer, criterion, dataloader, device):\n    model.train()\n    total_loss = 0.0\n    for batch in dataloader:\n        h, r, t = batch\n        h, r, t = h.to(device), r.to(device), t.to(device)\n        optimizer.zero_grad()\n        scores = model(h, r, t)\n        loss = criterion(scores, torch.ones_like(scores))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\n# \u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u6570\u636e\u96c6\ntriples = [(0, 1, 2), (2, 1, 3), (1, 2, 4), (2, 0, 5), (3, 2, 6)]\ndataset = KnowledgeGraphDataset(triples)\ndataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n\n# \u521d\u59cb\u5316\u6a21\u578b\u3001\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\nmodel = TransE(num_entities=7, num_relations=3, emb_dim=10)\ncriterion = nn.MarginRankingLoss(margin=1.0)\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\n# \u8bad\u7ec3\u6a21\u578b\nfor epoch in range(10):\n    loss = train(model, optimizer, criterion, dataloader, device)\n    print(f\"Epoch {epoch + 1} - loss: {loss:.4f}\")\n</code></pre>"},{"location":"AI/NLP/Paper%20Reading/Paper%20Reading/","title":"Summary","text":""},{"location":"AI/NLP/Paper%20Reading/Paper%20Reading/#\u8bba\u6587-1kge","title":"\u8bba\u6587 1\uff1aKGE","text":""},{"location":"AI/NLP/Paper%20Reading/Paper%20Reading/#an-overview-of-methods-and-tools-for-extraction-of-knowledge-for-covid-19-from-knowledge-graphs","title":"An Overview of Methods and Tools for Extraction of Knowledge for COVID-19 from Knowledge Graphs","text":"<p>\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u63d0\u53d6 COVID-19 \u77e5\u8bc6\u7684\u65b9\u6cd5\u548c\u5de5\u5177\u6982\u8ff0</p> <p>Abstract</p> <p>\u672c\u6587\u5b9a\u4e49\u4e86\u53ef\u7528\u4e8e COVID - 19 \u4fe1\u606f\u7684\u641c\u7d22\u5f15\u64ce\u7684\u603b\u4f53\u63cf\u8ff0\u3002\u5bf9 COVID - 19 \u4fe1\u606f\u7684\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u7b80\u8981\u7efc\u8ff0\uff0c\u5e76\u4ecb\u7ecd\u4e86\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u4e25\u91cd\u7279\u6b8a\u4f20\u67d3\u6027\u80ba\u708e\u77e5\u8bc6\u62bd\u53d6\u548c\u7406\u89e3\u7684\u4e3b\u8981\u76f8\u5173\u65b9\u6cd5\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411 COVID - 19 \u7684\u77e5\u8bc6\u63a8\u7406\u65b9\u6cd5\u3002</p> <p>Keywords</p> <p>Knowledge graph \u00b7 COVID-19 \u00b7 Extraction of information \u00b7 Reasoning \u00b7 Information searching \u00b7 Artificial intelligence \u00b7 Web search \u00b7 Information retrieval</p> <p>Introduction</p> <ul> <li>Section 1\uff1a\u5173\u4e8e COVID-19 \u7684\u641c\u7d22\u7cfb\u7edf\u4e0e\u6307\u793a\u56fe\u8c31\u7efc\u8ff0</li> <li>Section2\uff1a\u6784\u5efa COVID-19 \u77e5\u8bc6\u56fe\u8c31\u53ef\u80fd\u9762\u4e34\u95ee\u9898</li> <li>Section 3 and 4\uff1aCOVID-19 \u4fe1\u606f\u63a8\u7406\u7b97\u6cd5\u4f7f\u7528\u5206\u7c7b</li> </ul> <p>Section 1</p> <p>\u7b80\u8981\u4ecb\u7ecd\u4e86\u76ee\u524d\u5173\u4e8e COVID-19 \u7684\u641c\u7d22\u7cfb\u7edf\uff08\u7c7b\u578b\u3001\u7279\u70b9\u3001\u6784\u5efa\u65b9\u5f0f\u7b49\uff09\u4ee5\u53ca\u77e5\u8bc6\u56fe\u8c31</p> <p>Section 2</p> <ol> <li>\u6570\u636e\u7ea6\u675f\uff1a\u75ab\u60c5\u4e0d\u65ad\u6269\u5927\u9020\u6210\u7684\u6570\u636e\u96c6\u4e0d\u65ad\u589e\u591a\u4e14\u53d8\u5316\uff1b\u5d4c\u5165\u77e5\u8bc6\u4e0e\u540c\u5176\u4ed6\u75be\u75c5\u76f8\u5173\u6027\u7684\u4e0d\u65ad\u53d8\u5316\uff1b\u75ab\u60c5\u6269\u6563\u5bf9\u4e8e\u6570\u636e\u663e\u793a\u65b9\u9762\u7684\u5f71\u54cd\uff1b</li> <li>\u6570\u636e\u8d28\u91cf\uff1a\u6570\u636e\u8d28\u91cf\u5f71\u54cd\u7740\u77e5\u8bc6\u56fe\u8c31\u7684\u53ef\u4fe1\u5ea6\u4ee5\u53ca\u8f93\u51fa\u7684\u51c6\u786e\u5ea6\uff0c\u4ee5\u53ca\u5176\u4e0e\u641c\u7d22\u7cfb\u7edf\u7684\u7ed3\u5408</li> <li>\u89c4\u6a21\u7ea6\u675f\uff1a\u6570\u636e\u589e\u52a0\u81f3\u4e00\u5b9a\u91cf\u540e\uff08Big data\uff09\uff0c\u5176\u5206\u5e03\u5c31\u8d8b\u4e8e\u7a33\u5b9a\uff1b\u5927\u6570\u636e\u4e0e\u79c1\u4eba\u4f01\u4e1a\u9690\u79c1\u7684\u51b2\u7a81</li> <li>\u6570\u636e\u5448\u73b0\uff1a\u53ef\u89c6\u5316\u6548\u679c\u597d\u574f</li> <li>\u793e\u4f1a\u7ea6\u675f\uff1a\u65b0\u6280\u672f\u5bf9\u4fdd\u5b88\u7528\u6237\u7684\u51b2\u51fb</li> </ol> <p>Section 3 and 4</p> <ol> <li> <p>KG\u4e2d\u77e5\u8bc6\u63a8\u7406\u7684 COVID-19 \u7684\u62bd\u53d6\u5e94\u7528\u5b9a\u4e49</p> <ul> <li>\u77e5\u8bc6\u63a8\u7406\uff1a\u4ece\u4fe1\u606f\u62bd\u53d6\u4e2d\u5229\u7528\u5df2\u77e5\u77e5\u8bc6\u63a8\u7406\u51fa\u65b0\u77e5\u8bc6\u7684\u8fc7\u7a0b</li> <li>\u65e9\u671f\u7684\u63a8\u7406\u5b66\u4e60\u7531\u903b\u8f91\u5b66\u53ca\u77e5\u8bc6\u5de5\u7a0b\u9886\u57df\u7684\u4e13\u5bb6\u8fdb\u884c\uff0c\u8fd9\u4e00\u65f6\u671f\u7684\u77e5\u8bc6\u56fe\u8c31\u4e5f\u662f\u7531\u4e13\u5bb6\u8fdb\u884c\u6807\u6ce8</li> <li>\u968f\u7740\u4e92\u8054\u7f51\u9020\u6210\u7684\u6570\u636e\u89c4\u6a21\u7684\u7206\u70b8\u5f0f\u589e\u957f\uff0c\u4f20\u7edf\u4eba\u5de5\u6784\u5efa\u65b9\u5f0f\u5df2\u7ecf\u4e0d\u80fd\u9002\u5e94\u5927\u91cf\u77e5\u8bc6\u6316\u6398\u7684\u9700\u8981\uff0c\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u63a8\u7406\u65b9\u6cd5\u6210\u4e3a\u4e3b\u6d41</li> <li>\u968f\u7740\u77e5\u8bc6\u56fe\u8c31\u7684\u53d1\u5c55\uff0c\u5bf9\u77e5\u8bc6\u56fe\u8c31\u7684\u63a8\u7406\u4e5f\u6108\u53d1\u53d7\u5230\u5173\u6ce8</li> </ul> </li> <li> <p>\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684 COVID-19 \u77e5\u8bc6\u63a8\u7406\u901a\u7528\u6a21\u578b</p> <p>\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u53ef\u4ee5\u88ab\u5b9a\u4e49\u4e3a\u4ee5\u4e0b\u4efb\u52a1\uff1a + \u7f3a\u5931\u5b9e\u4f53\u9884\u6d4b\uff1a\u5df2\u77e5\u5b9e\u4f53 N1 \u4e0e\u5173\u7cfb RR\uff0c\u9884\u6d4b\u7f3a\u5931\u5b9e\u4f53 N2N2 + \u7f3a\u5931\u8054\u7cfb\u9884\u6d4b\uff1a\u5df2\u77e5 2 \u4e2a\u5b9e\u4f53\uff0c\u9884\u6d4b\u5176\u5173\u7cfb + \u4e8b\u5b9e\u9884\u6d4b\uff1a\u4ece\u4e09\u5143\u7ec4\u9884\u6d4b\u4e8b\u4ef6 FF \u4e3a true \u6216 false</p> </li> <li> <p>\u9762\u5411 COVID-19 \u5b9e\u4f53\u62bd\u53d6\u7684\u5206\u7c7b\u77e5\u8bc6\u63a8\u7406</p> <p>(1) \u57fa\u4e8e\u903b\u8f91\u89c4\u5219\u7684\u77e5\u8bc6\u63a8\u7406     + \u57fa\u4e8e\u89c4\u5219\u7684\u77e5\u8bc6\u63a8\u7406     + \u57fa\u4e8e\u4e8b\u4f8b\u7684\u77e5\u8bc6\u63a8\u7406\uff08\u4ece\u4e8b\u4f8b\u4e2d\u63a8\u65ad\u5b9e\u4f53\u5173\u7cfb\uff09     + \u57fa\u4e8e\u672c\u4f53\u7684\u77e5\u8bc6\u63a8\u7406\uff08\u6784\u5efa\u672c\u4f53\uff0c\u8fdb\u884c\u8bad\u7ec3\uff09     + \u57fa\u4e8e\u968f\u673a\u6e38\u8d70\u7b97\u6cd5\u7684\u77e5\u8bc6\u63a8\u7406\uff08\u968f\u673a\u6e38\u8d70\u7b97\u6cd5\u8bc6\u522b\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u5173\u7cfb\uff09</p> <p>(2) \u57fa\u4e8e\u5206\u5e03\u5f0f\u8868\u793a\u7684\u77e5\u8bc6\u63a8\u7406 + \u57fa\u4e8e\u5f20\u91cf\u5206\u89e3\u7684\u77e5\u8bc6\u63a8\u7406     + \u7528\u4e8e\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u7684\u5f20\u91cf\u77e9\u9635\u6846\u67b6 + \u8bed\u4e49\u5339\u914d\u6a21\u578b\u4e0a\u7684\u77e5\u8bc6\u63a8\u7406 + \u57fa\u4e8e\u591a\u6e90\u4fe1\u606f\u7684\u77e5\u8bc6\u63a8\u7406</p> <p>(3) \u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u77e5\u8bc6\u63a8\u7406 + \u5377\u79ef\u795e\u7ecf\u7f51\u7edc + \u6a21\u7cca\u5e72\u6270\u4e0e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6df7\u5408\u63a8\u7406\u6a21\u578b + \u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u56fe\u5377\u79ef\u7f51\u7edc\u7684\u9884\u6d4b\u6a21\u578b</p> </li> </ol>"},{"location":"AI/NLP/Paper%20Reading/Paper%20Reading/#\u8bba\u6587-2kge","title":"\u8bba\u6587 2\uff1aKGE","text":""},{"location":"AI/NLP/Paper%20Reading/Paper%20Reading/#tebc-net-an-effective-relation-extraction-approach-for-simple-question-answering-over-knowledge-graphs","title":"TEBC-Net: An Effective Relation Extraction Approach for Simple Question Answering over Knowledge Graphs","text":"<p>TEBC-Net\uff1a\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u7b80\u5355\u95ee\u7b54\u7684\u6709\u6548\u5173\u7cfb\u63d0\u53d6\u65b9\u6cd5</p> <p>Abstract</p> <p>\u77e5\u8bc6\u56fe\u8c31\u7b80\u5355\u95ee\u7b54 (KGSQA) \u65e8\u5728\u901a\u8fc7\u5bf9\u77e5\u8bc6\u56fe\u8c31\u4e0a\u5355\u4e2a\u5b9e\u4f53\u7684\u67e5\u627e\u6765\u56de\u7b54\u81ea\u7136\u8bed\u8a00\u95ee\u9898\uff0c\u800c\u5173\u7cfb\u62bd\u53d6\u4f5c\u4e3a\u5176\u6838\u5fc3\u4eba\u7269\u4e4b\u4e00\uff0c\u5bf9\u6700\u7ec8\u7b54\u6848\u7684\u8d28\u91cf\u6709\u7740\u91cd\u8981\u5f71\u54cd\u3002\u4e3a\u63d0\u9ad8 KGSQA \u4e2d\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u7684\u51c6\u786e\u7387\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b TEBC-Net\uff0c\u8be5\u6a21\u578b\u57fa\u4e8e Transformer Encoder\u3001BiLSTM \u4ee5\u53ca CNN net\uff0c\u5e76\u7531\u4e0a\u8ff0\u51e0\u79cd\u6a21\u578b\u8fdb\u884c\u65e0\u7f1d\u7ed3\u5408\u6784\u5efa\u3002\u5e76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u7387\u8f83\u9ad8\u3002</p> <p>Keywords</p> <p>Knowledge graph simple question answering \u00b7 Relation extraction \u00b7 Natural language processing \u00b7 Deep learning \u00b7 TEBC-Net</p> <p>Introduction</p> <p>\u77e5\u8bc6\u56fe\u8c31\u662f\u771f\u5b9e\u4e16\u754c\u5b9e\u4f53\u4e4b\u95f4\u76f8\u4e92\u8054\u7cfb\u7684\u62bd\u8c61\u8868\u793a\u3002\u4f46\u56e0\u4e3a\u77e5\u8bc6\u56fe\u8c31\u4e2d\u5305\u542b\u7684\u5927\u91cf\u6570\u636e\uff0c\u9664\u975e\u719f\u6089\u76f8\u5173\u7684\u67e5\u8be2\u8bed\u6cd5\uff0c\u4e00\u822c\u7528\u6237\u5f88\u96be\u6709\u6548\u7684\u5728\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u3002\u4e3a\u5e94\u5bf9\u8be5\u95ee\u9898\uff0c\u5f25\u8865\u8bed\u4e49\u5dee\u8ddd\uff0c\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u5668 (KGQA) \u88ab\u63d0\u51fa\u3002KGSQA \u5373\u662f\u8be5\u9886\u57df\u7684\u5178\u578b\u7814\u7a76\u95ee\u9898\uff0c\u5b83\u4fa7\u91cd\u4e8e\u901a\u8fc7\u5bf9 KG \u4e2d\u5355\u4e2a\u4e8b\u5b9e\u7684\u67e5\u627e\u6765\u56de\u7b54\u7b80\u5355\u95ee\u9898\u3002</p> <p>\u5bf9\u4e8e KGSQA\uff0c\u77e5\u8bc6\u62bd\u53d6 (\u5728\u67d0\u79cd\u7c7b\u578b\u7684\u4e24\u4e2a\u6216\u591a\u4e2a\u5b9e\u4f53\u4e2d\u62bd\u53d6\u8bed\u4e49\u8054\u7cfb) \u662f\u5176\u4e2d\u7684\u4e00\u9879\u91cd\u8981\u4efb\u52a1\u3002\u77e5\u8bc6\u62bd\u53d6\u53ef\u4ee5\u88ab\u7528\u6765\u63d0\u53d6\u81ea\u7136\u8bed\u8a00\u8be2\u95ee\u4e2d\u7684\u539f\u59cb\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u5728\u5176\u57fa\u7840\u4e0a\u6784\u5efa KG \u67e5\u8be2\u3002</p> <p>\u4e3a\u4e86\u63d0\u9ad8 KGSQA \u4e2d RE (\u5173\u7cfb\u62bd\u53d6) \u7684\u51c6\u786e\u5ea6\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e Transformer Encoder\u3001BiLSTM \u4ee5\u53ca CNN net \u65e0\u7f1d\u7ed3\u5408\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b \u2014\u2014 TEBS-Net\u3002\u5177\u4f53\u5730\uff0c\u9996\u5148\uff0c\u5f15\u5165 knowledge graph embedding (KGE) \u7b97\u6cd5\u5bf9 KG \u4e2d\u7684\u4e8b\u5b9e\u8fdb\u884c\u7f16\u7801\uff1b\u5176\u6b21\uff0c\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u95ee\u9898\uff0c\u4f7f\u7528 BiLSTM \u6355\u83b7\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u5229\u7528 Transformer \u4e2d\u7f16\u7801\u6a21\u5757\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u53bb\u62bd\u53d6\u5173\u952e\u4fe1\u606f\uff1b\u4e4b\u540e\uff0c\u4f7f\u7528 CNN \u6355\u83b7\u95ee\u9898\u7684\u5b57\u7b26\u4fe1\u606f\uff0c\u5e76\u518d\u6b21\u5229\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u5df2\u5b8c\u6210\u77e5\u8bc6\u62bd\u53d6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u4f18\u5316\u6548\u679c\u8f83\u4e3a\u660e\u663e\u3002</p> <p>\u6587\u7ae0\u7684\u7ed3\u6784\u5982\u4e0b\uff1a</p> <ul> <li>Section 2\uff1a\u76f8\u5173\u5de5\u4f5c</li> <li>Section 3\uff1aTEBC-Net \u7684\u8be6\u7ec6\u7ed3\u6784\u4e0e\u5b9e\u73b0</li> <li>Section 4\uff1a\u5b9e\u9a8c\u7ed3\u679c</li> <li>Section 5\uff1a\u603b\u7ed3\u4e0e\u524d\u666f</li> </ul> <p>Section 2</p> <ol> <li> <p>KGQA</p> </li> <li> <p>\u57fa\u4e8e\u6a21\u677f\u7684\u95ee\u7b54\u65b9\u6cd5\uff08\u9884\u5b9a\u4e49\u6a21\u677f\u5339\u914d\u3001\u5f62\u5f0f\u5316\u67e5\u8be2\uff09\uff08\u6a21\u677f\u751f\u6210\u6210\u672c\u9ad8\u3001\u6570\u91cf\u6709\u9650\uff09</p> </li> <li> <p>\u57fa\u4e8e\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u5b66\u4e60\u7684 KGQA \u65b9\u6cd5\uff08\u591a\u901a\u9053 CNN \u63d0\u53d6\u5173\u7cfb\uff0c\u5c06\u5173\u7cfb\u77ed\u8bed\u6620\u5c04\u5230\u77e5\u8bc6\u5e93\u8c13\u8bcd\uff09</p> </li> <li> <p>RE</p> </li> <li> <p>\u4f20\u7edf\u5206\u7c7b RE</p> </li> <li>\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b</li> </ol> <p>The Proposed TEBC-Net for KGSQA</p> <p>\u5bf9\u4e8e\u77e5\u8bc6\u56fe\u8c31\u6240\u80fd\u56de\u7b54\u7684\u6240\u6709\u95ee\u9898\uff0c\u5fc5\u987b\u5c06\u5173\u7cfb\u7684\u5411\u91cf\u8868\u793a\u63d0\u524d\u6295\u5f71\u5230\u5173\u7cfb\u5d4c\u5165\u7a7a\u95f4\u4e2d\uff0c\u8be5\u79cd\u60c5\u51b5\u4e0b\uff0c\u5c06 KGSQA \u7684\u8fc7\u7a0b\u5206\u4e3a 3 \u6b65\uff1a</p> <ol> <li> <p>\u4f7f\u7528\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165 (KGE) \u5c06 KG \u4e2d\u7684\u6bcf\u4e2a\u5b9e\u4f53\u4e0e\u5173\u7cfb\u4f5c\u4e3a\u4f4e\u7ef4\u5411\u91cf</p> <ul> <li>\u4f7f\u7528\u4f4e\u7ef4\u5411\u91cf\u5d4c\u5165\u77e5\u8bc6\u56fe\u8c31\u662f KGSQA \u4e2d\u5178\u578b\u7684\u9884\u5904\u7406\u65b9\u6cd5</li> <li>KGE \u5c06 KG \u4e2d\u6bcf\u4e2a\u5b9e\u4f53\u4e0e\u5173\u7cfb\u8868\u793a\u4e3a\u4e00\u4e2a\u4f4e\u7ef4\u5411\u91cf\uff0c\u5e76\u80fd\u4ee5\u6570\u503c\u5411\u91cf\u7684\u5f62\u5f0f\u4fdd\u6301 KG \u539f\u6709\u7684\u7ed3\u6784\u4e0e\u5173\u7cfb</li> <li>\u5bf9\u4e8e\u7b80\u5355\u95ee\u9898\uff08\u4e00\u4e2a\u5934\u5b9e\u4f53\u4e0e\u4e00\u4e2a\u5173\u7cfb\uff09\uff0c\u4f7f\u7528 TransE \u6a21\u578b</li> </ul> </li> <li> <p>\u5c06\u95ee\u9898\u4f5c\u4e3a\u8f93\u5165\uff0c\u8f93\u5165\u81f3 TEBC-Net\uff0c\u8fd4\u56de\u4e00\u4e2a\u6620\u5c04\u5411\u91cf\uff0c\u8ba1\u7b97\u4e24\u5411\u91cf\u95f4\u7684\u5411\u91cf\u7a7a\u95f4\u8ddd\u79bb\u5e76\u8fdb\u884c\u6392\u5e8f\uff0c\u5b8c\u6210 RE</p> </li> <li> <p>\u5bf9\u4e8e\u7ed9\u5b9a\u95ee\u9898\u8fdb\u884c\u77e5\u8bc6\u62bd\u53d6\uff0c\u65b9\u6cd5\u4e2d\u8003\u8651\u4e0b\u8ff0\u4e09\u9879\u6838\u5fc3\u56e0\u7d20\uff1a\u8bcd\u7684\u5168\u5c40\u7279\u5f81\u3001\u8bcd\u7684\u5b57\u7b26\u7279\u5f81\u3001\u8bcd\u7684\u91cd\u8981\u6027</p> </li> <li> <p>Transformer Encoding layer \u4e2d\u7684 self-attention mechanism \u5bf9\u8bcd\u8bed\u91cd\u8981\u6027\u8fc7\u6ee4\u7f16\u7801</p> </li> <li> <p>CNN \u8868\u793a\u5f85\u7f16\u7801\u95ee\u9898\u7279\u5f81</p> </li> <li> <p>BiLSTM \u53cc\u5411\u673a\u5236\u4ee3\u66ff Transformer \u4e09\u89d2\u4f4d\u7f6e\u7f16\u7801\uff0c\u4ee5\u5bf9\u751f\u6210\u8bcd\u4e0a\u4e0b\u6587\u5e8f\u5217\u6b63\u786e\u7f16\u7801 </p> <ul> <li>\u5728\u8bed\u5e8f\u7f16\u7801\u8fc7\u7a0b\u4e2d\u53ef\u540c\u65f6\u8868\u793a\u7edd\u5bf9\u4f4d\u7f6e\u4e0e\u76f8\u5bf9\u4f4d\u7f6e</li> <li>Transformer \u4e09\u89d2\u51fd\u6570\u4f4d\u7f6e\u5d4c\u5165\u53ef\u68c0\u67e5\u5355\u8bcd\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u4f46\u662f\u5ffd\u89c6\u4e86\u5355\u8bcd\u7684\u76f8\u5bf9\u65b9\u5411</li> </ul> </li> <li> <p>TEBC - Net  </p> </li> </ol> <p>TEBC - Net</p> <ul> <li> <p>Left Part \u2014\u2014 Global Features with Importance</p> <ul> <li>\u8054\u5408 BiLSTM \u4e0e self-attention \u5bf9\u95ee\u9898\u8fdb\u884c\u7f16\u7801</li> </ul> </li> <li> <p>Right Part \u2014\u2014 Character Features with Importance  </p> <ul> <li>\u8054\u5408 CNN \u4e0e self-attention \u5bf9\u95ee\u9898\u8fdb\u884c\u7f16\u7801</li> </ul> </li> <li> <p>\u57fa\u4e8e\u63d0\u53d6\u7684\u5173\u7cfb\u8fdb\u884c KGSQA</p> <ul> <li>\u5728 KEQA \u6846\u67b6\u4e0b\uff0c\u5728 KG \u4e2d\u4e0e QA \u6709\u5173\u7cfb\u7684\u4efb\u52a1\u4e3b\u8981\u6709\u4e09\u4e2a (HED)<ul> <li>\u5934\u90e8\u5b9e\u4f53\u8bc6\u522b</li> <li>\u5173\u7cfb\u62bd\u53d6</li> <li>\u5934\u90e8\u5b9e\u4f53\u68c0\u6d4b</li> </ul> </li> </ul> </li> </ul> <p>TEBC - Net HED</p> <p>Conclusion</p> <p>\u7c7b\u4f3c\u4e8e Introduction </p>"},{"location":"AI/NLP/Paper%20Reading/Paper%20Reading/#\u8bba\u6587-3survey\u5e38\u8bc6\u6027\u77e5\u8bc6","title":"\u8bba\u6587 3\uff1aSurvey\u3001\u5e38\u8bc6\u6027\u77e5\u8bc6","text":""},{"location":"AI/NLP/Paper%20Reading/Paper%20Reading/#information-to-wisdom-commonsense-knowledge-extraction-and-compilation","title":"Information to Wisdom: Commonsense Knowledge Extraction and Compilation","text":"<p>\u4fe1\u606f\u5230\u667a\u6167\uff1a\u5e38\u8bc6\u6027\u77e5\u8bc6\u7684\u63d0\u53d6\u4e0e\u6c47\u7f16</p> <p>Abstract</p> <p>\u5e38\u8bc6\u6027\u77e5\u8bc6\u662f\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u7684\u57fa\u77f3\u3002\u867d\u7136 (Whereas) \u9762\u5411\u5b9e\u4f8b\u65ad\u8a00\u7684\u4fe1\u606f\u62bd\u53d6\u4e0e\u77e5\u8bc6\u5e93\u6784\u5efa\u5df2\u7ecf\u5f97\u5230\u8f83\u591a\u5173\u6ce8\uff0c\u4f46\u5173\u4e8e\u4e00\u822c\u6982\u5ff5\u4e0e\u6d3b\u52a8\u7684\u7684\u5e38\u8bc6\u6027\u77e5\u8bc6\uff0c\u6700\u8fd1\u624d\u5f97\u5230\u89e3\u51b3\u3002\u5728\u672c\u6559\u7a0b (tutorial) \u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u6765\u6c47\u7f16\u4e0e\u5de9\u56fa\u8fd9\u4e9b\u5e38\u8bc6\u77e5\u8bc6\u3002\u6db5\u76d6\u4e86\u57fa\u4e8e\u6587\u672c\u62bd\u53d6\u3001\u591a\u6a21\u6001\u4ee5\u53ca\u57fa\u4e8e Transformer \u7684\u6280\u672f\uff0c\u7279\u522b\u5173\u6ce8 (with speical focus on) \u4e0e WSDM \u793e\u533a\u76f8\u5173\u7684\u7f51\u7edc\u641c\u7d22\u4e0e\u6392\u540d\u95ee\u9898\u3002</p> <p>Motivation</p> <p>Overview </p> <p>\u5e38\u8bc6\u77e5\u8bc6 (CSK) \u5bf9\u4e8e\u5efa\u7acb\u591a\u529f\u80fd\u53ea\u80fd\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u5728\u4ee5\u547d\u540d\u5b9e\u4f53\u4e3a\u4e2d\u5fc3\u7684\u767e\u79d1\u77e5\u8bc6\u5212\u5b9a\u4e2d\uff0c\u5e38\u8bc6\u88ab\u7528\u6765\u6307\u4ee3\u4e00\u822c\u6982\u5ff5\u4e4b\u95f4\u7684\u5c5e\u6027\u3001\u7279\u5f81\u4e0e\u5173\u7cfb\u3002CSK \u7684\u673a\u5668\u53ef\u8bfb\u96c6\u5408\u5bf9\u4e8e\u5b9e\u73b0\u95ee\u7b54\u4ee5\u53ca\u81ea\u7136\u4ea4\u6d41\u975e\u5e38\u91cd\u8981\u3002</p> <p>\u7ed3\u6784\u5316\u5e38\u8bc6\u77e5\u8bc6\u4e0e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u76f8\u4e92\u4f5c\u7528\uff1a\u540e\u8005\u53ef\u4ee5\u7528\u6765\u751f\u6210\u6d41\u7545\u7684\u81ea\u7136\u8bed\u8a00\uff0c\u800c\u524d\u8005\u5bf9\u4e8e\u5c06\u8bed\u8a00\u751f\u6210\u9650\u5236\u5728\u5408\u7406\u4e14\u8fde\u8d2f\u7684\u8bed\u53e5\u7684\u8303\u56f4\u5185\u4e5f\u9887\u4e3a\u91cd\u8981\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u5de5\u4e1a\u5e94\u7528\uff0c\u7ed3\u6784\u5316\u5e38\u8bc6\u4e3a\u9632\u6b62\u4e00\u4e9b\u5e94\u7528\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u8fa8\u8ba4\u7684\u8d44\u6e90\u3002</p> <p>Brief Outline Of Topic</p> <ol> <li> <p>\u7b80\u4ecb (Introduction)</p> <ul> <li>\u4ec0\u4e48\u662f CSK\uff1f\u4e3a\u4ec0\u4e48 CSK \u8fd9\u4e48\u91cd\u8981\uff1f\u662f\u4ec0\u4e48\u4ee4\u5176\u7f16\u8bd1\u5177\u6709\u6311\u6218\u6027\uff1f<ul> <li>\u6fc0\u53d1 CSK \u5728\u5bf9\u8bdd\u4e0e\u5176\u5b83\u5e94\u7528\u4e2d\u7684\u76f8\u5173\u6027\uff0c\u5e76\u5c06\u5176\u4e0e\u5c0f\u6570\u636e\u7528\u4f8b\u76f8\u8054\u7cfb </li> <li>\u7f16\u8bd1\u9762\u4e34\u83b7\u53d6 (reporting bias etc.)\u3001\u805a\u5408 (incoherence etc.)\u4ee5\u53ca\u5e94\u7528 (mismatches between structured data and neural networks)</li> </ul> </li> <li>\u77e5\u8bc6\u8868\u793a<ul> <li>CYC \u9879\u76ee\u4e2d\u4f7f\u7528\u7684\u8868\u8fbe\u6a21\u6001 (expressive modal) \u4ee5\u53ca\u8ba4\u77e5\u903b\u8f91</li> <li>\u8bed\u4e49 Web \u9879\u76ee\u4e2d\u6d41\u884c\u7684\u5355\u7ef4\u5f97\u5206\u4e09\u5143\u7ec4</li> <li>\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e0e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5206\u5e03\u5f0f\u8868\u793a</li> </ul> </li> <li>\u5386\u53f2<ul> <li>CSK \u7814\u7a76\u7684\u6839\u6e90</li> <li>\u65e9\u671f\u9879\u76ee\uff1aCYC\u3001WordNet \u4ee5\u53ca ConceptNet</li> <li>\u7528\u4f8b\u5386\u53f2\uff1a\u767e\u79d1\u5168\u4e66\u95ee\u7b54 \u2014\u2014&gt; \u5e38\u8bc6\u63a8\u7406</li> </ul> </li> </ul> </li> <li> <p>\u6587\u672c\u62bd\u53d6 (Text extraction)</p> <ul> <li>Recipe\uff1a\u4ecb\u7ecd\u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u3001\u5408\u5e76 CSK \u7684\u65b9\u6cd5<ul> <li>Source selection (\u6570\u636e\u6e90\u9009\u62e9)<ul> <li>popular resource\uff1aWikipedia\u3001open web crawls\u3001scientific documents\u3001QA-Forums etc.</li> <li>\u8ba8\u8bba\u6570\u91cf\u3001\u8d28\u91cf\u3001\u771f\u5b9e\u6027\u3001\u663e\u8457\u6027\u7684\u6743\u8861</li> </ul> </li> <li>Extraction paradigm (\u62bd\u53d6\u8303\u5f0f)<ul> <li>\u5bf9\u6d41\u884c\u7684\u62bd\u53d6\u8303\u5f0f\u5206\u7c7b<ul> <li>\u89c4\u8303\u5316 (canonicalized) || \u5f00\u653e\u5173\u7cfb (open relation) \u62bd\u53d6 || \u4e3b\u5ba2\u4f53\u89c4\u8303\u5316</li> <li>\u5b9e\u4f53 (entity) || \u6d3b\u52a8 (activity) || \u9762\u5411\u5c5e\u6027 (property-oriented) \u7684\u5bf9\u8c61</li> </ul> </li> <li>\u63d0\u51fa\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u6838\u5fc3\u5e95\u5c42\u6982\u5ff5</li> </ul> </li> <li>Consolidation (\u5408\u5e76)<ul> <li>\u6574\u5408\u521d\u59cb\u62bd\u53d6\u7684\u5fc5\u8981\u6027</li> <li>\u7efc\u8ff0\u57fa\u4e8e\u6807\u7b7e\u4f20\u64ad (label propagation)\u3001\u591a\u6e90\u9a8c\u8bc1 (multi-source validation)\u3001\u8f6f\u7ea6\u675f\u63a8\u7406 (soft constraint reasoning) \u7684\u7a81\u51fa\u65b9\u6cd5</li> </ul> </li> </ul> </li> <li>\u6848\u4f8b\u7814\u7a76</li> </ul> </li> <li> <p>\u591a\u6a21\u6001\u77e5\u8bc6 (Multimodel knowledge)</p> <p>\u63cf\u8ff0\u4e86\u5728\u89c6\u89c9\u4e0e\u6587\u672c\u7684 KG \u4e2d\u5448\u73b0\u4e92\u8865\u77e5\u8bc6\u7684\u8bba\u6587\u3002\u4f8b\u5982\u89c6\u89c9\u5e38\u8bc6\u5e93\u5c31\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u63d0\u4f9b\u89e3\u91ca\u3002\u540c\u65f6\u8ba8\u8bba\u4e86\u4ece\u73b0\u5b9e\u4e16\u754c\u9690\u5f0f\u3001\u663e\u5f0f\u6536\u96c6\u5e38\u8bc6\u7684\u65b9\u6cd5</p> </li> <li> <p>\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6280\u672f (Deep learning based techniques)</p> <ul> <li> <p>\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u77e5\u8bc6\u6e90</p> <p>\u6709\u8bc1\u636e\u8bc1\u660e\uff0c\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u5305\u542b\u90e8\u5206\u7c7b\u578b\u7684 CSK\uff0c\u4f46\u540c\u65f6\u4e0d\u5305\u542b\u5176\u4ed6\u7c7b\u578b\u7684\u77e5\u8bc6\u3002\u8fd9\u4e9b\u795e\u7ecf\u7ed3\u6784\u53ef\u4ee5\u901a\u8fc7\u73b0\u6709\u7684\u7b26\u53f7 KG \u8fdb\u884c\u589e\u5f3a\uff0c\u4ee5\u4f7f\u6a21\u578b\u80fd\u591f\u6cdb\u5316\u77e5\u8bc6\uff0c\u6216\u662f\u5728\u591a\u6a21\u6001\u6570\u636e\u4e0a\u5b66\u4e60\u89c6\u89c9\u4e0e\u8bed\u8a00\u7684\u901a\u7528\u8868\u793a\u3002\u7136\u540e\uff0c\u8ba8\u8bba\u5982\u4f55\u901a\u8fc7\u5c06\u6982\u7387\u8d28\u91cf (probability mass) \u4ece\u96be\u7f6e\u4fe1\u7684 (implausible) w.r.t CSK \u8f93\u51fa\u4e0a\u79fb\u9664\uff0c\u4ee5\u5728\u795e\u7ecf\u6a21\u578b\u4e2d\u878d\u5165\u7b26\u53f7\u6027\u77e5\u8bc6\u3002\u8fd9\u4e9b\u6a21\u578b\u5728\u8ba4\u4e3a\u5176\u5b58\u5728\u6570\u636e\u7f3a\u53e3\u65f6\uff0c\u53ef\u4ee5\u81ea\u52a8\u54a8\u8be2\u5916\u90e8\u77e5\u8bc6\u3002</p> </li> <li> <p>\u7528\u4e8e\u901a\u7528\u77e5\u8bc6\u7f16\u8bd1\u4e0e\u5e94\u7528\u7684\u795e\u7ecf\u6a21\u578b</p> <p>\u80fd\u5426\u5c06\u77e5\u8bc6\u3001\u7b26\u53f7\u3001\u795e\u7ecf\u7b49\u591a\u79cd\u5f02\u8d28\u8d44\u6e90\u7ed3\u5408\u8d77\u6765\uff1f\u4e00\u7bc7 paper \u4e2d\u63d0\u51fa\u4f7f\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7ec4\u5408\u65b9\u6cd5\uff0c\u800c\u5f53\u524d\u65b9\u6cd5\u5219\u662f\u8bad\u7ec3\u7528\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u5355\u4e00\u6a21\u578b\uff0c\u6211\u4eec\u8ba8\u8bba\u8fd9\u4e9b\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9</p> </li> </ul> </li> <li> <p>\u83b7\u53d6\u77e5\u8bc6\u8bc4\u4ef7 (Evaluation of the acquired knowledge)</p> <ul> <li>\u83b7\u53d6\u7684\u77e5\u8bc6\u8d28\u91cf\u662f\u5426\u591f\u597d\uff1f         \u56de\u987e\u5185\u5728\u8d28\u91cf (intrinsic quality) \u7684\u7ef4\u5ea6\uff0c\u4ee5\u53ca\u6ce8\u91ca\u4efb\u52a1\u7684\u9002\u5f53\u8bbe\u7f6e</li> <li>\u83b7\u53d6\u7684\u77e5\u8bc6\u662f\u5426\u53ef\u7528\uff1f</li> </ul> </li> <li> <p>\u4eae\u70b9\u3001\u5c55\u671b\u4e0e\u5f00\u653e\u95ee\u9898 (Highlights, Outlook and Open Issues)</p> </li> </ol> <p>Relevance To The Community</p> <p>\u5e38\u8bc6\u6027\u77e5\u8bc6\u76f8\u5173\u7126\u70b9\u6559\u7a0b\uff1a</p> <ol> <li> <p>Commonsense Reasoning for Natural Language Processing, Sap et al., ACL 2020.</p> <p>\u8be5\u6559\u7a0b\u4ee5 NLP \u4e3a\u4e2d\u5fc3\uff0c\u5f3a\u8c03\u63a8\u7406\uff0c\u901a\u5e38\u901a\u8fc7\u60c5\u5883\u5f0f\u591a\u9009\u9898\u56de\u7b54\u8fdb\u884c\u8bc4\u4f30\u3002\u5c55\u793a\u4e86\u56f4\u7ed5 Transform \u67b6\u6784\u6784\u5efa\u7684\u524d\u6cbf\u6280\u672f\uff0c\u4f46\u5bf9\u53ef\u89e3\u91ca\u7684 CSK \u96c6\u5408\u5173\u6ce8\u8f83\u5c11\u3002</p> </li> <li> <p>Common Sense Knowledge Graphs (CSKGs), Ilievski et al, ISWC 2020.</p> <p>\u8be5\u6587\u732e\u9488\u5bf9\u8bed\u4e49 Web \u793e\u533a\u3002\u91cd\u70b9\u7814\u7a76\u4e86 CSK \u7684\u94fe\u63a5\u6570\u636e\u65b9\u9762\uff0c\u53ca\u5176\u9ad8\u6548\u96c6\u6210\u4e0e\u5229\u7528</p> </li> </ol>"},{"location":"AI/NLP/Paper%20Reading/Paper%20Reading/#\u8bba\u6587-4surveykbqakr","title":"\u8bba\u6587 4\uff1aSurvey\u3001KBQA\u3001KR","text":""},{"location":"AI/NLP/Paper%20Reading/Paper%20Reading/#a-survey-on-complex-knowledge-base-question-answering-methods-challenges-and-solutions","title":"A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions","text":"<p>Abstract</p> <p>\u77e5\u8bc6\u5e93\u95ee\u7b54 (KBQA) \u65e8\u5728\u901a\u8fc7\u77e5\u8bc6\u5e93\u8fdb\u884c\u95ee\u7b54\u3002\u8fd1\u6765\uff0c\u5927\u91cf\u7814\u7a76\u66f4\u52a0\u5173\u6ce8\u8bed\u4e49/\u53e5\u6cd5\u590d\u6742\u7684\u95ee\u9898\u3002\u5728\u672c\u7bc7 paper \u4e2d\uff0c\u4f5c\u8005\u8be6\u5c3d\u7684\u603b\u7ed3\u4e86\u590d\u6742 KBQA \u9762\u4e34\u7684\u5404\u7c7b\u6311\u6218\u53ca\u5176\u89e3\u51b3\u63aa\u65bd\u3002\u6587\u7ae0\u7531\u4ecb\u7ecd KBQA \u4efb\u52a1\u7684\u80cc\u666f\u5f00\u59cb\u3002\u5176\u6b21\u4ecb\u7ecd\u4e24\u7c7b\u4e3b\u6d41\u7684\u590d\u6742 KBQA \u65b9\u6cd5\uff1a\u57fa\u4e8e\u8bed\u4e49\u89e3\u6790 (SP-based) \u7684\u65b9\u6cd5 \u4ee5\u53ca \u57fa\u4e8e\u4fe1\u606f\u68c0\u7d22 (IR-based) \u7684\u65b9\u6cd5\u3002\u7136\u540e\uff0c\u4ece\u4e0a\u8ff0\u4e24\u4e2a\u7c7b\u522b\u7684\u89d2\u5ea6\u5168\u65b9\u9762\u56de\u987e\u4e86\u5148\u8fdb\u65b9\u6cd5\u3002\u5177\u4f53\u800c\u8a00\uff0c\u4f5c\u8005\u4eec\u5728\u95ee\u4e2d\u9610\u8ff0\u4e86\u5404\u7c7b\u65b9\u6cd5\u5bf9\u5178\u578b\u6311\u6218\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6700\u540e\uff0c\u63a8\u65ad\u4e0e\u8ba8\u8bba\u672a\u6765\u7814\u7a76\u7684\u53d1\u5c55\u65b9\u5411</p> <p>Introduction</p> <p>KB \u662f\u4e00\u4e2a\u5305\u542b (subject, relation, object)(subject, relation, object) \u4e8b\u5b9e\u7684\u96c6\u5408\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\uff0c\u5e38\u89c1\u7684\u5927\u89c4\u6a21 KB \u6709\uff1a Freebase\u3001DBPedia\u3001Wikidata\uff0c\u4ed6\u4eec\u5df2\u7ecf\u88ab\u7528\u6765\u6784\u5efa\u8bb8\u591a\u4e0b\u6e38\u4efb\u52a1\u3002\u5728\u53ef\u5229\u7528 KB \u7684 \u57fa\u7840\u4e0a\uff0cKBQA \u662f\u4ee5KB \u4e3a\u77e5\u8bc6\u6e90\uff0c\u56de\u7b54\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u7684\u4efb\u52a1\u3002KBQA \u7684\u65e9\u671f\u4efb\u52a1\u5173\u6ce8\u56de\u7b54\u53ea\u542b\u5355\u4e00\u4e8b\u5b9e\u7684\u7b80\u5355\u95ee\u9898\u3002</p> <p>\u8fd1\u6765\uff0c\u7814\u7a76\u8005\u66f4\u5173\u6ce8\u4e8e\u901a\u8fc7 KB \u6765\u56de\u7b54\u590d\u6742\u95ee\u9898\u3002\u590d\u6742\u95ee\u9898\u901a\u5e38\u90fd\u5305\u542b\u591a\u4e2a\u4e3b\u4f53\uff0c\u8868\u73b0\u6df7\u5408\u7684\u5173\u7cfb\u5e76\u4e14\u5305\u542b\u6570\u503c\u8fd0\u7b97\u3002</p> <p>\u590d\u6742\u95ee\u9898\u793a\u4f8b</p> <p>This example question starts with the subject \u201cThe Jeff Probst Show\u201d. Instead of querying a single fact, the question requires the composition of two relations, namely, \u201cnominee\u201d and \u201cspouse\u201d. This query is also associated with an entity type constraint \u201c(Jeff Probst, is a, TV producer)\u201d. The final answer should be further aggregated by selecting the possible candidates with the earliest marriage date. Generally, complex questions are questions involving multi-hop reasoning, constrained relations, numerical operations, or some combination of the above.</p> <p>\u56de\u5230\u5bf9\u4e8e\u7b80\u5355 KBQA \u7684\u89e3\u51b3\u65b9\u5f0f\uff0c\u6765\u81ea\u4e0a\u6587\u4e24\u79cd\u4e3b\u6d41\u65b9\u6cd5\u7684\u5927\u91cf\u7814\u7a76\u88ab\u63d0\u51fa\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u9996\u5148\u8bc6\u522b\u95ee\u9898\u4e2d\u7684\u4e3b\u4f53\uff0c\u7136\u540e\u5c06\u5176\u4e0e KB \u4e2d\u7684\u5b9e\u4f53\u8fdb\u884c\u94fe\u63a5\u3002\u7136\u540e\uff0c\u4ed6\u4eec\u901a\u8fc7\u6267\u884c\u89e3\u6790\u7684\u903b\u8f91\u8868\u5355\u6216\u5728\u4ece KB \u4e2d\u62bd\u53d6\u7684\u7279\u5b9a\u95ee\u9898\u7684\u56fe\u4e0a\u8fdb\u884c\u63a8\u7406\uff0c\u4ece\u800c\u5728\u4e3b\u9898\u5b9e\u4f53\u7684\u90bb\u57df\u83b7\u5f97\u7b54\u6848\u3002</p> <p>\u4ed6\u4eec\u5305\u542b\u4e0d\u540c\u7684\u5de5\u4f5c\u673a\u5236\u6765\u89e3\u51b3 KBQA \u4efb\u52a1\u3002SP-based \u4ee5\u7b26\u53f7\u903b\u8f91\u5f62\u5f0f\u8868\u793a\u4e00\u4e2a\u95ee\u9898\uff0c\u7136\u540e\u5bf9 KB \u6267\u884c\u4ece\u800c\u83b7\u53d6\u6700\u7ec8\u7b54\u6848\u3002IR-based \u5219\u6784\u5efa\u4e00\u4e2a\u95ee\u9898\u7279\u5b9a\u56fe\u6765\u4f20\u9012\u4e0e\u95ee\u9898\u76f8\u5173\u7684\u7efc\u5408\u4fe1\u606f\u5e76\u6839\u636e\u95ee\u9898\u7684\u76f8\u5173\u6027\u5bf9\u62bd\u53d6\u56fe\u4e2d\u7684\u6240\u6709\u5b9e\u4f53\u8fdb\u884c\u6392\u5e8f\u3002</p> <p>\u590d\u6742\u95ee\u9898\u5bf9\u65b9\u6cd5\u63d0\u51fa\u6311\u6218\uff1a</p> <ul> <li>\u73b0\u6709 SP-based \u4f7f\u7528\u7684\u89e3\u6790\u5668\u96be\u4ee5\u8986\u76d6\u591a\u6837\u5316\u7684\u590d\u6742\u67e5\u8be2\u3002\u800c\u4ee5\u524d IR-based \u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u56de\u7b54\u590d\u6742\u7684\u67e5\u8be2\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u6392\u5e8f\u662f\u5728\u5c0f\u8303\u56f4\u5b9e\u4f53\u4e0a\u8fdb\u884c\u7684\uff0c\u6ca1\u6709\u53ef\u8ffd\u8e2a\u7684\u63a8\u7406\u3002</li> <li>\u590d\u6742\u95ee\u53e5\u4e2d\u66f4\u591a\u7684\u5173\u7cfb\u548c\u4e3b\u8bed\u9884\u793a\u7740\u89e3\u6790\u6f5c\u5728\u903b\u8f91\u5f62\u5f0f\u7684\u641c\u7d22\u7a7a\u95f4\u66f4\u5927\uff0c\u8fd9\u5c06\u5927\u5927\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u3002\u540c\u65f6\uff0c\u66f4\u591a\u7684\u5173\u7cfb\u4e0e\u4e3b\u4f53\u5bf9 IR \u76f8\u5173\u5b9e\u4f53\u6392\u5e8f\u9020\u6210\u963b\u788d</li> <li>\u4e24\u79cd\u65b9\u6cd5\u90fd\u4ee5\u95ee\u9898\u7406\u89e3\u4f5c\u4e3a\u9996\u8981\u6b65\u9aa4\uff0c\u4f46\u5f53\u95ee\u9898\u5728\u8bed\u4e49\u4e0e\u7b26\u53f7\u5c42\u9762\u8d8b\u4e8e\u590d\u6742\uff0cmodel \u4e5f\u88ab\u8981\u6c42\u5bf9\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u6982\u62ec\u65b9\u9762\u6709\u66f4\u9ad8\u7684\u80fd\u529b</li> <li>\u5bf9\u4e8e\u590d\u6742\u95ee\u9898\uff0c\u5c06\u57fa\u672c\u771f\u503c\u8def\u5f84\u6807\u6ce8\u5230\u7b54\u6848\u662f\u6602\u8d35\u7684\u3002\u4e00\u822c\u53ea\u63d0\u4f9b\u95ee\u7b54\u5bf9\u3002\u8fd9\u8868\u660e SP-based \u548c IR-based \u5206\u522b\u9700\u8981\u5728\u6ca1\u6709\u6b63\u786e\u903b\u8f91\u5f62\u5f0f\u548c\u63a8\u7406\u8def\u5f84\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u8bad\u7ec3\u3002\u5982\u6b64\u5fae\u5f31\u7684\u76d1\u7763\u4fe1\u53f7\u7ed9\u4e24\u79cd\u65b9\u6cd5\u90fd\u5e26\u6765\u4e86\u56f0\u96be\u3002</li> </ul> <p>2 Background</p> <p>\u4ecb\u7ecd\u4efb\u52a1\u5236\u5b9a\u7684\u521d\u7ea7\u77e5\u8bc6\uff0c\u4ee5\u53ca\u591a\u4e2a\u53ef\u7528\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae</p> <p>Tasks</p> <p>\u5bf9\u4e8e\u590d\u6742 KBQA \u7684\u4efb\u52a1\uff0c\u7ed9\u5b9a\u4e00\u4e2a\u7531\u4e00\u7ec4\u4e8b\u5b9e\u7ec4\u6210\u7684 KB \u4f5c\u4e3a\u8f93\u5165\uff0c \u5176\u4e2d\u4e3b\u4f53\u4e0e\u5ba2\u4f53\u901a\u8fc7\u5176\u5173\u7cfb\u8fdb\u884c\u94fe\u63a5\u3002\u4e8b\u5b9e\u4e2d\u7684\u6240\u6709\u4e3b\u4f53\u548c\u5bf9\u8c61\u6784\u6210\u4e86 KB \u7684\u5b9e\u4f53\u96c6\u3002\u8003\u8651\u5230\u53ef\u7528\u7684\u77e5\u8bc6\u5e93\uff0c\u590d\u6742 KBQA \u4efb\u52a1\u65e8\u5728\u4ee5 sequence of tokens \u7684\u683c\u5f0f\u56de\u7b54\u590d\u6742\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u3002\u7279\u522b\u5730\uff0c\u6211\u4eec\u5047\u8bbe\u6b63\u786e\u7b54\u6848\u6765\u81ea KB \u7684\u5b9e\u4f53\u96c6\u5408\u3002\u4e0e\u7b80\u5355 KBQA \u7684\u7b54\u6848\u4e0d\u540c\uff0c\u8fd9\u4e9b\u7b54\u6848\u662f\u76f4\u63a5\u8fde\u63a5\u5230\u4e3b\u9898\u5b9e\u4f53\u7684\u5b9e\u4f53\uff0c\u800c\u590d\u6742 KBQA \u4efb\u52a1\u7684\u7b54\u6848\u662f\u8ddd\u79bb\u4e3b\u9898\u5b9e\u4f53\u591a\u8df3\u7684\u5b9e\u4f53\uff0c\u751a\u81f3\u662f\u5b83\u4eec\u7684\u4e00\u4e9b\u805a\u5408\u3002</p> <p>Datasets</p> <p>\u901a\u5e38\u6765\u8bf4\uff0c\u4e3a\u4e86\u8bad\u7ec3\u4e00\u4e2a\u590d\u6742\u7684 KBQA \u7cfb\u7edf\u5e94\u8be5\u63d0\u4f9b\u95ee\u9898\u7684\u7b54\u6848\u3002\u56e0\u6b64\uff0c\u9488\u5bf9\u590d\u6742 KBQA \u6570\u636e\u96c6\u7684\u6784\u5efa\uff0c\u505a\u4e86\u5f88\u591a\u5de5\u4f5c\u3002\u53ef\u7528 (\u5e38\u7528) \u7684\u6570\u636e\u96c6\u5982\u4e0b\uff1a</p> <p>\u53ef\u5229\u7528\u6570\u636e\u96c6</p> <p>\u603b\u4f53\u800c\u8a00\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u662f\u901a\u8fc7\u4ee5\u4e0b\u6b65\u9aa4\u6784\u5efa\u7684\uff1a</p> <ul> <li>\u7ed9\u5b9a KB \u4e2d\u7684\u4e00\u4e2a\u4e3b\u9898\u5b9e\u4f53\u4f5c\u4e3a\u95ee\u9898\u4e3b\u4f53</li> <li>\u9996\u5148\u7528\u4e0d\u540c\u7684\u6a21\u677f\u521b\u5efa\u7b80\u5355\u7684\u95ee\u9898</li> <li>\u57fa\u4e8e\u7b80\u5355\u95ee\u9898\u548c\u77e5\u8bc6\u5e93\u4e2d\u4e3b\u9898\u5b9e\u4f53\u7684\u90bb\u57df\uff0c\u7528\u9884\u5b9a\u4e49\u7684\u6a21\u677f\u8fdb\u4e00\u6b65\u751f\u6210\u590d\u6742\u95ee\u9898\uff0c\u4e00\u4e9b\u5de5\u4f5c\u4e5f\u7528\u6a21\u677f\u751f\u6210\u53ef\u6267\u884c\u7684\u903b\u8f91\u5f62\u5f0f\u3002\u540c\u65f6\uff0c\u7528\u76f8\u5e94\u7684\u89c4\u5219\u63d0\u53d6\u7b54\u6848</li> </ul> <p>\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u96c7\u4f63\u5de5\u4f5c\u8005\u5c06\u6a21\u677f\u67e5\u8be2\u590d\u8ff0\u4e3a\u81ea\u7136\u8bed\u8a00\u95ee\u53e5\u5e76\u5bf9\u751f\u6210\u7684\u903b\u8f91\u5f62\u5f0f\u8fdb\u884c\u7ec6\u5316\uff0c\u4f7f\u5f97\u95ee\u53e5\u8868\u8fbe\u66f4\u52a0\u591a\u6837\u5316\u548c\u6d41\u7545\u3002\u4e3a\u4e86\u670d\u52a1\u4e8e\u73b0\u5b9e\u7684\u5e94\u7528\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u901a\u5e38\u521b\u5efa\u9700\u8981\u591a\u4e2aKB\u4e8b\u5b9e\u6765\u63a8\u7406\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u53ef\u80fd\u5305\u62ec\u6570\u503c\u8fd0\u7b97\u4ee5\u53ca\u7ea6\u675f\uff0c\u8fd9\u8fdb\u4e00\u6b65\u589e\u52a0\u4e86\u4ece\u77e5\u8bc6\u5e93\u4e2d\u63a8\u7406\u7b54\u6848\u7684\u96be\u5ea6\u3002</p> <p>Evaluation Protocol</p> <p>KBQA \u7cfb\u7edf\u901a\u5e38\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u5f97\u5206\u6700\u9ad8\u7684\u5b9e\u4f53\u5f62\u6210\u7b54\u6848\u96c6\u3002\u8fd9\u4e5f\u540c\u65f6\u544a\u8bc9\u6211\u4eec\uff0c\u5bf9\u4e00\u4e2a\u95ee\u9898\u6211\u4eec\u53ef\u4ee5\u4ea7\u751f\u591a\u4e2a\u7b54\u6848\u3002\u5728\u4ee5\u5f80\u7684\u7814\u7a76\u4e2d\uff0c\u6709\u4e00\u4e9b\u7ecf\u5178\u7684\u8bc4\u4ef7\u6307\u6807\uff0c\u5982\uff1a\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u548c Hits@1\u3002\u51c6\u786e\u7387\u8868\u793a\u6b63\u786e\u7b54\u6848\u5360\u6240\u6709\u9884\u6d4b\u7b54\u6848\u7684\u6bd4\u4f8b\uff0c\u53ec\u56de\u7387\u662f\u6b63\u786e\u9884\u6d4b\u7b54\u6848\u5360\u6240\u6709\u771f\u5b9e\u7b54\u6848\u7684\u6bd4\u4f8b\u3002</p> <p>3 Two Mainstream Approaches</p> <p>\u4e0a\u8ff0\u4e24\u79cd\u65b9\u6cd5\u9075\u5faa\u7740\u89e3\u6790-\u6267\u884c\u8303\u5f0f (parse-then - excute)(parse-then - excute) \u6216\u68c0\u7d22-\u6392\u5e8f\u8303\u5f0f (retrieval-and-rank)(retrieval-and-rank)</p> <p>SP \u4e0e IR \u65b9\u6cd5\u6982\u62ec</p> <p>Semantic Parsing-based Methods</p> <p>\u8be5\u7c7b\u65b9\u6cd5\u65e8\u5728\u5c06\u81ea\u7136\u8bed\u8a00\u89e3\u6790\u4e3a\u903b\u8f91\u5f62\u5f0f\u3002\u4e3b\u8981\u901a\u8fc7\u4ee5\u4e0b\u6b65\u9aa4\u8fdb\u884c\u5bf9\u95ee\u9898\u8fdb\u884c\u9884\u6d4b\u89e3\u7b54\uff1a</p> <ol> <li>\u4ed6\u4eec\u901a\u8fc7 question understanding \u6a21\u5757\u6765\u5b8c\u5168\u7406\u89e3\u4e00\u4e2a\u95ee\u9898 (\u8fdb\u884c\u8bed\u4e49\u3001\u53e5\u6cd5\u5206\u6790)\uff0c\u5f97\u5230\u4e00\u4e2a\u7f16\u7801\u540e\u95ee\u9898\uff0c\u7528\u4e8e\u540e\u7eed\u7684\u89e3\u6790\u6b65\u9aa4</li> <li>logical parsing \u6a21\u5757\u88ab\u7528\u6765\u5c06\u7f16\u7801\u540e\u95ee\u9898\u8f6c\u5316\u4e3a\u975e\u5b9e\u4f8b\u5316\u7684\u903b\u8f91\u5f62\u5f0f\u3002\u975e\u5b9e\u4f8b\u5316\u7684\u903b\u8f91\u5f62\u5f0f\u662f\u95ee\u9898\u7684\u53e5\u6cd5\u8868\u5f81\uff0c\u6ca1\u6709\u5b9e\u4f53\u4e0e\u5173\u7cfb\u57fa\u7840\u3002\u903b\u8f91\u5f62\u5f0f\u7684\u8bed\u6cd5\u548c\u6210\u5206\u53ef\u4ee5\u6839\u636e\u7cfb\u7edf\u7684\u5177\u4f53\u8bbe\u8ba1\u800c\u6709\u6240\u4e0d\u540c</li> <li>\u4e3a\u4e86\u5bf9 KB \u6267\u884c\uff0c\u901a\u8fc7 KB grounding \u6a21\u5757\u5bf9\u7ed3\u6784\u5316 KBs \u8fdb\u884c\u4e00\u4e9b\u8bed\u4e49\u5bf9\u9f50\uff0c\u8fdb\u4e00\u6b65\u5b9e\u4f8b\u5316\u3001\u9a8c\u8bc1\u903b\u8f91\u5f62\u5f0f</li> <li>\u89e3\u6790\u540e\u7684\u903b\u8f91\u5f62\u5f0f\u901a\u8fc7\u5bf9 KBs \u6267\u884c KB execution \u6a21\u5757\u4ee5\u751f\u6210\u9884\u6d4b\u7b54\u6848</li> </ol> <p>Information Retrieval-based Methods</p> <p>\u8be5\u7c7b\u65b9\u6cd5\u76f4\u63a5\u4ece\u77e5\u8bc6\u5e93\u4e2d\u68c0\u7d22\u7b54\u6848\uff0c\u5e76\u6839\u636e\u95ee\u9898\u6240\u4f20\u8fbe\u7684\u4fe1\u606f\u5bf9\u7b54\u6848\u8fdb\u884c\u6392\u5e8f\u3002\u6b65\u9aa4\u5982\u4e0b\uff1a</p> <ol> <li>\u7531\u4e3b\u9898\u5b9e\u4f53 (topic entity) \u51fa\u53d1\uff0c\u7cfb\u7edf\u9996\u5148\u4ece KBs \u4e2d\u62bd\u53d6\u51fa\u7279\u5b9a\u95ee\u9898\u56fe\u3002\u8be5\u56fe\u5305\u542b\u7740\u6240\u6709\u95ee\u9898\u7684\u76f8\u5173\u5b9e\u4f53\u4e0e\u5173\u7cfb\uff0c\u5e76\u5206\u522b\u5c06\u5176\u4f5c\u4e3a\u8282\u70b9\u4e0e\u8fb9\u3002\u6ca1\u6709\u663e\u5f0f\u751f\u6210\u53ef\u6267\u884c\u7684\u903b\u8f91\u5f62\u5f0f\uff0c\u800c\u662f\u5bf9\u56fe\u8fdb\u884c\u63a8\u7406\uff0c\u7136\u540e\u5bf9\u56fe\u4e2d\u7684\u5b9e\u4f53\u8fdb\u884c\u6392\u5e8f</li> <li>\u5176\u6b21\uff0c\u7cfb\u7edf\u901a\u8fc7 question representation \u6a21\u5757\u5bf9\u8f93\u5165\u95ee\u9898\u8fdb\u884c\u7f16\u7801\u3002\u8be5\u6a21\u5757\u5206\u6790\u95ee\u9898\u7684\u8bed\u4e49\u5e76\u8f93\u51fa\u63a8\u7406\u6307\u4ee4\uff0c\u901a\u5e38\u4ee5\u5411\u91cf\u7684\u5f62\u5f0f\u8868\u793a</li> <li>graph-based reasoning \u6a21\u5757\u901a\u8fc7\u57fa\u4e8e\u5411\u91cf\u7684\u8ba1\u7b97\u8fdb\u884c\u8bed\u4e49\u5339\u914d\uff0c\u5c06\u4fe1\u606f\u6cbf\u7740\u56fe\u4e2d\u7684\u76f8\u90bb\u5b9e\u4f53\u8fdb\u884c\u4f20\u64ad\u548c\u805a\u5408\u3002\u63a8\u7406\u72b6\u6001\u662f\u5728\u63a8\u7406\u6307\u4ee4\u7684\u57fa\u7840\u4e0a\u66f4\u65b0\u7684\uff0c\u5728\u4e0d\u540c\u7684\u65b9\u6cd5 (\u4f8b\u5982\uff0c\u9884\u6d4b\u5b9e\u4f53\u7684\u5206\u5e03\u3001\u5173\u7cfb\u7684\u8868\u793a) \u4e2d\u6709\u4e0d\u540c\u7684\u5b9a\u4e49\u3002</li> <li>answer ranking \u5728\u63a8\u7406\u7ed3\u675f\u65f6\u6839\u636e\u63a8\u7406\u72b6\u6001\u5bf9\u56fe\u4e2d\u5b9e\u4f53\u8fdb\u884c\u6392\u5e8f\u3002\u6392\u540d\u9760\u524d\u7684\u5b9e\u4f53\u88ab\u9884\u6d4b\u4e3a\u95ee\u9898\u7684\u7b54\u6848\u3002</li> </ol> <p>Pros and Cons (\u4f18\u7f3a\u70b9)</p> <p>\u603b\u7684\u6765\u8bf4\uff0cSP-based methods \u53ef\u4ee5\u901a\u8fc7\u751f\u6210\u8868\u8fbe\u6027\u903b\u8f91\u5f62\u5f0f\uff0c\u4ea7\u751f\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u7136\u540e\uff0c\u4ed6\u4eec\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u903b\u8f91\u5f62\u5f0f\u4e0e\u89e3\u91ca\u7b97\u6cd5\u7684\u8bbe\u8ba1\uff0c\u8fd9\u5bfc\u81f4\u6027\u80fd\u6539\u8fdb\u9047\u5230\u74f6\u9888\u3002</p> <p>\u4e0e\u4e4b\u6bd4\u8f83\uff0cIR-based methods \u5bf9\u56fe\u7ed3\u6784\u8fdb\u884c\u590d\u6742\u63a8\u7406\u5e76\u8fdb\u884c\u8bed\u4e49\u5339\u914d\u3002\u8fd9\u6837\u7684\u8303\u5f0f\u5f88\u81ea\u7136\u5730\u9002\u5408\u6d41\u884c\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u4f7f\u5f97\u8be5\u6a21\u578b\u66f4\u5bb9\u6613\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u63a8\u7406\u6a21\u578b\u7684\u9ed1\u7bb1\u98ce\u683c\u4f7f\u5f97\u4e2d\u95f4\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u6027\u8f83\u5dee\u3002</p> <p>4 Challenges and Solutions</p> <p>\u7531\u4e8e\u4e0a\u8ff0\u65b9\u6cd5\u662f\u57fa\u4e8e\u4e0d\u540c\u7684\u8303\u5f0f\u53d1\u5c55\u8d77\u6765\u7684\uff0c\u4e0b\u9762\u5c06\u5206\u522b\u4ecb\u7ecd\u4e24\u79cd\u4e3b\u6d41\u65b9\u6cd5\u5728\u590d\u6742KBQA\u4e2d\u9762\u4e34\u7684\u6311\u6218\u548c\u76f8\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4e0b\u8868\u603b\u7ed3\u4e86\u8fd9\u4e9b\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u3002</p> <p>\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6cd5</p> <p>5 Conclusion and Future Directions</p> <p>\u672c\u6587\u8bd5\u56fe\u63d0\u4f9b\u5173\u4e8e\u590d\u6742 KBQA \u7684\u5178\u578b\u6311\u6218\u548c\u76f8\u5e94\u89e3\u51b3\u65b9\u6848\u7684\u6982\u8ff0\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86\u5e38\u7528\u7684\u6570\u636e\u96c6\u5e76\u603b\u7ed3\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u4e8e SP \u7684\u65b9\u6cd5\u548c\u57fa\u4e8e IR \u7684\u65b9\u6cd5\u3002\u73b0\u6709\u7684\u590d\u6742 KBQA \u65b9\u6cd5\u4e00\u822c\u5f52\u7eb3\u4e3a\u8fd9\u4e24\u7c7b\u3002\u9664\u6b64\u4e4b\u5916\uff0c\u5176\u4ed6\u4e00\u4e9b\u65b9\u6cd5\u80fd\u4e0d\u5c5e\u4e8e\u8fd9\u4e24\u7c7b\u3002\u4f8b\u5982\uff0c\u901a\u8fc7\u57fa\u4e8e\u89c4\u5219\u7684\u5206\u89e3\u5c06\u590d\u6742\u95ee\u9898\u8f6c\u5316\u4e3a\u7b80\u5355\u95ee\u9898\u7684\u7ec4\u5408\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u95ee\u9898\u5206\u89e3\uff0c\u800c\u4e0d\u662f\u57fa\u4e8e KBs \u7684\u63a8\u7406\u6216\u903b\u8f91\u5f62\u5f0f\u751f\u6210\u3002\u6211\u4eec\u76f8\u4fe1\uff0c\u590d\u6742\u77e5\u8bc6\u5e93\u95ee\u7b54\u7cfb\u7edf\u5c06\u7ee7\u7eed\u6210\u4e3a\u4e00\u4e2a\u6d3b\u8dc3\u7684\u3001\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\u7684\u7814\u7a76\u9886\u57df\uff0c\u5982\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u7ec4\u5408\u6cdb\u5316\u3001\u591a\u8df3\u63a8\u7406\u7b49\u3002\u8fd9\u9879\u8c03\u67e5\u4e2d\u63d0\u51fa\u7684\u8bb8\u591a\u6311\u6218\u4ecd\u7136\u662f\u5f00\u653e\u7684\u548c\u63a2\u7d22\u4e0d\u8db3\u7684</p>"}]}